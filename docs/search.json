[
  {
    "objectID": "labs/04.LogisticRegression.html",
    "href": "labs/04.LogisticRegression.html",
    "title": "4  Lab: LogisticRegression",
    "section": "",
    "text": "4.1 Preparing the input variables\nPrepare the data for implementing a logistic regression model. The data set used in this practical is the “sar_sample_label.csv” and “sar_sample_code.csv”. The SAR is a snapshot of census microdata, which are individual level data. The data sample has been drawn and anonymised from census and known as the Samples of Anonymised Records (SARs).\nYou may need to download the two datasets from our github website if you haven’t. The two csv are actually the same dataframe, only one uses the label as the value but the other uses the code. We will first read in both for the data overview the labels are more friendly, and then we focus on using “sar_sample_code.csv” in the regression model as it is easier for coding.\nif(!require(\"tidyverse\"))\n  install.packages(\"tidyverse\",dependencies = T)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nif(!require(\"broom\"))\n  install.packages(\"broom\",dependencies = T)\n\nLoading required package: broom\n\nlibrary(tidyverse)\nlibrary(broom)\nsar_label &lt;- read_csv(\"../data/SAR/sar_sample_label.csv\")\n\nRows: 50000 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (39): country, region, county, la_group, age_group, sex, marital_status,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsar_code &lt;- read_csv(\"../data/SAR/sar_sample_code.csv\")\n\nRows: 50000 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (39): country, region, county, la_group, age_group, sex, marital_status,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nView(sar_label)\nView(sar_code)\n\nglimpse(sar_label)\nglimpse(sar_code)\nsummary(sar_code)\nsummary(sar_label)\nThe outcome variable is a person’s commuting distance captured by the variable “work_distance”.\ntable(sar_label$work_distance)\n\n\n                                10 to &lt;20 km \n                                        3650 \n                                  2 to &lt;5 km \n                                        4414 \n                                20 to &lt;40 km \n                                        2014 \n                                 40 to &lt;60km \n                                         572 \n                                 5 to &lt;10 km \n                                        4190 \n                                60km or more \n                                         703 \n                       Age&lt;16 or not working \n                                       25975 \n                                     At home \n                                        2427 \n                              Less than 2 km \n                                        4028 \n                              No fixed place \n                                        1943 \nWork outside England and Wales but within UK \n                                          29 \n                             Work outside UK \n                                          21 \n  Works at offshore installation (within UK) \n                                          34\ntable(sar_code$work_distance)\n\n\n   -9     1     2     3     4     5     6     7     8     9    10    11    12 \n25975  4028  4414  4190  3650  2014   572   703  2427  1943    29    21    34\nThere are a variety of categories in the variable, however, we are only interested in commuting distance and therefore in people reporting their commuting distance. Thus, we will explore the numeric codes of the variable ranging from 1 to 8.\nAs we are also interested in exploring whether people with different socio-economic statuses (or occupations) tend to be associated with varying probabilities of commuting over long distances, we further filter or select cases.\ntable(sar_label$nssec)\n\n\n                              Child aged 0-15 \n                                         9698 \n                            Full-time student \n                                         3041 \n              Higher professional occupations \n                                         3162 \n                     Intermediate occupations \n                                         5288 \n          Large employers and higher managers \n                                          909 \nLower managerial and professional occupations \n                                         8345 \n  Lower supervisory and technical occupations \n                                         2924 \n         Never worked or long-term unemployed \n                                         2261 \n                          Routine occupations \n                                         4660 \n                     Semi-routine occupations \n                                         5893 \n      Small employers and own account workers \n                                         3819 \n\ntable(sar_code$nssec)\n\n\n   1    2    3    4    5    6    7    8    9   10   12 \n 909 3162 8345 5288 3819 2924 5893 4660 2261 3041 9698\nUsing nssec, we select people who reported an occupation, and delete cases with numeric codes from 9 to 12, who are unemployed, full-time students, children and not classifiable.\nNow, similar to next week, we use the filter() to prepare our dataframe today. You may already realise that using sar_code is easier to do the filtering.\nsar_df &lt;- sar_code %&gt;% filter(work_distance&lt;=8 & nssec &lt;=8 )\nRecode the “work_distance” variable into a binary dependent variable\nA simple way to create a binary dependent variable representing long-distance commuting is to use the mutate() function as discussed in last week’s practical session. Before creating the binary variables from the “work_distance” variable, we need to define what counts as a long-distance commuting move. Such definition can vary. Here we define a long-distance commuting move as any commuting move over a distance above 60km (the category of “60km or more”).\nsar_df &lt;- sar_df %&gt;% mutate(\n  New_work_distance = if_else(work_distance &gt;6, 1,0))\nPrepare your “nssec” variable before the regression model\nThe nssec is a categorical variable. Therefore, as we’ve learnt last week, before adding the categorical variables into the regression model, we need first make it a factor and then identify the reference category.\nWe are interested in whether people with occupations being “Higher professional occupations” are associated with a lower probability of commuting over long distances when comparing to people in other occu\nsar_df$nssec &lt;- relevel(as.factor(sar_df$nssec), ref = \"2\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab: LogisticRegression</span>"
    ]
  },
  {
    "objectID": "labs/04.LogisticRegression.html#preparing-the-input-variables",
    "href": "labs/04.LogisticRegression.html#preparing-the-input-variables",
    "title": "4  Lab: LogisticRegression",
    "section": "",
    "text": "Code for Work_distance\nCategories\n\n\n\n\n1\nLess than 2 km\n\n\n2\n2 to &lt;5 km\n\n\n3\n5 to &lt;10 km\n\n\n4\n10 to &lt;20 km\n\n\n5\n20 to &lt;40 km\n\n\n6\n40 to &lt;60 km\n\n\n7\n60km or more\n\n\n8\nAt home\n\n\n9\nNo fixed place\n\n\n10\nWork outside England and Wales but within UK\n\n\n11\nWork outside UK\n\n\n12\nWorks at offshore installation (within UK)\n\n\n\n\n\n\n\n\n\nCode for nssec\nCategory labels\n\n\n\n\n1\nLarge employers and higher managers\n\n\n2\nHigher professional occupations\n\n\n3\nLower managerial and professional occupations\n\n\n4\nIntermediate occupations\n\n\n5\nSmall employers and own account workers\n\n\n6\nLower supervisory and technical occupations\n\n\n7\nSemi-routine occupations\n\n\n8\nRoutine occupations\n\n\n9\nNever worked or long-term employed\n\n\n10\nFull-time student\n\n\n11\nNot classifiable\n\n\n12\nChild aged 0-15\n\n\n\n\n\n\nQ1. Summarise the frequencies of the two variables “work_distance” and “nssec” with the new data.\n\n\n\n\n\nQ2. Check the new sar_df dataframe with new column named New_work_distance by using the codes you have learnt.\n\n\n\n\n\n\n4.1.1 Implementing a logistic regression model\nThe binary dependent variable is long-distance commuting, variable name New_work_distance.\nThe independent variables are gender and socio-economic status.\nFor gender, we use male as the basline.\n\nsar_df$sex &lt;- relevel(as.factor(sar_df$sex),ref=\"1\")\n\nFor socio-economic status, we use code 5 (Small employers and Own account workers) as the baseline category to explore whether people work as independent employers show lower probability of commuting longer than 60km compared with other occupations.\n\n#create the model\nm.glm = glm(New_work_distance~sex + nssec, \n            data = sar_df, \n            family= \"binomial\")\n# inspect the results\nsummary(m.glm) \n\n\nCall:\nglm(formula = New_work_distance ~ sex + nssec, family = \"binomial\", \n    data = sar_df)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.67337    0.05329 -31.401  &lt; 2e-16 ***\nsex2        -0.36678    0.04196  -8.742  &lt; 2e-16 ***\nnssec1      -0.12881    0.11306  -1.139    0.255    \nnssec3      -0.38761    0.06467  -5.994 2.05e-09 ***\nnssec4      -1.03079    0.08439 -12.214  &lt; 2e-16 ***\nnssec5       1.22639    0.06489  18.898  &lt; 2e-16 ***\nnssec6      -1.38992    0.10919 -12.730  &lt; 2e-16 ***\nnssec7      -1.43909    0.09002 -15.986  &lt; 2e-16 ***\nnssec8      -1.48534    0.09646 -15.398  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 20441  on 33025  degrees of freedom\nResidual deviance: 17968  on 33017  degrees of freedom\nAIC: 17986\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n# odds ratios\nexp(coef(m.glm)) \n\n(Intercept)        sex2      nssec1      nssec3      nssec4      nssec5 \n  0.1876138   0.6929649   0.8791416   0.6786766   0.3567267   3.4088847 \n     nssec6      nssec7      nssec8 \n  0.2490946   0.2371432   0.2264258 \n\n\n\n# confidence intervals\nexp(confint(m.glm, level = 0.95)) \n\nWaiting for profiling to be done...\n\n\n                2.5 %    97.5 %\n(Intercept) 0.1688060 0.2080319\nsex2        0.6381810 0.7522773\nnssec1      0.7017990 1.0935602\nnssec3      0.5981911 0.7708192\nnssec4      0.3020431 0.4205270\nnssec5      3.0037298 3.8739884\nnssec6      0.2002766 0.3073830\nnssec7      0.1984396 0.2824629\nnssec8      0.1869397 0.2729172\n\n\nQ3. If we want to explore whether people with occupation being “Large employers and higher managers”, “Higher professional occupations” and “Routine occupations” are associated with higher probability of commuting over long distance when comparing to people in other occupation, how will we prepare the input independent variables and what will be the specified regression model?\nHint: use mutate() to create a new column, set the value of “Large employers and higher managers”, “Higher professional occupations” and “Routine occupations” as original, while the rest as “Other occupations” ().\n\nsar_df &lt;- sar_df %&gt;% mutate(New_nssec = if_else(!nssec %in% c(1,2,8), \"0\" ,nssec))\n\nUse “Other occupations” (code: 0) as the reference category by relevel(as.factor()) and then create the regression model: glm(New_work_distance~sex + New_nssec, data = sar_df, family= \"binomial\")\n\n\n4.1.2 Model fit\nWe include the R library pscl for calculate the measures of fit.\n\nif(!require(\"pscl\"))\n  install.packages(\"pscl\")\n\nLoading required package: pscl\n\n\nWarning: package 'pscl' was built under R version 4.3.3\n\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\nlibrary(pscl)\n\nRelating back to this week’s lecture notes, what is the Pseudo R2 of the fitted logistic model (from the Model Summary table below)?\n\n# Pseudo R-squared\npR2(m.glm)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-8.983928e+03 -1.022037e+04  2.472890e+03  1.209785e-01  7.214246e-02 \n         r2CU \n 1.563288e-01 \n\n# or in better format\npR2(m.glm) %&gt;% round(4) %&gt;% tidy()\n\nfitting null model for pseudo-r2\n\n\n# A tibble: 6 × 2\n  names              x\n  &lt;chr&gt;          &lt;dbl&gt;\n1 llh       -8984.    \n2 llhNull  -10220.    \n3 G2         2473.    \n4 McFadden      0.121 \n5 r2ML          0.0721\n6 r2CU          0.156 \n\n\n\nllh: The log-likelihood of the fitted model.\nllhNull: The log-likelihood of the null model (without predictors).\nG2: The likelihood ratio statistic, showing the model’s improvement over the null model.\nMcFadden: McFadden’s pseudo R-squared (a common measure of model fit).\nr2ML: Maximum likelihood pseudo R-squared.\nr2CU: Cox & Snell pseudo R-squared.\n\nDifferent from the multiple linear regression, whose R-squared indicates % of the variance in the dependent variables that is explained by the independent variable. In logistic regression model, R-squared is not directly applicable. Instead, we use pseudo R-squared measures, such as McFadden’s pseudo R-squared, or Cox & Snell pseudo R-squared to provide an indication of model fit. For the individual level dataset like SAR, value around 0.3 is considered good for well-fitting.\n\n\n4.1.3 Statistical significance of regression coefficients or covariate effects\nSimilar to the statistical inference in a linear regression model context, p-values of regression coefficients are used to assess significances of coefficients; for instance, by comparing p-values to the conventional level of significance of 0.05:\n·       If the p-value of a coefficient is smaller than 0.05, the coefficient is statistically significant. In this case, you can say that the relationship between an independent variable and the outcome variable is statistically significant.\n·       If the p-value of a coefficient is larger than 0.05, the coefficient is statistically insignificant. In this case, you can say or conclude that there is no statistically significant association or relationship between an independent variable and the outcome variable.\n\n\n4.1.4 Interpreting estimated regression coefficients\n\nThe interpretation of coefficients (B) and odds ratios (Exp(B)) for the independent variables differs from that in a linear regression setting.\nInterpreting the regression coefficients.\n\no   For the variable Sex, a negative sign and the odds ratio estimate indicate that the probability of commuting over long distances for female is 0.693 times less likely than male (the reference group), with the confidence intervals (CI) or likely range between 0.6 to 0.7, holding all other variables constant (the socio-economic classification variable). Put it differently, being females reduces the probability of long-distance commuting by 30.7% (1-0.693).\no   For variable “nssec”, a positive significant and the odds ratio estimate indicate that the probability of long-distance commuting for those whose socio-economic classification as:\n\nsmall employers and own account workers (nssec=5) are 3.409 times more likely than the higher prof occupations, holding all other variables constant (the Sex variable), with a likely range (CI) of between 3.0 to 3.8.\nthe p-value of Large employers and higher managers (nssec=1) is &gt; 0.05, so thre is no statistically significant relationship between large employers and higher managers and long-distance commuting.\nRoutine occupations (nssec=8) are 0.226 times (or 22.6%) less likely than the higher professional occupations, when other variable constant. Or, we can see being routine occupations decreases the probability of long-distance commuting by 77.4% (1-0.226).\n\n\nQ4. Interpret the regression coefficients (i.e. Exp(B)) of variables “nssec=Lower managerial and professional occupations” and “nssec=Semi-routine occupation”.\n\n\nQ5. Could you identify significant factors of commuting over long distances?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab: LogisticRegression</span>"
    ]
  },
  {
    "objectID": "labs/04.LogisticRegression.html#extension-activities",
    "href": "labs/04.LogisticRegression.html#extension-activities",
    "title": "4  Lab: LogisticRegression",
    "section": "4.2 Extension activities",
    "text": "4.2 Extension activities\nThe extension activities are designed to get yourself prepared for the Assignment 2 in progress. For this week, try whether you can:\n\nSelect a regression strategy and explain why a linear or logit model is appropriate\nPerform one or a series of regression models, including different combinations of your chosen independent variables to explain and/or predict your dependent variable",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Lab: LogisticRegression</span>"
    ]
  },
  {
    "objectID": "general/assessment.html",
    "href": "general/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "Required Report Structure\nFollow this structure and include ALL these points, do not make your life harder.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#required-report-structure",
    "href": "general/assessment.html#required-report-structure",
    "title": "Assessment",
    "section": "",
    "text": "Introduction\n\nContext: Why is the topic relevant or worth being investigated?\nBrief discussion of existing literature.\nKnowledge gap and Aim.\nResearch questions.\n\nLiterature review\n\nMore detailed Literature review, i.e. what do we already know about this subject\nRationale for including certain predictor variables in the model.\nWhat knowledge gap remains that this article will address? (includes “not studied before in this area”). Note: there is no expectation on totally original research. The focus is on a clean, sensible, data analysis situated in existing ideas.\n\nMethodology:\n\nA brief introduction to the dataset being analysed (who collected it? When? How many responses? etc.)\nA description of the variables chosen to be analysed.\nA description of any transformation made to the original data, i.e. turning a continuous variable of income into intervals, or reducing the number of age groups from 11 to 3.\nA description and justification of the statistical techniques used in the subsequent analysis (i.e. the Multivariate regression model: Multiple or Logistic Linear Regression).\n\nResults and Discussion\n\nDescriptive statistics and summary of the variables employed.\nCorrect interpretation of correlation coefficients.\nUsage and results of an appropriate multivariate regression model.\nInterpretation of the results, including links and contrasts to existing literature.\nSelective illustrations (graphs and tables) to make your findings as clear as possible.\n\nConclusion\n\nSummary of main findings.\nLimitations of study (self-critique).\n\n\n\nHighlight any implications derived from the study.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-to-get-there",
    "href": "general/assessment.html#how-to-get-there",
    "title": "Assessment",
    "section": "How to get there?",
    "text": "How to get there?\nThe first stage is to identify ONE a relevant research question to be addressed. Based on the chosen question, you will need to identify a dependent (or outcome) variable which you want to explain, and at least two relevant independent variables that you can use to explain the chosen dependent variable. The selection of variables should be informed by the literature and empirical evidence.\nTo detail in the Methods Section: Once the variables have been chosen, you will need to describe the data and appropriate type of regression to be used for the analysis. You need to explain any transformation done to the original data source, such as reclassifying variables, or changing variables from continuous to nominal scales. You also need to briefly describe the data use: source of data, year of data collection, indicate the number of records used, state if you are using individual records or geographical units, explain if you are selecting a sample, and any relevant details. You also need to identify type of regression to be used and why.\nTo detail in the Results and Discussion Section: Firstly, you need to provide two types of analyses. First, you need to provide a descriptive analysis of the data. Here you could use tables and/or plots reporting relevant descriptive statistics, such as the mean, median and standard deviation; variable distributions using histograms; and relationships between variables using correlation matrices or scatter plots. Secondly, you need to present an estimated regression model or models and the interpretation of the estimated coefficients. You need a careful and critical analysis of the regression estimates. You should think that you intend to use your regression models to advice your boss who is expecting to make some decisions based on the information you will provide. As part of this process, you need to discuss the model assessment results for the overall model and regression coefficients. Remember to substantiate your arguments using relevant literature and evidence, and present results clearly in tables and graphs.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-to-submit",
    "href": "general/assessment.html#how-to-submit",
    "title": "Assessment",
    "section": "How to submit",
    "text": "How to submit\nYou should submit a .pdf file, that is a rendered version of a Quarto Markdown file (qmd file). This will allow you to write a research paper that also includes your working code, without the need of including the data (rendered .qmd files are executed before being converted to R).\nHow to get a PDF?\n\nInstall Quarto: Make sure you have Quarto installed. You can download it from quarto.org.\nLaTeX Installation: For PDF output, you’ll need a LaTeX distribution like TinyTeX from R, by executing this in the R console:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOpen the Quarto File: Open your .qmd file in RStudio.\nSet Output Format: In the YAML header at the top of your Quarto file, specify pdf under format:\n\n\n\n\n\n\n\n\n\n\n    title: \"Your Document Title\"\n    author: \"Anonymous\" # do not change\n    format: pdf\n\nClick the Render button in the RStudio toolbar (next to the Knit button).",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-is-it-graded",
    "href": "general/assessment.html#how-is-it-graded",
    "title": "Assessment",
    "section": "How is it graded?",
    "text": "How is it graded?\n\n\n\n\n\n\n\n\n\n\nGrade\nScore Range\nUG\nDescriptor\nAssignment Expectations\n\n\n\n\nFail\n0-34%\nFail\nInadequate\nLiterature Review: Lacks relevance and fails to justify variable choice. Evidence is irrelevant or missing, providing no support to the research question. Methods: Data is not described, and the regression model is entirely missing. No appropriate statistical method is applied. Results and Discussion: No descriptive statistics, graphs, or tables are provided. Model results and interpretation are absent. Structure and References: Report is disorganized with significant referencing and citation errors throughout.\n\n\nNarrow Fail\n35-39%\nFail\nHighly Deficient\nLiterature Review: Review is present but lacks coherence and fails to justify variable choice. Evidence is poorly aligned with the research question and mostly irrelevant. Methods: Minimal data description; the regression model is missing but some statistical methods are mentioned. Results and Discussion: Few or no descriptive statistics or visuals are present. Statistical methods are unclear or incorrectly applied. Results are vague and lack meaningful interpretation. Structure and References: Report structure is poor, with referencing errors in multiple sections.\n\n\nThird / Fail\n40-49%\nThird (UG)\nDeficient\nLiterature Review: Relevant literature is partially addressed but lacks depth, with limited justification for variable choice. Evidence is minimally aligned with the research question. Methods: A very basic data description is provided, but the selected regression model is deeply inadequate or incorrect (e.g., multiple linear regression for a categorical outcome; logistic regression for a continuous outcome). Results and Discussion: Descriptive statistics or visuals may be present but insufficient. Model results are presented with little to no interpretation. Structure and References: Report structure is present but lacks clarity, with inconsistencies in citations and citation style.\n\n\n2.2 / Pass\n50-59%\n2.2 (UG)\nAdequate\nLiterature Review: Addresses relevant literature but with limited justification of variable choices. Evidence generally supports the research question but lacks detail. Methods: Data description is present but brief; a regression model is included but applied illogically or incorrectly (e.g., multiple linear regression for a categorical outcome; logistic regression for a continuous outcome) and with little explanation. Results and Discussion: Basic descriptive statistics, graphs, or tables are presented; the regression model is applied with some inaccuracies and/or interpretation is minimal. Structure and References: Report is mostly organized, though with referencing inconsistencies.\n\n\n2.1 / Merit\n60-69%\n2.1 (UG)\nGood\nLiterature Review: Relevant literature is discussed, with some justification for variable choice. Evidence supports the research question well. Methods: Data is described with some detail, though potential data transformations are under-explored. The regression model is appropriate for the selected variable types. Results and Discussion: Descriptive statistics and visuals are provided. Model results are discussed, though interpretation lacks depth. Findings are compared to existing literature. Structure and References: Report is logically structured and clear, with mostly correct citations.\n\n\nFirst / Distinction\n70-79%\nFirst (UG)\nVery Good\nLiterature Review: Strong grasp of relevant literature, with well-justified variable selection. Evidence aligns well with the research question. Methods: Data is comprehensively described with consideration of relevant transformations. The regression model is appropriate and well-justified. Results and Discussion: Descriptive statistics and clear visuals support findings. Model results are accurately interpreted with strong connections to existing literature. Structure and References: Report has a coherent, professional structure with only minor referencing errors.\n\n\nHigh First / High Distinction\n80-100%\nHigh First (UG)\nExcellent to Outstanding\nLiterature Review: Critical and thorough literature review with strong, well-justified variable selection. Evidence fully supports the research question with insightful connections. Methods: Detailed data description and transformation steps are clearly articulated. Regression model is expertly applied and justified. Results and Discussion: Comprehensive descriptive statistics, graphs, and tables are provided. Model results are innovatively interpreted with strong links to existing research. Structure and References: Report is professionally structured, with flawless citations and a high standard of organization.\n\n\n\n\nIn summary:\n\nIntroduction: Should establish the topic’s relevance, present a concise literature overview, identify a knowledge gap, and outline research questions.\nLiterature Review: Requires an in-depth review of relevant studies, justification for chosen independent variables, and identification of a potential knowledge gap or unexplored area aligned with the chosen research question.\nMethods and Data: Should describe the dataset, variable transformations, and justify the regression technique. Key transformations, such as reclassifying variables, should be explained with clarity and relevance.\nResults and Discussion: Involves presenting descriptive statistics, followed by a clear regression analysis. Discussion should interpret results, compare findings with existing literature, and include meaningful tables and graphs.\nConclusion: Summarize findings, discuss limitations, and suggest future directions.\nReferencing: Requires correct and consistent citations and a well-structured reference list.\n\nEmploying a novel dataset, i.e. not employed during the practical sessions, for the assignment will be awarded with a higher grade. For example, the quantitative dataset from Secondary datasets for Human Geography and Planning Students: 202425-ENVS203.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html",
    "href": "labs/03.QualitativeVariable.html",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "",
    "text": "3.1 Analysis categorical variables\nRecall in Week 7, you get familiar to R by using the Family Resource Survey data. Today we will keep explore the data by using its categorical variables. As usual we first load the necessary libraries.\nSome tips to avoid R returning can’t find data errors:\nCheck your working directory by\ngetwd()\nCheck the relative path of your data folder on your PC/laptop, make sure you know the relative path of your data from your workding directory, returned by getwd().\nLibrary knowledge used in today:\nA useful shortcut to format your code: select all your code lines, use Ctrl+Shift+A for automatically format them in a tidy way.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html#analysis-categorical-variables",
    "href": "labs/03.QualitativeVariable.html#analysis-categorical-variables",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "",
    "text": "dplyr: a basic library provides a suite of functions for data manipulation\nggplot2: a widely-used data visualisation library to help you create nice plots through layered plotting.\ntidyverse: a collection of R packages designed for data science, offering a cohesive framework for data manipulation, visualization, and analysis. Containing dyplyr, ggplot2 and other basic libraries.\nbroom: a part of the tidyverse and is designed to convert statistical analysis results into tidy data frames.\nforcats: designed to work with factors, which are used to represent categorical data. It simplifies the process of creating, modifying, and ordering factors.\nvcd: visualise and analyse categorical data.\n\n\n\n3.1.1 Data overview\n\nif(!require(\"dplyr\"))\n  install.packages(\"dplyr\",dependencies = T)\n\nLoading required package: dplyr\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# Load necessary libraries \nif(!require(\"ggplot2\"))\n  install.packages(\"ggplot2\",dependencies = T)\n\nLoading required package: ggplot2\n\nif(!require(\"broom\"))\n  install.packages(\"broom\",dependencies = T)\n\nLoading required package: broom\n\nlibrary(dplyr) \nlibrary(ggplot2)\nlibrary(broom)\n\nOr we can use library tidyverse which includes ggplot2, dplyr,broom and other foundamental libraries together already, remember you need first install the package if you haven’t by using install.packages(\"tidyverse\").\n\nif(!require(\"tidyverse\"))\n  install.packages(\"tidyverse\",dependencies = T)\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)\n\nWe will also use forcat library, so\n\nif(!require(\"forcats\"))\n  install.packages(\"forcats\")\n\nlibrary(forcats)\n\nExactly as you did in previous weeks, we first load in the dataset:\n\nfrs_data &lt;- read.csv(\"../data/FamilyResourceSurvey/FRS16-17_labels.csv\")\n\nRecall in previous weeks, we used the following code to overview the dataset. Familiar yourself again by using them:\n\nView(frs_data)\nglimpse(frs_data)\n\nand also summary() to produce summaries of each variable\n\nsummary(frs_data)\n\nYou may notice that for the numeric variables such as hh_income_gross (household gross income) and work_hours(worked hours per week), the summary() offers useful descriptive statistics. While for the qualitative information, such as age_group (age group), highest_qual (Highest educational qualification), marital_status (Marital status) and nssec (Socio-economic status), the summary() function is not that useful by providing mean or median values.\nPerforming descriptive analysis for categorical variables or qualitative variables, we focus on summarising the frequency and distribution of categories within the variable. This analysis helps understand the composition and diversity of categories in the data, which is especially useful for identifying patterns, common categories, or potential data imbalances.\n\n# Frequency count\ntable(frs_data$age_group)\n\n\n  0-4 05-10 11-15 16-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 \n 2914  3575  2599  1858  1929  2353  2800  2840  2790  2883  2975  2767  2775 \n65-69 70-74   75+ \n 2990  2354  3743 \n\ntable(frs_data$highest_qual)\n\n\nA-level or equivalent       Degree or above       Dependent child \n                 5260                  9156                 10298 \n   GCSE or equivalent             Not known                 Other \n                 9729                  6820                  2882 \n\ntable(frs_data$marital_status)\n\n\n                          Cohabiting Divorced/civil partnership dissolved \n                                4015                                 2199 \n           Married/Civil partnership                            Separated \n                               18195                                  747 \n                              Single                              Widowed \n                               16663                                 2326 \n\ntable(frs_data$nssec)\n\n\n                              Dependent child \n                                        10299 \n                            Full-time student \n                                          963 \n              Higher professional occupations \n                                         3004 \n                     Intermediate occupations \n                                         4372 \n          Large employers and higher managers \n                                         1025 \nLower managerial and professional occupations \n                                         8129 \n  Lower supervisory and technical occupations \n                                         2400 \n         Never worked or long-term unemployed \n                                         1516 \n                             Not classifiable \n                                          107 \n                          Routine occupations \n                                         4205 \n                     Semi-routine occupations \n                                         5226 \n      Small employers and own account workers \n                                         2899 \n\n\nBy using ggplot2, it is easy to create some nice descriptive charts for the categorical variables, such like what you did for the continuous variables last week.\n\nggplot(frs_data, aes(x = highest_qual)) +\n  geom_bar(fill=\"brown\",width=0.5) +\n  labs(title = \"Histogram of Highest Qualification in FRS\", x = \"Highest Qualification\", y = \"Count\")+#set text info\n  theme_classic()#choose theme type, try theme_bw(), theme_minimal() see differences\n\n\n\n\n\n\n\n\n\nggplot(frs_data, aes(x = health)) +\n  geom_bar(fill=\"skyblue\") +\n  geom_text(stat = \"count\", aes(label = ..count..),vjust = -0.3,colour = \"grey\")+ #add text\n  labs(title = \"Histogram of Health in FRS\", x = \"Health\", y = \"Count\")+#set text info\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(frs_data, aes(x = nssec)) + \n  geom_bar(fill = \"yellow4\") + \n  labs(title = \"Histogram of NSSEC in FRS\", x = \"NSSEC\", y = \"Count\") +\n  coord_flip()+ #Flip the Axes, add a # in front of this line, to make the code in gray and you will see why we would better flip the axes at here\n  theme_bw() \n\n\n\n\n\n\n\n\nIf we want to reorder the Y axis by from highest to lowest, we use the functions in forcats library. fct_infreq(): orders by the value’s frequency of the variable nssec. fct_rev(): reverses the order to go from highest to lowest.\n\nggplot(frs_data, aes(x = fct_rev(fct_infreq(nssec)))) + \n  geom_bar(fill = \"yellow4\") + \n  labs(title = \"Histogram of NSSEC in FRS\", x = \"NSSEC\", y = \"Count\") +\n  coord_flip()+ #Flip the Axes, add a # in front of this line, to make the code in gray and you will see why we would better flip the axes at here\n  theme_bw() \n\n\n\n\n\n\n\n\nYou can change the variables in ggplot() to make your own histogram chart for the variables you are interested in. You will learn more of visualisation methods in Week11’s practical.\n\n\n3.1.2 Correlation\n\nQ1. Which of the associations do you think is strongest? Which is the weakest?\n\nAs before, rather than relying upon an impressionistic view of the strength of the association between two variables, we can measure that association by calculating the relevant correlation coefficient.\nTo calculate the correlation between categorical data, we first use Chi-squared test to assess the independence between pairs of categorical variables, then we use Cramer’s V to measures the strength of association - the correlation coefficents in R.\nPearson’s chi-squared test (χ2) is a statistical test applied to sets of categorical data to evaluate how likely it is that any observed difference between the sets arose by chance. If the p-value is low (typically &lt; 0.05), it suggests a significant association between the two variables.\n\nchisq.test(frs_data$health,frs_data$happy) \n\nWarning in chisq.test(frs_data$health, frs_data$happy): Chi-squared\napproximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  frs_data$health and frs_data$happy\nX-squared = 45594, df = 60, p-value &lt; 2.2e-16\n\n\nIf you see a warning message of Chi-squared approximation may be incorrect. This is because some expected frequencies in one or more cells of the cross-tabular (health * happy) are too low. The df means degrees of freedom and it related to the size of the table and the number of categories in each variable. The most important message from the output is the estimated p-value, which shows as p-value &lt; 2.2e-16 (2.2 with 16 decimals move to the left, it is a very small number so written in scientific notation). P-value of the chi-squared test is far smaller than 0.05, so we can say the correlation is statistically significant.\nCramér’s V is a measure of association for categorical (nominal or ordinal) data. It ranges from 0 (no association) to 1 (strong association). The main downside of using Cramer’s V is that no information is provided on whether the correlation is positive or negative. This is not a problem if the variable pair includes a nominal variable but represents an information loss if the both variables being correlated are ordinal.\n\n# Install the 'vcd' package if not installed \nif(!require(\"vcd\"))   \ninstall.packages(\"vcd\", repos = \"https://cran.r-project.org\", dependencies = T)\n\nLoading required package: vcd\n\n\nWarning: package 'vcd' was built under R version 4.3.3\n\n\nLoading required package: grid\n\nlibrary(vcd)  \n\n# creat the crosstable \ncrosstab &lt;- table(frs_data$health, frs_data$happy)\n\n# Calculate Cramér's V \nassocstats(crosstab)\n\n                   X^2 df P(&gt; X^2)\nLikelihood Ratio 54036 60        0\nPearson          45594 60        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.713 \nCramer's V        : 0.454 \n\n#you can also directly calculate the assoication between variables\nassocstats(table(frs_data$health, frs_data$age_group))\n\n                   X^2 df P(&gt; X^2)\nLikelihood Ratio 26557 75        0\nPearson          23854 75        0\n\nPhi-Coefficient   : NA \nContingency Coeff.: 0.592 \nCramer's V        : 0.329 \n\n\n\nResearch Question 1. Which of our selected person-level variables is most strongly correlated with an individual’s health status?\n\nUse the codes of Chi-test and Cramer’s V to answer this question by completing Table 1.\nTable 1 Person-level correlations with health status\n\n\n\n\n\n\n\n\n\nCovariates\n\nCorrelation Coefficient\nStatistical Significance\n\n\n\n\nCramer’s V\np-value\n\n\nhealth\nage_group\n\n\n\n\nHealth\nhighest_qual\n\n\n\n\nhealth\nmarital_status\n\n\n\n\nHealth\nnssec",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html#implementing-a-linear-regression-model-with-a-qualitative-independent-variable",
    "href": "labs/03.QualitativeVariable.html#implementing-a-linear-regression-model-with-a-qualitative-independent-variable",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "3.2 Implementing a linear regression model with a qualitative independent variable",
    "text": "3.2 Implementing a linear regression model with a qualitative independent variable\n\nResearch Question 2: How does health vary across regions in the UK?\n\nThe practical is split into two main parts. The first focuses on implementing a linear regression model with a qualitative independent variable. Note that you need first to set the reference category (baseline) as the outcomes of the model reflects the differences between categories with the baseline. The second part focuses prediction based the estimated linear regression model.\nFirst we load the UK district-level census dataset.\n\n# load data\nLAcensus &lt;- read.csv(\"../data/Census2011/UK_DistrictPercentages.csv\") # Local authority level\n\nUsing the district-level census dataset “UK_DistrictPercentages.csv”. the variable “Region” (labelled as Government Office Region) is used to explore regional inequality in health.\nFamiliar yourself with the dataset by using the same codes as last week:\n\n#view the data \nView(LAcensus)  \nglimpse(LAcensus)\n\nThe names() function returns all the column names.\n\nnames(df)\n\nThe dim() function can merely returns the number of rows and number of columns.\n\ndim(LAcensus) \n\n[1] 406 128\n\n\nThere are 406 rows and 130 columns in the dataset. It would be very hard to scan throught the data if we use so many variables altogether. Therefore, we can select several columns to tailor for this practical. You can of course include other variables you are interested in also by their names:\n\ndf &lt;- LAcensus %&gt;% select(c(\"pct_Long_term_ill\",\n                            \"pct_No_qualifications\",\n                            \"pct_Males\",\n                            \"pct_Higher_manager_prof\",\n                            \"Region\"))\n\nSimply descriptive of this new data\n\nsummary(df)\n\n pct_Long_term_ill pct_No_qualifications   pct_Males    \n Min.   :11.20     Min.   : 6.721        Min.   :47.49  \n 1st Qu.:15.57     1st Qu.:19.406        1st Qu.:48.67  \n Median :18.41     Median :23.056        Median :49.09  \n Mean   :18.27     Mean   :23.257        Mean   :49.10  \n 3rd Qu.:20.72     3rd Qu.:26.993        3rd Qu.:49.48  \n Max.   :27.97     Max.   :40.522        Max.   :55.47  \n pct_Higher_manager_prof     Region      \n Min.   : 4.006          Min.   : 1.000  \n 1st Qu.: 7.664          1st Qu.: 3.000  \n Median : 9.969          Median : 6.000  \n Mean   :10.747          Mean   : 6.034  \n 3rd Qu.:12.986          3rd Qu.: 8.000  \n Max.   :37.022          Max.   :12.000  \n\n\nNow we can retrieve the “Region” column from the data frame by simply use df$Region. But what if we want to understand the data better, like the following questions?\n\nQ2. How many categories do the variable “Region” entail? How many local authority districts does each region include?\n\nSimply use the function table() would return you the answer.\n\ntable(df$Region) \n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 \n40 47 33 12 39 67 37 30 21 22 32 26 \n\n\nThe numbers in Region column indicate different regions in the UK - 1: East Midlands; 2: East of England; 3: London; 4: North East; 5: North West; 6: South East; 7: South West; 8: West Midlands; 9: Yorkshire and the Humber; 10: Wales; 11: Scotland; and 12: Northern Ireland.\nThe table() function tells us that this data frame contains 12 regions, and the number of LAs belongs to each region.\nNow, for better interpration of our regions with their real name rather than the code, we can create a new column named “Region_label” by using the following code. **R can only include the categorical variables in the factor type, so we set the new column Region_label in factor()\n\ndf$Region_label &lt;- factor(df$Region,c(1:12),labels=c(\"East Midlands\",\n                                                     \"East of England\",\n                                                     \"London\",\n                                                     \"North East\",\n                                                     \"North West\",\n                                                     \"South East\",\n                                                     \"South West\",\n                                                     \"West Midlands\",\n                                                     \"Yorkshire and the Humber\",\n                                                     \"Wales\",\n                                                     \"Scotland\",\n                                                     \"Northern Ireland\")) \n\nIf you re-run the table() function, the output is now more readable:\n\ntable(df$Region_label)\n\n\n           East Midlands          East of England                   London \n                      40                       47                       33 \n              North East               North West               South East \n                      12                       39                       67 \n              South West            West Midlands Yorkshire and the Humber \n                      37                       30                       21 \n                   Wales                 Scotland         Northern Ireland \n                      22                       32                       26 \n\n\n\n3.2.1 Include the categorical variables into a regression model\nWe will continue with a very similar regression model fitted in last week that relates Percentages long-term illness (pct_Long_term_ill) to Percentages no-qualification (pct_No_qualifications), Percentage Males (pct_Males) and Percentages Higher Managerial or Professional occupation (pct_Higher_manager_prof).\nDecide which region to be set as the baseline category. The principle is that if you want to compare the (average) long term illness outcome of Region A to those of other regions, Region A should be chosen as the baseline category. For example, if you want to compare the (average) long term illness outcome of London to rest of regions in the UK, London should be selected as the baseline category.\nImplement the regression model with the newly created categorical variables - Region_label in our case. R will automatically handle the qualitative variable as dummy variables so you don’t need to concern any of that. But you need to let R knows which category of your qualitative variable is your reference category or the baseline. Here we will use London as our first go. Note: We choose London as the baseline category so the London region will be excluded in the independent variable list.\nTherefore, first, we set London as the reference:\n\ndf$Region_label &lt;- fct_relevel(df$Region_label,  \"London\")\n\nSimilar to last week, we build our linear regression model, but also include the Region_label variable into the model.\n\nmodel &lt;- lm(pct_Long_term_ill ~ pct_Males + pct_No_qualifications + pct_Higher_manager_prof + Region_label, data = df)\n\nsummary(model)\n\n\nCall:\nlm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + \n    pct_Higher_manager_prof + Region_label, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2963 -0.9090 -0.1266  0.8168  5.2821 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          41.54134    5.22181   7.955 1.95e-14 ***\npct_Males                            -0.75756    0.10094  -7.505 4.18e-13 ***\npct_No_qualifications                 0.50573    0.03062  16.515  &lt; 2e-16 ***\npct_Higher_manager_prof               0.08910    0.03674   2.426  0.01574 *  \nRegion_labelEast Midlands             1.14167    0.35015   3.260  0.00121 ** \nRegion_labelEast of England          -0.01113    0.33140  -0.034  0.97322    \nRegion_labelNorth East                2.70447    0.49879   5.422 1.03e-07 ***\nRegion_labelNorth West                2.64240    0.35468   7.450 6.03e-13 ***\nRegion_labelSouth East                0.48327    0.30181   1.601  0.11013    \nRegion_labelSouth West                2.62729    0.34572   7.600 2.22e-13 ***\nRegion_labelWest Midlands             0.91064    0.37958   2.399  0.01690 *  \nRegion_labelYorkshire and the Humber  1.03930    0.41050   2.532  0.01174 *  \nRegion_labelWales                     4.63424    0.41368  11.202  &lt; 2e-16 ***\nRegion_labelScotland                  0.46291    0.38916   1.189  0.23497    \nRegion_labelNorthern Ireland          0.55722    0.42215   1.320  0.18762    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.394 on 391 degrees of freedom\nMultiple R-squared:  0.8298,    Adjusted R-squared:  0.8237 \nF-statistic: 136.2 on 14 and 391 DF,  p-value: &lt; 2.2e-16\n\n\nYou have already learnt how to interpret the output of regression model last week: Significance (p-value), Coefficient Estimates, and Model fit (R squared and Adjusted R-squared).\n\nQ3. Relating back to this week’s lecture notes, indicate what regions have statistically significant differences in the percentage of long-term illness, compared to London?\n\nFirst, the Significance and the Coefficient Estimates. By examining the P-value, which is the last column in the output table, we can see that most of the independent variables are significant predictor of pct_Long_term_ill.\n\nSimilarly to last week, we learn that the changes in pct_No_qualifications and pct_Malesare significantly associated with changes in pct_Long_term_ill at the &lt;0.001 level (with the three asterisks *** ), which is actually an indicator of highly statistically significant, while we are less confident that the observed relationship between pct_Higher_manager_prof and pct_Long_term_ill are statistically significant (with the two asterisks **). Through their coefficient estimates, we learn that:\n\nThe association of pct_Males is negative and strong: each decrease in 1% of pct_Males is associated with an increase of 0.75% of long term illness rate in the population of UK.\nThe association of pct_No_qualifications is positive and strong: each increase in 1% of pct_No_qualifications is associated with an increase of 0.5% of long term illness rate.\nThe association of pct_Higher_manager_prof is positive but weak: each increase in 1% of pct_Higher_manager_prof is associated with an increase of 0.08% of pct_Long_term_ill.\n\nNow comes to the dummy variables (all the items starts with Region_label) created by R for our qualitative variable Region_label: Region_labelNorth East, Region_labelNorth West, Region_labelSouth West and Region_labelWales are also statistically significant at the &lt;0.001 level. The changes in Region_labelEast Midlands are significantly associated with changes in pct_Long_term_ill at the 0.001 level, while the changes in Region_labelWest Midlands and Region_labelYorkshire and the Humber are significantly associated with changes in pct_Long_term_ill at the 0.01 level. The 0.01 level suggests that it is a mild likelihood that the relationship between these independent variables and the dependent variable is not due to random change. They are just mildly statistically significant.\nThe coefficient estimates of them need to be interpreted by comparing to the reference category London. The Estimate column tells us: North East region is associated with a 2.7% higher rate of long term illness than London when the other predictors remain the same. Similarly, Wales is 4.6% higher rate of long term illness than London when the other predictors remain the same. You can draw the conclusion for the other regions in this way by using their coefficient estimate values.\nReminder: You cannot draw conclusion between North East and Wales, nor comparison between any regions beyond London. It is because the regression model is built for the comparison between regions to your reference category London. If we want to compare between North East and Wales, we need to set either of them as the reference category by using df$Region_label &lt;- fct_relevel(df$Region_label, \"North East\") or df$Region_label &lt;- fct_relevel(df$Region_label, \"Wales\").\nRegion_labelEast of England, Region_labelSouth Eest, Region_labelScotland and RegionlabelNorthern Ireland were not found to be significantly associated with pct_Long_term_ill.\n\nLast but not least, the Measure of Model Fit. The model output suggests the R-squared and Adjusted R-squared are of greater than 0.8 indicate a reasonably well fitting model. he model explains 83.0 % of the variance in the dependent variable. After adjusting for the number of independent variable, the model explains 82.4% of the variance. They two suggest a strong fit of the model.\nNow, complete the following table.\n\n\n\n\n\n\n\n\nRegion names\nHigher or lower than London\nWhether the difference is statistically significant (Yes or No)\n\n\n\n\nEast Midlands\n\n\n\n\nEast of England\n\n\n\n\nNorth East\n\n\n\n\nNorth West\n\n\n\n\nSouth East\n\n\n\n\nSouth West\n\n\n\n\nWest Midlands\n\n\n\n\nYorkshire and The Humber\n\n\n\n\nWales\n\n\n\n\nScotland\n\n\n\n\nNorthern Ireland\n\n\n\n\n\n\n\n3.2.2 Change the baseline category\nIf you would like to learn about differences in long-term illness between North East and other regions in the UK, you need to change the baseline category (from London) to the North East region (with variable name “Region_2”).\n\ndf$Region_label &lt;- fct_relevel(df$Region_label, \"North East\")\n\nThe regression model is specified again as follows:\n\nmodel1 &lt;- lm(\n  pct_Long_term_ill ~ pct_Males + pct_No_qualifications + pct_Higher_manager_prof + Region_label,\n  data = df\n)\n\nsummary(model1)\n\n\nCall:\nlm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + \n    pct_Higher_manager_prof + Region_label, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2963 -0.9090 -0.1266  0.8168  5.2821 \n\nCoefficients:\n                                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                          44.24582    5.20125   8.507 3.85e-16 ***\npct_Males                            -0.75756    0.10094  -7.505 4.18e-13 ***\npct_No_qualifications                 0.50573    0.03062  16.515  &lt; 2e-16 ***\npct_Higher_manager_prof               0.08910    0.03674   2.426 0.015738 *  \nRegion_labelLondon                   -2.70447    0.49879  -5.422 1.03e-07 ***\nRegion_labelEast Midlands            -1.56281    0.46292  -3.376 0.000809 ***\nRegion_labelEast of England          -2.71561    0.45836  -5.925 6.87e-09 ***\nRegion_labelNorth West               -0.06208    0.46209  -0.134 0.893206    \nRegion_labelSouth East               -2.22120    0.45667  -4.864 1.67e-06 ***\nRegion_labelSouth West               -0.07718    0.47482  -0.163 0.870957    \nRegion_labelWest Midlands            -1.79384    0.48230  -3.719 0.000229 ***\nRegion_labelYorkshire and the Humber -1.66517    0.50695  -3.285 0.001113 ** \nRegion_labelWales                     1.92976    0.50111   3.851 0.000137 ***\nRegion_labelScotland                 -2.24157    0.47299  -4.739 3.01e-06 ***\nRegion_labelNorthern Ireland         -2.14725    0.49296  -4.356 1.70e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.394 on 391 degrees of freedom\nMultiple R-squared:  0.8298,    Adjusted R-squared:  0.8237 \nF-statistic: 136.2 on 14 and 391 DF,  p-value: &lt; 2.2e-16\n\n\nNow it is very easy to use your model to estimate the results Y (dependent variable) by setting all the input independent variable X.\n\nobj_London &lt;- data.frame(\n  pct_Males = 49.7,\n  pct_No_qualifications = 24.3,\n  pct_Higher_manager_prof = 14.7,\n  Region_label = \"London\"\n) \nobj_NW &lt;- data.frame(\n  pct_Males = 49.8,\n  pct_No_qualifications = 23.3,\n  pct_Higher_manager_prof = 11.2,\n  Region_label = \"North West\"\n) \nobj_NE &lt;- data.frame(\n  pct_Males = 49.8,\n  pct_No_qualifications = 23.3,\n  pct_Higher_manager_prof = 11.2,\n  Region_label = \"North East\"\n  \n) \n\npredict(model1, obj_London) \n\n       1 \n17.48952 \n\npredict(model1, obj_NW) \n\n       1 \n19.23858 \n\npredict(model1, obj_NE)\n\n       1 \n19.30065 \n\n\n\n\n3.2.3 Recode the Region variable and explore regional inequality in health\nIn many real-word studies, we might not be interested in health inequality across all regions. For example, in this case study, we are interested in health inequality between London, Other regions in England, Wales, Scotland and Northern Ireland. We can achieve this by re-grouping regions in the UK based on the variable “Region”. That said, we need to have a new grouping of regions as follows:\n\n\n\nOriginal region labels\nNew region labels\n\n\nEast Midlands\nOther regions in England\n\n\nEast of England\nOther regions in England\n\n\nLondon\nLondon\n\n\nNorth East\nOther regions in England\n\n\nNorth West\nOther regions in England\n\n\nSouth East\nOther regions in England\n\n\nSouth West\nOther regions in England\n\n\nWest Midlands\nOther regions in England\n\n\nYorkshire and The Humber\nOther regions in England\n\n\nWales\nWales\n\n\nScotland\nScotland\n\n\nNorthern Ireland\nNorthern Ireland\n\n\n\nHere we use mutate() function in R to make it happen:\n\ndf &lt;- df %&gt;% mutate(New_region_label = fct_other(\n  Region_label,\n  keep = c(\"London\", \"Wales\", \"Scotland\", \"Northern Ireland\"),\n  other_level = \"Other regions in England\"\n))\n\nThis code may looks a bit complex. You can simply type ?mutate in your console. Now in your right hand Help window, the R studio offers your the explanation of the mutate function. This is a common way you can use R studio to help you learn what the function caate() creates new columns that are functions of existing variables. Therefore, the df %&gt;% mutate() means add a new column into the current dataframe df; the New_region_label in the mutate() function indicates the name of this new column is New_region_label. The right side of the New_region_label = indicates the value we want to assign to the New_region_label in each row.\nThe right side of New_region_label is\nfct_other(Region_label, keep=c(\"London\",\"Wales\",\"Scotland\",\"Northern Ireland\"), other_level=\"Other regions in England\")\nBy using the code, the fct_other() function checks whether each value in the Region_label column is one of the keep regions: “London”, “Wales”, “Scotland”, or “Northern Ireland”. If the region is not in this list, the value is replaced with the label “Other regions in England”. If the region is one of these four, the original value in Region_label is kept. This process categorizes regions that are outside of the four specified ones into a new group labeled “Other regions in England”, while preserving the original labels for the specified regions.\nNow we use the same way to examine our new column New_region_label:\n\ntable(df$New_region_label)\n\n\n                  London                    Wales                 Scotland \n                      33                       22                       32 \n        Northern Ireland Other regions in England \n                      26                      293 \n\n\nComparing with the Region_label, we now can see the mutate worked:\n\ndf[,c(\"Region_label\",\"New_region_label\")]\n\nNow you will have a new qualitative variable named New_region_label in which the UK is divided into five regions: London, Other regions in England, Wales, Scotland and Northern Ireland.\nBased on the newly generated qualitative variable New_region_label, we can now build our new linear regression model. Don’t forget:\n(1) R need to deal with the categorical variables in regression model in the factor type;\n\nclass(df$New_region_label)\n\n[1] \"factor\"\n\n\nThe class() returns the type of the variable. The New_region_label is already a factor variable. If not, we need to convert it by the as.factor(), as we used above.\n\ndf$New_region_label = as.factor(df$New_region_label)\n\n2) Let R know which region you want to use as the baseline category. Here I will use London again, but of course you can choose other regions.\n\ndf$New_region_label &lt;- fct_relevel(df$New_region_label, \"London\")\n\nThe linear regression window is set up below. This time we include New_region_label rather than Region_label as the region variable:\n\nmodel2 &lt;- lm(\n  pct_Long_term_ill ~ pct_Males + pct_No_qualifications + pct_Higher_manager_prof + New_region_label,\n  data = df\n)\n\nsummary(model2)\n\n\nCall:\nlm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + \n    pct_Higher_manager_prof + New_region_label, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.6719 -1.1252 -0.0556  0.9564  7.3768 \n\nCoefficients:\n                                          Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                              47.217525   5.795439   8.147 4.86e-15\npct_Males                                -0.834471   0.113398  -7.359 1.07e-12\npct_No_qualifications                     0.472354   0.032764  14.417  &lt; 2e-16\npct_Higher_manager_prof                   0.000497   0.040851   0.012 0.990300\nNew_region_labelWales                     4.345262   0.471088   9.224  &lt; 2e-16\nNew_region_labelScotland                  0.143672   0.442989   0.324 0.745863\nNew_region_labelNorthern Ireland          0.264425   0.474074   0.558 0.577314\nNew_region_labelOther regions in England  1.071719   0.312746   3.427 0.000674\n                                            \n(Intercept)                              ***\npct_Males                                ***\npct_No_qualifications                    ***\npct_Higher_manager_prof                     \nNew_region_labelWales                    ***\nNew_region_labelScotland                    \nNew_region_labelNorthern Ireland            \nNew_region_labelOther regions in England ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.609 on 398 degrees of freedom\nMultiple R-squared:  0.7693,    Adjusted R-squared:  0.7653 \nF-statistic: 189.6 on 7 and 398 DF,  p-value: &lt; 2.2e-16\n\n\n\nQ4. Are there statistically significant differences in the percentage of people with long-term illness between London and Scotland, and between London and Wales, controlling for other variables? What conclusions could be drawn in terms of regional differences in health outcome?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html#predictions-using-fitted-regression-model",
    "href": "labs/03.QualitativeVariable.html#predictions-using-fitted-regression-model",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "3.3 Predictions using fitted regression model",
    "text": "3.3 Predictions using fitted regression model\n\n3.3.1 Write down the % illness regression model with the new region label categorical variables\nRelating to this week’s lecture, the % pct_Long_term_ill is equal to:\n[write down the model]\n\nQ5. Now imagine that the values of variables pct_Males, pct_No_qualifications, and pct_Higher_manager_prof are 49, 23 and 11, respectively, what would the percentage of persons with long-term illness in Wales and London be?\n\nCheck the answer at the end of this practical page.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html#income-inequality-with-respect-to-gender-and-health-status",
    "href": "labs/03.QualitativeVariable.html#income-inequality-with-respect-to-gender-and-health-status",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "3.4 Income inequality with respect to gender and health status",
    "text": "3.4 Income inequality with respect to gender and health status\nIn this section, we will work with individual-level data (“FRS 2016-17_label.csv”) again to explore income inequality with respect to gender and health status.\nTo explore income inequality, we need to work with a data set excluding dependent children. In addition, we look at individuals who are the representative persons of households. Therefore, we will select cases (or samples) that meet both conditions.\nWe want R to select persons only if they are the representative persons of households and they are not dependent children. The involved variables are hrp and Dependent for the categories “Household Reference Person” and “independent”, you can select the appropriate cases. We also want to exclude the health variable reported as “Not known”.\n\nfrs_df &lt;- frs_data %&gt;% filter(hrp == \"HRP\" &\n                                dependent == \"Independent\" & health != \"Not known\") \n\nThen, we create a new numeric variable Net_inc_perc indicate net income per capita as our dependent variable:\n\nfrs_df$Net_inc_pp = frs_df$hh_income_net / frs_df$hh_size\n\nsummary(frs_df$Net_inc_pp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-238160    9074   13347   15834   19136  864812 \n\n\nThe distribution of the net household income per capita can be visualised by using ggplot()\n\nggplot(frs_df, aes(x = Net_inc_pp)) +\n  geom_histogram(\n    bins = 1000,\n    color = \"black\",\n    fill = \"skyblue\",\n    alpha = 0.7\n  ) +\n  labs(title = \"Distribution of Net_inc_pp\", x = \"Value\", y =  \"Frequency\") +\n  scale_x_continuous(labels = scales::label_comma()) +  # Prevent scientific notation on the x axis\n  theme_minimal()\n\n\n\n\n\n\n\n\nOur two qualitative independent variables “sex” and “health”. Let’s first know what they look like:\n\ntable(frs_df$sex)\n\n\nFemale   Male \n  7647   9180 \n\ntable(frs_df$health)\n\n\n      Bad      Fair      Good  Very Bad Very Good \n     1472      4253      6277       426      4399 \n\n\nRemember what we did in the Region long-term illness practical previously before we put the qualitative variable into the regression model? Yes. First, make sure they are in factor type and Second, decide the reference category. Here, I will use Female and Very Bad health status as my base categories. You can decide what you wish to use. This time, I use the following codes to combine these two steps in one line.\n\nfrs_df$sex &lt;- fct_relevel(as.factor(frs_df$sex), \"Female\")\nfrs_df$health &lt;- fct_relevel(as.factor(frs_df$health), \"Very Bad\")\n\nImplement the regression model with the two qualitative independent variables.\n\nmodel_frs &lt;- lm(Net_inc_pp ~ sex + health, data = frs_df)\nsummary(model_frs)\n\n\nCall:\nlm(formula = Net_inc_pp ~ sex + health, data = frs_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-255133   -6547   -2213    3515  845673 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      12115.5      762.9  15.881  &lt; 2e-16 ***\nsexMale           2091.2      240.6   8.691  &lt; 2e-16 ***\nhealthBad         -102.8      854.3  -0.120 0.904205    \nhealthFair        1051.3      789.0   1.332 0.182751    \nhealthGood        2766.0      777.4   3.558 0.000375 ***\nhealthVery Good   4931.8      787.8   6.260 3.95e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15530 on 16821 degrees of freedom\nMultiple R-squared:  0.01646,   Adjusted R-squared:  0.01616 \nF-statistic: 56.29 on 5 and 16821 DF,  p-value: &lt; 2.2e-16\n\n\nThe result can be formatted by:\n\nlibrary(broom)\ntidy(model_frs)\n\n# A tibble: 6 × 5\n  term            estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)       12116.      763.    15.9   2.21e-56\n2 sexMale            2091.      241.     8.69  3.90e-18\n3 healthBad          -103.      854.    -0.120 9.04e- 1\n4 healthFair         1051.      789.     1.33  1.83e- 1\n5 healthGood         2766.      777.     3.56  3.75e- 4\n6 healthVery Good    4932.      788.     6.26  3.95e-10\n\n\n\nQ6. What conclusions could be drawn in terms of income inequalities with respect to gender and health status? Also think about the statistical significance of these differences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/03.QualitativeVariable.html#extension-activities",
    "href": "labs/03.QualitativeVariable.html#extension-activities",
    "title": "3  Lab: Correlation and Multiple Linear Regression with Qualitative Variables",
    "section": "3.5 Extension activities",
    "text": "3.5 Extension activities\nThe extension activities are designed to get yourself prepared for the Assignment 2 in progress. For this week, try whether you can:\n\nPresent descriptive statistics for independent variable and the dependent variable: counts, percentages, a centrality measure, a spread measure, histograms or any relevant statistic\nReport the observed association between the dependent and independent variables: correlation plus a graphic or tabular visualisation\nBriefly describe and critically discuss the results\nThink about other potential factors of long-term illness and income, and then test your ideas with linear regression models\nSummaries your model outputs and interpret the results.\n\nAnswer of the written down model and Q5\nThe model of the new region label is: pct_Long_term_ill (%) = 47.218+ (-0.834)* pct_Males (%) + 0.472 * pct_No_qualifications (%) + 1.072*Other Regions in England + 4.345* Wales\nSo if the values of variables pct_Males, pct_No_qualifications, and pct_Higher_manager_prof are 49, 23 and 11,\nthe model of Wales will be: pct_Long_term_ill (%) = 47.218+ (-0.834)* 49 + 0.472 * 23 + 1.072*0+ 4.345* 1 = 21.553 (you can direct paste the number sentence into your R studio Console and the result will be returned)\nthe model of London will be: pct_Long_term_ill (%) = 47.218+ (-0.834)* 49 + 0.472 * 23 + 1.072*0+ 4.345* 0 = 17.208\nYou can also make a new object like\n\nobj_London &lt;- data.frame(\n  pct_Males = 49,\n  pct_No_qualifications = 23,\n  pct_Higher_manager_prof = 11,\n  New_region_label = \"London\"\n)\nobj_Wales &lt;- data.frame(\n  pct_Males = 49,\n  pct_No_qualifications = 23,\n  pct_Higher_manager_prof = 11,\n  New_region_label = \"Wales\"\n)\npredict(model2, obj_London)\n\n       1 \n17.19808 \n\npredict(model2, obj_Wales)\n\n       1 \n21.54334 \n\n\nTherefore, the percentage of persons with long-term illness in Wales and London be 21.5% and 17.2% separately. If you got the right answers, then congratulations you can now use regression model to make prediction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation and Multiple Linear Regression with Qualitative Variables</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html",
    "href": "labs/02.MultipleLinear.html",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "2.1 Part I. Correlation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-i.-correlation",
    "href": "labs/02.MultipleLinear.html#part-i.-correlation",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "2.1.1 Data Overview: Descriptive Statistics:\nLet’s start by picking one dataset derived from the English-Wales 2021 Census data. You can choose one dataset that aggregates data either at a) county, b) district, or c) ward-level. Lower Tier Local Authority-, Region-, and Country-level data is also available in the data folder.\nsee also: https://canvas.liverpool.ac.uk/courses/77895/pages/census-data-2021\n\n# Load necessary libraries \nlibrary(ggplot2) \n\nWarning: package 'ggplot2' was built under R version 4.3.2\n\nlibrary(dplyr) \n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\noptions(scipen = 999, digits = 4)  # Avoid scientific notation and round to 4 decimals globally\n\n# load data\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\") # District level\n\nWe’re using a (district/ward/etc.)-level census dataset that includes:\n\n% of population with poor health (variable name: pct_Very_bad_health).\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nFirst, let’s get some descriptive statistics that help identify general trends and distributions in the data.\n\n# Summary statistics\nsummary_data &lt;- census %&gt;%\n  select(pct_Very_bad_health, pct_No_qualifications, pct_Males, pct_Higher_manager_prof) %&gt;%\n  summarise_all(list(mean = mean, sd = sd))\nsummary_data\n\n  pct_Very_bad_health_mean pct_No_qualifications_mean pct_Males_mean\n1                    1.173                       17.9          48.97\n  pct_Higher_manager_prof_mean pct_Very_bad_health_sd pct_No_qualifications_sd\n1                        13.22                 0.3402                    3.959\n  pct_Males_sd pct_Higher_manager_prof_sd\n1       0.6603                       4.73\n\n\nQ1. Complete the table below by specifying each variable type (continuous or categorical) and reporting its mean and standard deviation.\n\n\n\n\n\n\n\n\n\nVariable Name\nType (Continuous or Categorical)\nMean\nStandard Deviation\n\n\n\n\npct_Very_bad_health\n\n\n\n\n\npct_No_qualifications\n\n\n\n\n\npct_Males\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\n\n2.1.2 Simple visualisation for continuous data\nYou can visualise the relationship between two continuous variables using a scatter plot. Using the chosen census datasets, visualise the association between the % of population with bad health (pct_Very_bad_health) and each of the following:\n\nthe % of population with no qualifications (pct_No_qualifications);\nthe % of population aged 65 to 84 (pct_Age_65_to_84);\nthe % of population in a married couple (pct_Married_couple);\nthe % of population in a Higher Managerial or Professional occupation (pct_Higher_manager_prof).\n\n\n# Scatterplot for each variable variables \nvariables &lt;- c(\"pct_No_qualifications\", \"pct_Age_65_to_84\", \"pct_Married_couple\", \"pct_Higher_manager_prof\")\n\n# Loop to create scatterplots and calculate correlations \n# x and y variables for each scatter plot,\nfor (var in variables) { \n  # Scatterplot \n  ggplot(census, aes_string(x = var, y = \"pct_Very_bad_health\")) +\n    geom_point() + \n    labs(title = paste(\"Scatterplot of pct_Very_bad_health vs\", var), \n         x = var, y = \"pct_Very_bad_health\") +\n    theme_minimal()\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nQ2. Which of the associations do you think is strongest, which one is the weakest?\nAs noted, before, an observed association between two variables is no guarantee of causation. It could be that the observed association is:\n\nsimply a chance one due to sampling uncertainty;\ncaused by some third underlying variable which explains the spatial variation of both of the variables in the scatterplot;\ndue to the inherent arbitrariness of the boundaries used to define the areas being analysed (the ‘Modifiable Area Unit Problem’).\n\nQ3. Setting these caveats to one side, are the associations observed in the scatter-plots suggestive of any causative mechanisms of bad health?\nRather than relying upon an impressionistic view of the strength of the association between two variables, we can measure that association by calculating the relevant correlation coefficient. The Table below identifies the statistically appropriate measure of correlation to use between two continuous variables.\n\n\n\n\n\n\n\n\nVariable Data Type\nMeasure of Correlation\nRange\n\n\n\n\nBoth symmetrically distributed\nPearson’s\n-1 to +1\n\n\nOne or both with a skewed distribution\nSpearman’s Rank\n-1 to +1\n\n\n\nDifferent Calculation Methods: Pearson’s correlation assumes linear relationships and is suitable for symmetrically distributed (normally distributed) variables, measuring the strength of the linear relationship. Spearman’s rank correlation, however, works on ranked data, so it’s more suitable for skewed data or variables with non-linear relationships, measuring the strength and direction of a monotonic relationship.\nWhen calculating correlation for a single pair of variables, select the method that best fits their data distribution:\n-   Use **Pearson’s** if both variables are symmetrically distributed.\n-   Use **Spearman’s** if one or both variables are skewed.\n\n\n\n\n\n\n\n\n\nYou can check the distribution of a variable (e.g. pct_No_qualifications like this):\n\n# Plot histogram with density overlay for a chosen variable (e.g., 'pct_No_qualifications')\nggplot(census, aes(x = pct_No_qualifications)) + \n    geom_histogram(aes(y = after_stat(density)), bins = 30, color = \"black\", fill = \"skyblue\", alpha = 0.7) +\n    geom_density(color = \"darkblue\", linewidth = 1) +\n    labs(title = \"Distribution of pct_No_qualifications\", x = \"Value\", y =  \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen analyzing multiple pairs of variables, using different measures (Pearson for some pairs, Spearman for others) creates inconsistencies since Pearson and Spearman values aren’t directly comparable in size due to their different calculation methods. To maintain consistency across comparisons, calculate both Pearson’s and Spearman’s correlations for each pair, e.g. do the trends align (both showing strong, weak, or moderate correlation in the same direction)? This consistency check can give confidence that the relationships observed are not dependent on the correlation method chosen. While in a report you’d typically include only one set of correlations (usually Pearson’s if the relationships appear linear), calculating both can validate that your observations aren’t an artifact of the correlation method.\n\nResearch Question 1: Which of our selected variables are most strongly correlated with % of population with bad health?\n\nTo answer this question, complete the Table below by editing/running this code:.\nPearson correlations\n\npearson_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications,use = \"complete.obs\", method = \"pearson\")\n    \n# Display the results\ncat(\"Pearson Correlation:\", pearson_correlation, \"\\n\")\n\nPearson Correlation: 0.7621 \n\n\nSpearman correlations:\n\nspearman_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications, use = \"complete.obs\", method = \"spearman\")\n\ncat(\"Spearman Correlation:\", spearman_correlation, \"\\n\")\n\nSpearman Correlation: 0.7785 \n\n\n\n\n\nCovariates\nPearson\nSpearman\n\n\n\n\npct_Very_bad_health - pct_No_qualifications\n\n\n\n\npct_Very_bad_health - pct_Age_65_to_84\n\n\n\n\npct_Very_bad_health - pct_Married_couple\n\n\n\n\npct_Very_bad_health - pct_Higher_manager_prof\n\n\n\n\n\nWhat can you make of this numbers?\nIf you think you have found a correlation between two variables in our dataset, this doesn’t mean that an association exists between these two variables in the population at large. The uncertainty arises because, by chance, the random sample included in our dataset might not be fully representative of the wider population.\nFor this reason, we need to verify whether the correlation is statistically significant,\n\n# significance test for pearson, for example\npearson_test &lt;- cor.test(census$pct_Very_bad_health,\n    census$pct_No_qualifications, method = \"pearson\", use = \"complete.obs\")\npearson_test\n\n\n    Pearson's product-moment correlation\n\ndata:  census$pct_Very_bad_health and census$pct_No_qualifications\nt = 21, df = 329, p-value &lt;0.0000000000000002\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7128 0.8038\nsample estimates:\n   cor \n0.7621 \n\n\nLook at https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor.test for details about the function. But in general, when calculating the correlation between two variables, a p-value accompanies the correlation coefficient to indicate the statistical significance of the observed association. This p-value tests the null hypothesis that there is no association between the two variables (i.e., that the correlation is zero).\nWhen interpreting p-values, certain thresholds denote different levels of confidence. A p-value less than 0.05 is generally considered statistically significant at the 95% confidence level, suggesting that we can be 95% confident there is an association between the variables in the broader population. When the p-value is below 0.01, the result is significant at the 99% confidence level, meaning we have even greater confidence (99%) that an association exists. Sometimes, on research papers or tables significance levels are denoted with asterisks: one asterisk (*) typically indicates significance at the 95% level (p &lt; 0.05), two asterisks (**) significance at the 99% level (p &lt; 0.01), three asterisks (***) significance at the 99.99% level (p &lt; 0.01).\nTypically, p-values are reported under labels such as “Sig (2-tailed),” where “2-tailed” refers to the fact that the test considers both directions (positive and negative correlations). Reporting the exact p-value (e.g., p = 0.002) is more informative than using thresholds alone, as it gives a clearer picture of how strongly the data contradicts the null hypothesis of no association.\nIn a nutshell, lower p-values suggest a stronger statistical basis for believing that an observed correlation is not due to random chance. A statistically significant p-value reinforces confidence that an association is likely to exist in the wider population, though it does not imply causation.\n\n\n2.1.3 Part. 2: Implementing a Linear Regression Model\nA key goal of data analysis is to explore the potential factors of health at the local district level. So far, we have used cross-tabulations and various bivariate correlation analysis methods to explore the relationships between variables. One key limitation of standard correlation analysis is that it remains hard to look at the associations of an outcome/dependent variable to multiple independent/explanatory variables at the same time. Regression analysis provides a very useful and flexible methodological framework for such a purpose. Therefore, we will investigate how various local factors impact residents’ health by building a multiple linear regression model in R.\nWe use pct_Very_bad_health as a proxy for residents’ health.\n\nResearch Question 2: How do local factors affect residents’ health?\n\nDependent (or Response) Variable:\n\n% of population with bad health (pct_Very_bad_health).\n\nIndependent (or Explanatory) Variables:\n\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nLoad some other Libraries\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'tibble' was built under R version 4.3.2\n\n\nWarning: package 'tidyr' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.3.2\n\n\nand the data (if not loaded):\n\n# Load dataset\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\")\n\nRegression models are the standard method for constructing predictive and explanatory models. They tell us how changes in one variable (the target variable or independent variable, \\(Y\\)) are associated with changes in explanatory variables, or dependent variables, \\(X_1, X_2, X_3\\) (\\(X_n\\)), etc. Classic linear regression is referred to Ordinary least squares (OLS) regression because they estimate the relationship between one or more independent variables and a dependent variable \\(Y\\) using a hyperplane (i.e. a multi-dimensional line) that minimises the sum of the squared difference between the observed values of \\(Y\\) and the values predicted by the model (denoted as \\(\\hat{Y}\\), \\(Y\\)-hat).\nHaving seen Single Linear Regression in class - where the relationship between one independent variable and a dependent variable is modeled - we can extend this concept to situations where more than one explanatory variable might influence the outcome. While single linear regression helps us understand the effect of ONE variable in isolation, real-world phenomena are often influenced by multiple factors simultaneously. Multiple linear regression addresses this complexity by allowing us to model the relationship between a dependent variable and multiple independent variables, providing a more comprehensive view of how various explanatory variables contribute to changes in the outcome.\nHere, regression allows us to examine the relationship between people’s health rates and multiple dependent variables.\nBefore starting, we define two hypotheses:\n\nNull hypothesis (\\(H_0\\)): For each variable \\(X_n\\), there is no effect of \\(X_n\\) on \\(Y\\).\nAlternative hypothesis (\\(H_1\\)): There is an effect of\\(X_n\\) on \\(Y\\).\n\nWe will test if we can reject the null hypothesis.\n\n\n2.1.4 Model fit\n\n# Linear regression model\nmodel &lt;- lm(pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof, data = census)\nsummary(model)\n\n\nCall:\nlm(formula = pct_Very_bad_health ~ pct_No_qualifications + pct_Males + \n    pct_Higher_manager_prof, data = census)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4903 -0.1369 -0.0352  0.0983  0.7658 \n\nCoefficients:\n                        Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)              4.01799    0.88004    4.57            0.0000071 ***\npct_No_qualifications    0.05296    0.00591    8.96 &lt; 0.0000000000000002 ***\npct_Males               -0.07392    0.01785   -4.14            0.0000440 ***\npct_Higher_manager_prof -0.01309    0.00494   -2.65               0.0084 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.213 on 327 degrees of freedom\nMultiple R-squared:  0.61,  Adjusted R-squared:  0.607 \nF-statistic:  171 on 3 and 327 DF,  p-value: &lt;0.0000000000000002\n\n\nCode explanation\nlm() Function:\n\nlm() stands for “linear model” and is used to fit a linear regression model in R.\nThe formula syntax pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof specifies a relationship between:\n\nDependent Variable: pct_Very_bad_health.\nIndependent Variables: pct_No_qualifications, pct_Males, and pct_Higher_manager_prof. The model is trained on the data dataset.\n\n\nStoring the Model: The model &lt;- syntax stores the fitted model in an object called model.\nsummary(model) provides a detailed output of the model’s results, including:\n\nCoefficients: Estimates of the regression slopes (i.e., how each independent variableaffects pct_Very_bad_health).\nStandard Errors: The variability of each coefficient estimate.\nt-values and p-values: Indicate the statistical significance of the effect of each independent (explanatory) variable.\n\nR-squared and Adjusted R-squared: Show how well the independent variables explain the variance in the dependent variable.\nF-statistic: Tests the overall significance of the model.\n\nWe can focus only on certain output metrics:\n\n# Regression coefficients\ncoefficients &lt;- tidy(model)\ncoefficients\n\n# A tibble: 4 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.02     0.880        4.57 7.06e- 6\n2 pct_No_qualifications     0.0530   0.00591      8.96 2.54e-17\n3 pct_Males                -0.0739   0.0179      -4.14 4.40e- 5\n4 pct_Higher_manager_prof  -0.0131   0.00494     -2.65 8.43e- 3\n\n\nThese are:\n\nRegression Coefficient Estimates.\nP-values.\nAdjusted R-squared.\n\n\n\n2.1.5 How to interpret the output metrics\n\n2.1.5.1 Regression Coefficient Estimates\nThe Estimate column in the output table tells us the rate of change between each dependent variable \\(X_n\\) and \\(Y\\).\nIntercept: In the regression equation, this is \\(β_0\\) and it indicates the value of \\(Y\\) when \\(X_n\\) are equal to zero.\nSlopes: These are the other regression coefficients of an independent variable, e.g. \\(β_1\\), i.e. estimated average changes in \\(Y\\) for a one unit change in an independent variable, e.g. \\(X_1\\), when all other dependent or explanatory variables are held constant.\nThere are two key points worth mentioning:\n\nThe unit of \\(X\\) and \\(Y\\): you need to know what the units are of the independent and dependent variables. For instance, one unit could be one year if you have an age variable, or a one percentage point if the variable is measured in percentages (all the variables in this week’s practical).\nAll the other explanatory variables are held constant. It means that the coefficient of an explanatory variable \\(X_1\\) (e.g. \\(β_1\\)) should be interpreted as: a one unit change in \\(X_1\\) is associated with \\(β_1\\) units change in \\(Y\\), keeping other values of explanatory variables (e.g. \\(X_2\\), \\(X_3\\)) constant – for instance, \\(X_2\\)= 0.1 or \\(X_3\\)= 0.4.\n\nFor the independent variable \\(X\\), we can derive how changes of 1 unit for the independent are associated with the changes in pct_Very_bad_health, for example:\n\nThe association of pct_No_qualifications is positive and strong: each increase in 1% of pct_No_qualifications is associated with an increase of 0.05% of very bad health rate.\nThe association of pct_Males is negative and strong: each decrease in 1% of pct_Males is associated with an increase of 0.07% of pct_Very_bad_health in the population in England and Wales.\nThe association of pct_Higher_manager_prof is negative but weak: each decrease in 1% of pct_Higher_manager_prof is associated with an increase of 0.013% of pct_Very_bad_health.\n\n\n\n2.1.5.2 P-values and Significance\nThe t tests of regression coefficients are used to judge the statistical inferences on regression coefficients, i.e. associations between independent variables and the outcome variable. For a t-statistic of a dependent variable, there is a corresponding p-value that indicates different levels of significance in the column Pr(&gt;|t|) and the asterisks ∗.\n\n*** indicates “changes in \\(X_n\\) are significantly associated with changes in \\(Y\\) at the &lt;0.001 level”.\n** suggests that “changes in \\(X_n\\) are significantly associated with changes in \\(Y\\) between the 0.001 and (&lt;) 0.01 levels”.\nNow you should know what * means: The significance is between the 0.01 and 0.05 levels, which means that we observe a less significant (but still significant) relationship between the variables.\n\nP-value provide a measure of how significant the relationship is; it is an indication of whether the relationship between \\(X_n\\) and \\(Y\\) found in this data could have been found by chance. Very small p-values suggest that the level of association found here might not have come from a random sample of data.\nIn this case, we can say:\n\nGiven that the p-value is indicated by ***, changes in pct_No_qualifications and pct_Males are significantly associated with changes in pct_Very_bad_health at the &lt;0.001 level; the association is highly statistically significant; we can be confident that the observed relationship between these variables and pct_Very_bad_health is not due to chance.\nGiven that the p-value is indicated by **, changes in pct_Higher_manager_prof are significantly associated with changes in pct_Very_bad_health at the 0.001 level. This means that the association between the independent and dependent variable is not one that would be found by chance in a series of random sample 99.999% of the time.\n\nIn both cases we can then confidently reject the Null hypothesis (\\(H_0\\): no association between dependent and independent variables exist).\nRemember, If the p-value of a coefficient is smaller than 0.05, that coefficient is statistically significant. In this case, you can say that the relationship between this independent variable and the outcome variable is statistically significant. Contrarily, if the p-value of a coefficient is larger than 0.05 you can conclude that there is no evidence of an association or relationship between the independent variable and the outcome variable.\n\n\n2.1.5.3 R-squared and Adjusted R-squared\nThese provide a measure of model fit. They are calculated as the difference between the actual value of \\(Y\\) and the value predicted by the model. The R-squared and Adjusted R-squared values are statistical measures that indicate how well the independent variables in your model explain the variability of the dependent variable. Both R-squared and Adjusted R-squared help us understand how closely the model’s predictions align with the actual data. An R-squared of 0.6, for example, indicates that 60% of the variability in \\(Y\\) is explained by the independent variables in the model. The remaining 40% is due to other factors not captured by the model.\nAdjusted R-squared also measures the goodness of fit, but it adjusts for the number of independent variables in the model, accounting for the fact that adding more variables can artificially inflate R-squared without genuinely improving the model. This is especially useful when comparing models with different numbers of independent variables. If Adjusted R-squared is close to or above 0.6, as in your example, it implies that the model has a strong explanatory power while not being overfit with unnecessary explanatory variables.\nA high R-squared and Adjusted R-squared indicate that the model captures much of the variation in the data, making it more reliable for predictions or for understanding the relationship between \\(Y\\) and the explanatory variables. However Low R-squared values suggest (e.g. 0.15) that the model might be missing important explanatory variables or that the relationship between \\(Y\\) and the selected explanatory variables is not well-captured by a linear approach.\nAn R-squared and Adjusted R-squared over 0.6 are generally seen as signs of a well-fitting model in many fields, though the ideal values can depend on the context and the complexity of the data.\n\n\n\n2.1.6 Interpreting the Results\n\ncoefficients\n\n# A tibble: 4 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.02     0.880        4.57 7.06e- 6\n2 pct_No_qualifications     0.0530   0.00591      8.96 2.54e-17\n3 pct_Males                -0.0739   0.0179      -4.14 4.40e- 5\n4 pct_Higher_manager_prof  -0.0131   0.00494     -2.65 8.43e- 3\n\n\nQ4. Complete the table above by filling in the coefficients, t-values, p-values, and indicating if each variable is statistically significant.\n\n\n\n\n\n\n\n\n\n\nVariable Name\nCoefficients\nt-values\np-values\nSignificant?\n\n\n\n\npct_No_qualifications\n\n\n\n\n\n\npct_Males\n\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\nFrom the lecture notes, you know that the Intercept or Constant represents the estimated average value of the outcome variable when the values of all independent variables are equal to zero.\nQ5. When values of pct_Males, pct_No_qualifications and pct_Higher_manager_prof are all \\(zero\\), what is the % of population with very bad health? Is the intercept term meaningful? Are there any districts (or zones, depending on the dataset you chose) with zero percentages of persons with no qualification in your data set?\nQ6. Interpret the regression coefficients of pct_Males, pct_No_qualifications and pct_Higher_manager_prof. Do they make sense?\n\n\n2.1.7 Identify factors of % bad health\nNow combine the above two sections and identify factors affecting the percentage of population with very bad health. Fill in each row for the direction (positive or negative) and significance level of each variable.\n\n\n\n\n\n\n\n\nVariable Name\nPositive or Negative\nStatistical Significance\n\n\n\n\npct_No_qualifications\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\npct_Males\n\n\n\n\n\nQ7. Think about the potential conclusions that can be drawn from the above analyses. Try to answer the research question of this practical: How do local factors affect residents’ health? Think about causation vs association and consider potential confounders when interpreting the results. How could these findings influence local health policies?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "href": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "2.2 Part C: Practice and Extension",
    "text": "2.2 Part C: Practice and Extension\nIf you haven’t understood something, if you have doubts, even if they seem silly, ask.\n\nFinish working through the practical.\nRevise the material.\nExtension activities (optional): Think about other potential factors of very bad health and test your ideas with new linear regression models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  }
]