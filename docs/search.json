[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the Social World - Quantitative Block: Statistics",
    "section": "",
    "text": "Welcome\nThis is the website for “Exploring the Social World - Quantitative Block: Statistics” (module ENVS225) at the University of Liverpool. This block of the module is designed and delivered by Dr. Gabriele Filomena and Dr. Zi Ye from the Geographic Data Science Lab at the University of Liverpool. The module seeks to provide hands-on experience and training in introductory statistics for human geographers.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Exploring the Social World - Quantitative Block: Statistics",
    "section": "Contact",
    "text": "Contact\n\nGabriele Filomena - gfilo [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 1xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nZi Ye - zi.ye [at] liverpool.ac.uk Lecturer in Geographic Information Science Office 107, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "general/overview.html",
    "href": "general/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aim and Learning Objectives\nThis sub-module aims to provide training and skills on a set of basic quantitative research methods for data collection, analysis, and interpretation. You will learn how to define coherent, relevant research questions, utilise various research quantitative methods, and identify appropriate methodologies to tackle your research questions. This block serves as the foundation for the dissertation and fieldwork modules.\nBackground\nData and research are key pillars of the global economy and society today. We need rigorous approaches to collecting and analysing both the statistics that can tell us ‘how much’ and if there are observable relationships between phenomena; and the information gives us a nuanced understanding of cultural contexts and human dynamics. Quantitative skills enable us to explore and measure socio-economic activities and processes at large scales, while qualitative skills enable understanding of social, cultural, and political contexts and diverse lived experiences. Rather than being in opposition, qualitative and quantitative research can complement one another in the investigation of today’s pressing research questions.\nTo these ends, this block will help you develop your quantitative (statistical) skills, as critical tools. This course will help you understand what quantitative statistical researchers use and develop a set of research techniques that can be used in your field classes and dissertations.\nLearning objectives:",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#aim-and-learning-objectives",
    "href": "general/overview.html#aim-and-learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Understand how to explore a dataset, containing a number of observations described by a set of variables.\nDemonstrate an understanding in the application and interpretation of commonly used quantitative research methods.\nDemonstrate an understanding of how to work with quantitative data to address real-world research questions.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#module-structure",
    "href": "general/overview.html#module-structure",
    "title": "Overview",
    "section": "Module Structure",
    "text": "Module Structure\nStaff: Dr Zi Ye and Dr Gabriele Filomena\nWhere and When\nQuantitative Block (Weeks 7-12):\n\nLecture: 10 am – 10.45 am Fridays\nPC Practical sessions: 11am – 1 pm, following the Lecture\n\nWeek 7: Central Teaching Hub: PC Teaching Centre BLUE+GREEN+ORANGE ZONES\nWeek 8 -12: Central Teaching Hub, PCTC\nLectures will introduce and explain the fundamentals of quantitative methods, with the opportunity to apply the method introduced in the labs later in the week.\nThe computer practical sessions, will give you the chance to use and apply quantitative methods to real-world data. These are primarily self-directed sessions, but with support on hand if you get stuck. Support and training in R will be provided through these sessions. Weekly sessions will be driven by empirical research questions.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nFormat\nStaff\n\n\n\n\n7\nIntroduction & Review\nLecture and Computer Lab Practical\nGF\n\n\n8\nSingle & Multiple Linear Regression\nLecture and Computer Lab Practical\nGF\n\n\n9\nMultiple Linear Regression with Categorical Variables\nLecture and Computer Lab Practical\nZY\n\n\n10\nLogistic Regression\nLecture and Computer Lab Practical\nZY\n\n\n11\nData Visualisation\nLecture and Computer Lab Practical\nGF\n\n\n12\nSummary and Assessment Support\nLecture and Computer Lab Practical\nZY",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#software-and-data",
    "href": "general/overview.html#software-and-data",
    "title": "Overview",
    "section": "Software and Data",
    "text": "Software and Data\nFor quantitative training sessions, ensure you have installed and/or have access to RStudio. To run the analysis and reproduce the code in R, you need the following software installed on your machine:\n\nR-4.2.2\nRStudio 2022.12.0-353\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN).\nRStudio, download the appropriate version from here.\n\nThis software is already installed on University Machines. But you will need it to run the analysis on your personal devices.\nData\nExample datasets could be accessed through Canvas or the GitHub Repository of the module. These include:\n\n2021 UK Census Data.\n2021 Annulation Population Survey.\n2016 Family Resource Survey.\n\nNote: The Annual Population Survey requires the completion of a form prior to its usage, as it is licensed.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/assessment.html",
    "href": "general/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "Required Report Structure\nFollow this structure and include ALL these points, do not make your life harder.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#required-report-structure",
    "href": "general/assessment.html#required-report-structure",
    "title": "Assessment",
    "section": "",
    "text": "Introduction\n\nContext: Why is the topic relevant or worth being investigated?\nBrief discussion of existing literature.\nKnowledge gap and Aim.\nResearch questions.\n\nLiterature review\n\nMore detailed Literature review, i.e. what do we already know about this subject\nRationale for including certain predictor variables in the model.\nWhat knowledge gap remains that this article will address? (includes “not studied before in this area”). Note: there is no expectation on totally original research. The focus is on a clean, sensible, data analysis situated in existing ideas.\n\nMethodology:\n\nA brief introduction to the dataset being analysed (who collected it? When? How many responses? etc.)\nA description of the variables chosen to be analysed.\nA description of any transformation made to the original data, i.e. turning a continuous variable of income into intervals, or reducing the number of age groups from 11 to 3.\nA description and justification of the statistical techniques in the subsequent analysis.\n\nResults and Discussion\n\nDescriptive statistics and summary of the variables employed.\nResults and interpretation, including links and contasts to existing literature.\nSelective illustrations (graphs and tables) to make your findings as clear as possible.\n\nConclusion\n\nSummary of main findings.\nLimitations of study (self-critique).\nPotential future directions.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-to-get-there",
    "href": "general/assessment.html#how-to-get-there",
    "title": "Assessment",
    "section": "How to get there?",
    "text": "How to get there?\nThe first stage is to identify ONE a relevant research question to be addressed. Based on the chosen question, you will need to identify a dependent (or outcome) variable which you want to explain, and at least two relevant independent variables that you can use to explain the chosen dependent variable. The selection of variables should be informed by the literature and empirical evidence.\nTo detail in the Methods Section: Once the variables have been chosen, you will need to describe the data and appropriate type of regression to be used for the analysis. You need to explain any transformation done to the original data source, such as reclassifying variables, or changing variables from continuous to nominal scales. You also need to briefly describe the data use: source of data, year of data collection, indicate the number of records used, state if you are using individual records or geographical units, explain if you are selecting a sample, and any relevant details. You also need to identify type of regression to be used and why.\nTo detail in the Results and Discussion Section: Firstl,y, you need to provide two types of analyses. First, you need to provide a descriptive analysis of the data. Here you could use tables and/or plots reporting relevant descriptive statistics, such as the mean, median and standard deviation; variable distributions using histograms; and relationships between variables using correlation matrices or scatter plots. Secondly, you need to present an estimated regression model or models and the interpretation of the estimated coefficients. You need a careful and critical analysis of the regression estimates. You should think that you intend to use your regression models to advice your boss who is expecting to make some decisions based on the information you will provide. As part of this process, you need to discuss the model assessment results for the overall model and regression coefficients. Remember to substantiate your arguments using relevant literature and evidence, and present results clearly in tables and graphs.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-to-submit",
    "href": "general/assessment.html#how-to-submit",
    "title": "Assessment",
    "section": "How to submit",
    "text": "How to submit\nYou should submit a .pdf file, that is a rendered version of a Quarto Markdown file (qmd file). This will allow you to write a research paper that also includes your working code, without the need of including the data (rendered .qmd files are executed before being converted to R).\nHow to get a PDF?\n\nInstall Quarto: Make sure you have Quarto installed. You can download it from quarto.org.\nLaTeX Installation: For PDF output, you’ll need a LaTeX distribution like TinyTeX from R, by executing this in the R console:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOpen the Quarto File: Open your .qmd file in RStudio.\nSet Output Format: In the YAML header at the top of your Quarto file, specify pdf under format:\n\n\n\n\n\n\n\n\n\n\n    title: \"Your Document Title\"\n    author: \"Anonymous\" # do not change\n    format: pdf\n\nClick the Render button in the RStudio toolbar (next to the Knit button).",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "labs/01.introR.html",
    "href": "labs/01.introR.html",
    "title": "1  Lab: Introduction to R for Statistics",
    "section": "",
    "text": "1.1 R?\nR is an open-source program that is commonly used in Statistics. It runs on almost every platform and is completely free and is available at www.r-project.org. Most of the cutting-edge statistical research is first available on R.\nR is a script based language, so there is no point and click interface. While the initial learning curve will be steeper, understanding how to write scripts will be valuable because it leaves a clear description of what steps you performed in your data analysis. Typically you will want to write a script in a separate file and then run individual lines. This saves you from having to retype a bunch of commands and speeds up the debugging process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R for Statistics</span>"
    ]
  },
  {
    "objectID": "labs/01.introR.html#rstudio-basics",
    "href": "labs/01.introR.html#rstudio-basics",
    "title": "1  Lab: Introduction to R for Statistics",
    "section": "1.2 R(Studio) Basics",
    "text": "1.2 R(Studio) Basics\nWe will be running R through the program RStudio which is located at rstudio.com. When you first open up RStudio the console window gives you some information about the version of R you are running and then it gives the prompt &gt;. This prompt is waiting for you to input a command. The prompt + tells you that the current command is spanning multiple lines. In a script file you might have typed something like this:\nfor( i in 1:5 ){\n    print(i)\n}\nFinding help about a certain function is very easy. At the prompt, just type help(function.name) or ?function.name. If you don’t know the name of the function, your best bet is to go the the web page www.rseek.org which will search various R resources for your keyword(s). Another great resource is the coding question and answer site stackoverflow.\n\n1.2.1 Starting a session in RStudio\nUpon startup, RStudio will look something like this.\n\n\n\n\n\n\n\n\n\nNote: the Pane Layout and Appearance settings can be altered:\n\non Windows by clicking RStudio&gt;Tools&gt;Global Options&gt;Appearance or Pane Layout\non Mac OS by clicking RStudio&gt;Preferences&gt;Appearance or Pane Layout.\n\nYou will also have a standard white background; but you can choose specific themes.\nSource Panel (Top-Left)\nThis is where you write, edit, and view scripts, R Markdown/Quarto documents, or R scripts. It allows:\n\nEditing Scripts: Write and edit R scripts or documents (.R, .Rmd, .qmd).\nExecuting the Code: Run lines, blocks, or the entire script directly from the editor.\n\nConsole Panel (Bottom-Left)\nThe Console is the main place to run R commands interactively. It allows:\n\nExecuting the Code: Type and run R commands directly.\nViewing outputs, warnings, and errors for immediate feedback.\nBrowsing and reusing past commands (History Tab).\nToggling between the R Console, and the Terminal (yuo don’t really need the latter).\n\nEnvironment Panel (Top-Right)\nThis panel helps track variables, functions, and the history of commands used. It contains:\n\nEnvironment Tab: Shows all current variables, datasets, and objects in your session, including their structure and values.\nHistory Tab: Provides a record of past commands. You can re-run or move commands to the console or script.\n\nFiles / Plots / Packages / Help Panel (Bottom-Right)\nThis multifunctional panel is for file navigation, plotting, managing packages, viewing help, and managing jobs. It contains:\n\nFiles Tab: Navigate, open, and manage files and directories within your project.\nPlots Tab: Displays plots generated in your session. You can export or navigate through multiple plots here.\nPackages Tab: Lists installed packages and allows you to install, load, and update packages.\nHelp Tab: Displays help documentation for R functions, packages, and other resources. You can search for documentation by typing a function or package name.\n\nImportant: Unless you are working with a script, you will be likely writing code on the console.\nAt the start of a session, it’s good practice clearing your R environment (console):\n\nrm(list = ls())\n\nIn R, we are going to be working with relative paths. With the command getwd(), you can see where your working directory is currently set.\n\ngetwd() \n\nFor ENVS225, download the material of the module an unzip it whever you like.\nThe folder structure should look like:\nstats/\n├── data/\n├── labs_img/\n└── labs/\nYou can delete other sub-folders (e.g. docs).\nThis should be on your personal computer or if on a local machine, I suggest using the directory M: to store the folder, it can be accessed from every computer.\nThen, in R Studio - on Windows by clicking RStudio&gt;Tools&gt;Global Options&gt;General.. - on Mac OS by clicking RStudio&gt;Preferences&gt;Appearance or Pane Layout…\nbrowse and set the folder you just creted as your working directory.\nCheck if that has been applied.\n\ngetwd() \n\nFile paths in R work like this:\n\n\n\n\n\n\n\nFile Path\nDescription\n\n\n\n\nMyFile.csv\nLook in the working directory for MyFile.csv.\n\n\nMyFolder/MyFile.csv\nIn the working directory, there is a subdirectory called MyFolder and inside that folder is MyFile.csv.\n\n\n\nYou do not need to set your working directory if you are using an R-markdown or Quarto document and you have it saved in the right location. The pathway will start from where your document is saved.\n\n\n1.2.2 Using the console\nTry to use the console to perform a few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\n1.2.3 R as a simple calculator\nYou can use R as a simple calculator. At the prompt, type 2+3 and hit enter. What you should see is the following\n\n# Some simple addition\n2+3\n\n[1] 5\n\n\nIn this fashion you can use R as a very capable calculator.\n\n6*8\n\n[1] 48\n\n4^3\n\n[1] 64\n\nexp(1)   # exp() is the exponential function\n\n[1] 2.718282\n\n\nR has most constants and common mathematical functions you could ever want. For example, the absolute value of a number is given by abs(), and round() will round a value to the nearest integer.\n\npi     # the constant 3.14159265...\n\n[1] 3.141593\n\nabs(1.77) \n\n[1] 1.77\n\n\nWhenever you call a function, there will be some arguments that are mandatory, and some that are optional and the arguments are separated by a comma. In the above statements the function abs() requires at least one argument, and that is the number you want the absolute value of.\nWhen functions require more than one argument, arguments can be specified via the order in which they are passed or by naming the arguments. So for the log() function, for example, which calculates the logarithm of a number, one can specify the arguments using the named values; the order woudn’t matter:\n\n# Demonstrating order does not matter if you specify\n# which argument is which\nlog(x=5, base=10)   \n\n[1] 0.69897\n\nlog(base=10, x=5)\n\n[1] 0.69897\n\n\nWhen we don’t specify which argument is which, R will decide that x is the first argument, and base is the second.\n\n# If not specified, R will assume the second value is the base...\nlog(5, 10)\n\n[1] 0.69897\n\nlog(10, 5)\n\n[1] 1.430677\n\n\nWhen we want to specify the arguments, we can do so using the name=value notation.\n\n\n1.2.4 Variables Assignment\nWe need to be able to assign a value to a variable to be able to use it later. R does this by using an arrow &lt;- or an equal sign =. While R supports either, for readability, I suggest people pick one assignment operator and stick with it.\nVariable names cannot start with a number, may not include spaces, and are case sensitive.\n\nvar &lt;- 2*7.5       # create two variables\nanother_var = 5   # notice they show up in 'Environment' tab in RStudio!\nvar \n\n[1] 15\n\nvar * another_var \n\n[1] 75\n\n\nAs your analysis gets more complicated, you’ll want to save the results to a variable so that you can access the results later. If you don’t assign the result to a variable, you have no way of accessing the result.\n\n\n1.2.5 Working with Scripts\nR Scripts (.R files)\nTraditional script files look like this:\n\n# Problem 1 \n# Calculate the log of a couple of values and make a plot\n# of the log function from 0 to 3\nlog(0)\nlog(1)\nlog(2)\nx &lt;- seq(.1,3, length=1000)\nplot(x, log(x))\n\n# Problem 2\n# Calculate the exponential function of a couple of values\n# and make a plot of the function from -2 to 2\nexp(-2)\nexp(0)\nexp(2)\nx &lt;- seq(-2, 2, length=1000)\nplot(x, exp(x))\n\nIn RStudio you can create a new script by going to File -&gt; New File -&gt; R Script. This opens a new window in RStudio where you can type commands and functions as a common text editor.\nThis looks perfectly acceptable as a way of documenting what one does, but this script file doesn’t contain the actual results of commands you ran, nor does it show you the plots. Also anytime you want to comment on some output, it needs to be offset with the commenting character #. It would be nice to have both the commands and the results merged into one document. This is what the R Markdown file does for us.\nR Markdown (.Rmd and .qmd files)\nThe R Markdown is an implementation of the Markdown syntax that makes it extremely easy to write webpages or scientific documents that include conde. This syntax was extended to allow users to embed R code directly into more complex documents. Perhaps the easiest way to understand the syntax is to look at an at the RMarkdown website.\nThe R code in a R Markdown document (.rmd file extension) can be nicely separated from regular text using the three backticks (3 times `, see below) and an instruction that it is R code that needs to be evaluated. A code chunk will look like:\n\n    for (i in 1:5) {print(i)}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nIn ENVS225: In this module we will be using .qmd a more flexible development of .rmd files.\nMarkdown files present several advantages compared to writing your code in the console or just using scripts. You’ll save yourself a huge amount of work by embracing Markdown files from the beginning; you will keep track of your code and your steps, be able to document and present how you did your analysis (helpful when writing the methods section of a paper), and it will make it easier to re-run an analysis after a change in the data (such as additional data values, transformed data, or removal of outliers) or once you spot an error. Finally, it makes the script more readable.\n\n\n1.2.6 R Packages\nOne of the greatest strengths about R is that so many people have developed add-on packages to do some additional function. To download and install the package from the Comprehensive R Archive Network (CRAN), you just need to ask RStudio it to install it via the menu Tools -&gt; Install Packages.... Once there, you just need to give the name of the package and RStudio will download and install the package on your computer.\nOnce a package is downloaded and installed on your computer, it is available, but it is not loaded into your current R session by default. To improve overall performance only a few packages are loaded by default and the you must explicitly load packages whenever you want to use them. You only need to load them once per session/script.\n\nlibrary(dplyr)   # load the dplyr library, will be useful later",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R for Statistics</span>"
    ]
  },
  {
    "objectID": "labs/01.introR.html#practice-dataset-and-dataframes",
    "href": "labs/01.introR.html#practice-dataset-and-dataframes",
    "title": "1  Lab: Introduction to R for Statistics",
    "section": "1.3 Practice: Dataset and Dataframes",
    "text": "1.3 Practice: Dataset and Dataframes\n\nFirst of all, create a new Markdown document. We use the File -&gt; New File -&gt; Quarto Document.. dropdown option, and a menu will appear asking you for the document title, author, and preferred output type. You can select HTML, but you will need your assignment to be submitted in PDF; more on that later.\nFollow the practical below. You can describe what you are doing in normal text. See here for how to format normal text in Markdown documents\nRemember, when you want to write code in a markdown document you have to enclose it like this:\n\n\n\n\n\n\n\n\n\n\n\nor you can insert it manually:\n\n\n\n\n\n\n\n\n\n\nWithin this module we will be working with data stored in so-called datasets. A dataset is a structured collection of data points that represent various measurements or observations, often organized in a tabular format with rows and columns. A dataset might contain information about different locations, such as neighborhoods or cities, with each row representing a place and each column detailing characteristics like population density, average income, or number of green parks. For example, a dataset could be compiled to study patterns in urban mobility, where the data includes the number of daily commuters, the distance they travel, and the mode of transport they use. Datasets provide the essential building blocks for statistical analysis; they enable exploring relationships, identifying patterns, and drawing conclusions about certatin phenomena.\nExamples of everyday datasets:\n\nPremier League Standings: Each row represents a team, with columns for points, games played, wins, draws, and losses.\nMovie Dataset: Each row represents a movie, with columns showing its title, genre, release year, director, and rating.\nWeather Dataset: Each row shows a day’s weather in a city, with columns for temperature, humidity, wind speed, and precipitation.\n\nUsually, data is organized in\n\nColumns of data representing some trait or variable that we might be interested in. In general, we might wish to investigate the relationship between variables.\nRows represent a single object on which the column traits are measured.\n\nFor example, in a grade book for recording students scores throughout the semester, their is one row for every student and columns for each assignment. A greenhouse experiment dataset will have a row for every plant and columns for treatment type and biomass.\n\n1.3.1 Datasets in R\nIn R, we want a way of storing data where it feels just as if we had an Excel Spreadsheet where each row represents an observation and each column represents some information about that observation. We will call this object a data.frame, an R represention of a data set. The easiest way to understand data frames is to create one.\n\nTask: Copy the code below in your markdown. Create a data.frame that represents an instructor’s grade book, where each row is a student, and each column represents some sort of assessment.\n\n\nGrades &lt;- data.frame(\n  Name  = c('Bob','Jeff','Mary','Valerie'),     \n  Exam.1 = c(90, 75, 92, 85),\n  Exam.2 = c(87, 71, 95, 81)\n)\n# Show the data.frame \n# View(Grades)  # show the data in an Excel-like tab.  Doesn't work when knitting \nGrades          # show the output in the console. This works when knitting\n\n     Name Exam.1 Exam.2\n1     Bob     90     87\n2    Jeff     75     71\n3    Mary     92     95\n4 Valerie     85     81\n\n\nTo execute just one chunk of code press the green arrow top-right of the chunk:\n\n\n\n\n\n\n\n\n\nR allows two differnt was to access elements of the data.frame. First is a matrix-like notation for accessing particular values.\n\n\n\nFormat\nResult\n\n\n\n\n[a,b]\nElement in row a and column b\n\n\n[a,]\nAll of row a\n\n\n[,b]\nAll of column b\n\n\n\nBecause the columns have meaning and we have given them column names, it is desirable to want to access an element by the name of the column as opposed to the column number.\n\nTask: Copy and Run:\n\n\nGrades[, 2]       # print out all of column 2 \n\n[1] 90 75 92 85\n\nGrades$Name       # The $-sign means to reference a column by its label\n\n[1] \"Bob\"     \"Jeff\"    \"Mary\"    \"Valerie\"\n\n\n\n\n1.3.2 Importing Data in R\nFrom: https://raw.githubusercontent.com/dereksonderegger/570L/master/07_DataImport.Rmd\nUsually we won’t type the data in by hand, but rather load the data from some package. Reading data from external sources is a necessary skill.\nComma Separated Values Data\nTo consider how data might be stored, we first consider the simplest file format: the comma separated values file (.csv). In this file time, each of the “cells” of data are separated by a comma. For example, the data file storing scores for three students might be as follows:\nAble, Dave, 98, 92, 94\nBowles, Jason, 85, 89, 91\nCarr, Jasmine, 81, 96, 97\nTypically when you open up such a file on a computer with MS Excel installed, Excel will open up the file assuming it is a spreadsheet and put each element in its own cell. However, you can also open the file using a more primitive program (say Notepad in Windows, TextEdit on a Mac) you’ll see the raw form of the data.\nHaving just the raw data without any sort of column header is problematic (which of the three exams was the final??). Ideally we would have column headers that store the name of the column.\nLastName, FirstName, Exam1, Exam2, FinalExam\nAble, Dave, 98, 92, 94\nBowles, Jason, 85, 89, 91\nCarr, Jasmine, 81, 96, 97\nReading (.csv) files\nTo make R read in the data arranged in this format, we need to tell R three things:\n\nWhere does the data live? Often this will be the name of a file on your computer, but the file could just as easily live on the internet (provided your computer has internet access).\nIs the first row data or is it the column names?\nWhat character separates the data? Some programs store data using tabs to distinguish between elements, some others use white space. R’s mechanism for reading in data is flexible enough to allow you to specify what the separator is.\n\nThe primary function that we’ll use to read data from a file and into R is the function read.csv(). This function has many optional arguments but the most commonly used ones are outlined in the table below.\n\n\n\n\n\n\n\n\nArgument\nDefault\nDescription\n\n\n\n\nfile\nRequired\nA character string denoting the file location.\n\n\nheader\nTRUE\nSpecifies whether the first line contains column headers.\n\n\nsep\n\",\"\nSpecifies the character that separates columns. For read.csv(), this is usually a comma.\n\n\nskip\n0\nThe number of lines to skip before reading data; useful for files with descriptive text before the actual data.\n\n\nna.strings\n\"NA\"\nValues that represent missing data; multiple values can be specified, e.g., c(\"NA\", \"-9999\").\n\n\nquote\n\"\nSpecifies the character used to quote character strings, typically \" or '.\n\n\nstringsAsFactors\nFALSE\nControls whether character strings are converted to factors; FALSE means they remain as character data.\n\n\nrow.names\nNULL\nAllows specifying a column as row names, or assigning NULL to use default indexing for rows.\n\n\ncolClasses\nNULL\nSpecifies the data type for each column to speed up reading for large files, e.g., c(\"character\", \"numeric\").\n\n\nencoding\n\"unknown\"\nSets the text encoding of the file, which can be useful for files with special or international characters.\n\n\n\nMost of the time you just need to specify the file. |\n\nTask: Let’s read in a dataset of terrorist attacks that have taken place in the UK:\n\n\nattacks &lt;- read.csv(file   = '../data/attacksUK.csv')  # where the data lives                                 \nView(attacks)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R for Statistics</span>"
    ]
  },
  {
    "objectID": "labs/01.introR.html#practice-descriptive-statistics",
    "href": "labs/01.introR.html#practice-descriptive-statistics",
    "title": "1  Lab: Introduction to R for Statistics",
    "section": "1.4 Practice: Descriptive Statistics",
    "text": "1.4 Practice: Descriptive Statistics\n\n1.4.1 Summarizing Data\nIt is very important to be able to take a data set and produce summary statistics such as the mean and standard deviation of a column. For this sort of manipulation, we use the package dplyr. This package allows chaining together many common actions to form a particular task.\nThe foundational operations to perform on a data set are:\n\nSubsetting - Returns a with only particular columns or rows\n– select - Selecting a subset of columns by name or column number.\n– filter - Selecting a subset of rows from a data frame based on logical expressions.\n– slice - Selecting a subset of rows by row number.\narrange - Re-ordering the rows of a data frame.\nmutate - Add a new column that is some function of other columns.\nsummarise - calculate some summary statistic of a column of data. This collapses a set of rows into a single row.\n\nEach of these operations is a function in the package dplyr. These functions all have a similar calling syntax,: - The first argument is a data set;. - Subsequent arguments describe what to do with the input data frame and you can refer to the columns without using the df$column notation.\nAll of these functions will return a data set.\nThe dplyr package also includes a function that “pipes” commands together. The idea is that the %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function f. The beauty of this comes when you have a suite of functions that takes input arguments of the same type as their output. For example if we wanted to start with x, and first apply function f(), then g(), and then h(), the usual R command would be h(g(f(x))) which is hard to read because you have to start reading at the innermost set of parentheses. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h(). For example:\n\nGrades # Recall the Grades data \n\n     Name Exam.1 Exam.2\n1     Bob     90     87\n2    Jeff     75     71\n3    Mary     92     95\n4 Valerie     85     81\n\n# The following code takes the Grades data.frame and calculates \n# a column for the average exam score, and then sorts the data \n# according to the that average score\nGrades %&gt;%\n  mutate( Avg.Score = (Exam.1 + Exam.2) / 2 ) %&gt;%\n  arrange( Avg.Score )\n\n     Name Exam.1 Exam.2 Avg.Score\n1    Jeff     75     71      73.0\n2 Valerie     85     81      83.0\n3     Bob     90     87      88.5\n4    Mary     92     95      93.5\n\n\nKeep it in mind, it is not necessary to memorise this.\nLet’s consider the summarize function to calculate the mean score for Exam.1. Notice that this takes a data frame of four rows, and summarizes it down to just one row that represents the summarized data for all four students.\n\nlibrary(dplyr) # load the library\nGrades %&gt;%\n  summarize( Exam.1.mean = mean( Exam.1 ) )\n\n  Exam.1.mean\n1        85.5\n\n\nSimilarly you could calculate the standard deviation for the exam as well.\n\nGrades %&gt;%\n  summarize( Exam.1.mean = mean( Exam.1 ),\n             Exam.1.sd   = sd(   Exam.1   ) )\n\n  Exam.1.mean Exam.1.sd\n1        85.5  7.593857\n\n\n\nTask: Write the code above in your markdown file and run it. Do not to copy it this time.\n\nLet’s go back to the terrorist attacks. There are attacks perpetrated by several different groups. Each record is a single attack and contains information about who perpetrated the attack, what year, how many were killed and how many were wounded. You can get a glimpse of the dataframe with the function head\n\nhead(attacks, n = 10)\n\n   nrKilled nrWound year        country                        group\n1         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n2         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n3         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n4         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n5         0       1 1982 United Kingdom Abu Nidal Organization (ANO)\n6         0       0 2014 United Kingdom                   Anarchists\n7         0       0 2014 United Kingdom                   Anarchists\n8         0       0 2014 United Kingdom                   Anarchists\n9         0       0 2014 United Kingdom                   Anarchists\n10        0       0 2014 United Kingdom                   Anarchists\n                           attack                      target\n1               Bombing/Explosion              Transportation\n2               Bombing/Explosion              Transportation\n3               Bombing/Explosion              Transportation\n4               Bombing/Explosion              Transportation\n5                   Assassination     Government (Diplomatic)\n6  Facility/Infrastructure Attack                    Business\n7  Facility/Infrastructure Attack                    Business\n8  Facility/Infrastructure Attack                    Business\n9  Facility/Infrastructure Attack Private Citizens & Property\n10 Facility/Infrastructure Attack                      Police\n                      weapon\n1  Explosives/Bombs/Dynamite\n2  Explosives/Bombs/Dynamite\n3  Explosives/Bombs/Dynamite\n4  Explosives/Bombs/Dynamite\n5                   Firearms\n6                 Incendiary\n7                 Incendiary\n8                 Incendiary\n9                 Incendiary\n10                Incendiary\n\n\nWe might want to compare different actors and see the mean and standard deviation of the number of people wound, by each group’s attack, across time. To do this, we are still going to use the summarize, but we will precede that with group_by(group) to tell the subsequent dplyr functions to perform the actions separately for each breed.\n\nattacks %&gt;%\n  group_by( group) %&gt;%\n  summarise( Mean = mean(attacks$nrWound), \n             Std.Dev = sd(attacks$nrWound))\n\n# A tibble: 38 × 3\n   group                                               Mean Std.Dev\n   &lt;chr&gt;                                              &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abu Hafs al-Masri Brigades                         0.963    7.22\n 2 Abu Nidal Organization (ANO)                       0.963    7.22\n 3 Anarchists                                         0.963    7.22\n 4 Animal Liberation Front (ALF)                      0.963    7.22\n 5 Animal Rights Activists                            0.963    7.22\n 6 Armenian Secret Army for the Liberation of Armenia 0.963    7.22\n 7 Black September                                    0.963    7.22\n 8 Continuity Irish Republican Army (CIRA)            0.963    7.22\n 9 Dissident Republicans                              0.963    7.22\n10 Informal Anarchist Federation                      0.963    7.22\n# ℹ 28 more rows\n\n\n\nTask: Write the code above in your markdown file and run it. Try out another categorical variable instead of group (e.g. year) and nrKilled instead of nrWound.\n\nLet’s now move to another dataset to address a research question. For illustration purposes, we will use the Family Resources Survey (FRS). The FRS is an annual survey conducted by the UK government that collects detailed information about the income, living conditions, and resources of private households across the United Kingdom. Managed by the Department for Work and Pensions (DWP), the FRS provides data that is essential for understanding the economic and social conditions of households and informing public policy.\nConsider questions such as:\n\nHow many respondents (persons) are there in the 2016-17 FRS?\nHow many variables (population attributes) are there?\nWhat types of variables are present in the FRS?\nWhat is the most detailed geography available in the FRS?\n\n\nTask: To answer these questions, load and inspect the dataset.\n\n\n# the FRS dataset should be already loaded, otherwise\nfrs_data &lt;- read.csv(\"../data/FamilyResourceSurvey/FRS16-17.csv\") \n\n# Display basic structure \nglimpse(frs_data)\n\nRows: 44,145\nColumns: 45\n$ household        &lt;int&gt; 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 4, 4, 4, 5, 6, 6, 7, 7,…\n$ family           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1,…\n$ person           &lt;int&gt; 1, 2, 3, 4, 5, 1, 1, 2, 3, 4, 1, 2, 3, 1, 1, 2, 1, 2,…\n$ country          &lt;int&gt; 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1,…\n$ region           &lt;int&gt; 8, 8, 8, 8, 8, 12, 4, 4, 4, 4, 11, 11, 11, 6, 8, 8, 4…\n$ age_group        &lt;int&gt; 9, 9, 2, 2, 4, 8, 8, 8, 2, 1, 13, 12, 7, 14, 7, 8, 15…\n$ sex              &lt;int&gt; 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1,…\n$ marital_status   &lt;int&gt; 3, 3, 1, 1, 1, 1, 3, 3, 1, 1, 3, 3, 1, 1, 3, 3, 3, 3,…\n$ ethnicity        &lt;int&gt; 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hrp              &lt;int&gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,…\n$ rel_to_hrp       &lt;int&gt; 1, 0, 3, 3, 4, 0, 0, 1, 4, 3, 0, 1, 3, 0, 1, 0, 1, 0,…\n$ lifestage        &lt;int&gt; 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ dependent        &lt;int&gt; 2, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ arrival_year     &lt;int&gt; 1995, 1995, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -…\n$ birth_country    &lt;int&gt; 8, 8, -9, -9, -9, 3, 1, 1, -9, -9, 2, 2, 2, 1, 4, 8, …\n$ care_hours       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ educ_age         &lt;int&gt; 17, 17, -9, -9, -9, 30, 21, 18, -9, -9, 19, 18, 17, 2…\n$ educ_type        &lt;int&gt; 10, 0, 1, 1, 1, 4, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ fam_youngest     &lt;int&gt; 6, 6, 6, 6, 6, -1, 4, 4, 4, 4, -1, -1, -1, -1, -1, -1…\n$ fam_toddlers     &lt;int&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ fam_size         &lt;int&gt; 5, 5, 5, 5, 5, 1, 4, 4, 4, 4, 2, 2, 1, 1, 2, 2, 2, 2,…\n$ happy            &lt;int&gt; 1, 6, -9, -9, -9, 4, 4, -1, -9, -9, 7, 7, 7, 10, 8, -…\n$ health           &lt;int&gt; 1, 1, -1, -1, 1, 2, 3, -1, -1, -1, 2, 2, 2, 3, 2, -1,…\n$ hh_accom_type    &lt;int&gt; 3, 3, 3, 3, 3, 4, 1, 1, 1, 1, 1, 1, 1, 2, 4, 4, 2, 2,…\n$ hh_benefits      &lt;int&gt; 2496, 2496, 2496, 2496, 2496, 0, 1768, 1768, 1768, 17…\n$ hh_composition   &lt;int&gt; 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 9, 9, 9, 1, 7, 7, 5, 5,…\n$ hh_ctax_band     &lt;int&gt; 4, 4, 4, 4, 4, 8, 4, 4, 4, 4, 7, 7, 7, 4, 4, 4, 2, 2,…\n$ hh_housing_costs &lt;int&gt; 8060, 8060, 8060, 8060, 8060, 6604, 4108, 4108, 4108,…\n$ hh_income_gross  &lt;int&gt; 29640, 29640, 29640, 29640, 29640, 17264, 78520, 7852…\n$ hh_income_net    &lt;int&gt; 26884, 26884, 26884, 26884, 26884, 17264, 60112, 6011…\n$ hh_size          &lt;int&gt; 5, 5, 5, 5, 5, 1, 4, 4, 4, 4, 3, 3, 3, 1, 2, 2, 2, 2,…\n$ hh_tenure        &lt;int&gt; 3, 3, 3, 3, 3, 4, 2, 2, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1,…\n$ highest_qual     &lt;int&gt; 2, 3, -9, -9, -9, 2, 1, 1, -9, -9, 3, 3, 3, 1, 1, 1, …\n$ income_gross     &lt;dbl&gt; 2496, 27144, 0, 0, 0, 17264, 40924, 37596, 0, 0, -301…\n$ income_net       &lt;dbl&gt; 2496, 24388, 0, 0, 0, 17264, 31772, 28340, 0, 0, -317…\n$ jobs             &lt;int&gt; 0, 1, -9, -9, -9, 1, 1, 1, -9, -9, 1, 1, 1, 1, 1, 1, …\n$ life_satisf      &lt;int&gt; 8, 5, -9, -9, -9, 6, 8, -1, -9, -9, 7, 7, 7, 9, 9, -1…\n$ nssec            &lt;int&gt; 7, 7, 12, 12, 12, 10, 1, 3, 12, 12, 5, 3, 3, 7, 5, 3,…\n$ sic_chapter      &lt;int&gt; 17, 14, -9, -9, -9, 7, 13, 13, -9, -9, 1, 17, 3, 7, 1…\n$ sic_division     &lt;int&gt; 87, 80, -9, -9, -9, 47, 72, 72, -9, -9, 1, 88, 28, 47…\n$ soc2010          &lt;int&gt; 6, 6, -9, -9, -9, 9, 1, 3, -9, -9, 5, 2, 3, 7, 1, 4, …\n$ work_hours       &lt;dbl&gt; -2.0, 48.0, -9.0, -9.0, -9.0, 12.0, 40.0, 37.0, -9.0,…\n$ workstatus       &lt;int&gt; 8, 1, -9, -9, -9, 1, 1, 1, -9, -9, 3, 1, 1, 2, 3, 1, …\n$ years_ft_work    &lt;int&gt; 16, 25, -9, -9, -9, 8, 18, 15, -9, -9, 41, 30, 15, 10…\n$ survey_weight    &lt;int&gt; 1870, 1870, 1870, 1870, 1870, 1083, 1571, 1571, 1571,…\n\n\nand summary:\n\nsummary(frs_data)\n\n   household         family          person        country          region    \n Min.   :    1   Min.   :1.000   Min.   :1.00   Min.   :1.000   Min.   : 1.0  \n 1st Qu.: 4816   1st Qu.:1.000   1st Qu.:1.00   1st Qu.:1.000   1st Qu.: 5.0  \n Median : 9673   Median :1.000   Median :2.00   Median :1.000   Median : 8.0  \n Mean   : 9677   Mean   :1.106   Mean   :1.98   Mean   :1.618   Mean   : 7.8  \n 3rd Qu.:14553   3rd Qu.:1.000   3rd Qu.:3.00   3rd Qu.:2.000   3rd Qu.:11.0  \n Max.   :19380   Max.   :6.000   Max.   :9.00   Max.   :4.000   Max.   :13.0  \n   age_group           sex        marital_status    ethnicity    \n Min.   : 1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.: 5.000   1st Qu.:1.000   1st Qu.:1.000   1st Qu.:1.000  \n Median : 9.000   Median :2.000   Median :3.000   Median :1.000  \n Mean   : 8.707   Mean   :1.519   Mean   :2.429   Mean   :1.462  \n 3rd Qu.:13.000   3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:1.000  \n Max.   :16.000   Max.   :2.000   Max.   :6.000   Max.   :9.000  \n      hrp          rel_to_hrp       lifestage       dependent    \n Min.   :0.000   Min.   : 0.000   Min.   :0.000   Min.   :1.000  \n 1st Qu.:0.000   1st Qu.: 0.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :0.000   Median : 1.000   Median :1.000   Median :2.000  \n Mean   :0.439   Mean   : 1.553   Mean   :0.773   Mean   :1.766  \n 3rd Qu.:1.000   3rd Qu.: 3.000   3rd Qu.:1.000   3rd Qu.:2.000  \n Max.   :1.000   Max.   :20.000   Max.   :1.000   Max.   :2.000  \n  arrival_year    birth_country       care_hours        educ_age    \n Min.   :  -9.0   Min.   :-9.0000   Min.   :0.0000   Min.   :-9.00  \n 1st Qu.:  -9.0   1st Qu.: 1.0000   1st Qu.:0.0000   1st Qu.:-1.00  \n Median :  -9.0   Median : 1.0000   Median :0.0000   Median :16.00  \n Mean   : 207.4   Mean   :-0.1686   Mean   :0.2819   Mean   :11.14  \n 3rd Qu.:  -9.0   3rd Qu.: 3.0000   3rd Qu.:0.0000   3rd Qu.:18.00  \n Max.   :2017.0   Max.   : 8.0000   Max.   :7.0000   Max.   :55.00  \n   educ_type       fam_youngest     fam_toddlers       fam_size    \n Min.   : 0.000   Min.   :-1.000   Min.   :0.0000   Min.   :1.000  \n 1st Qu.: 0.000   1st Qu.:-1.000   1st Qu.:0.0000   1st Qu.:2.000  \n Median : 0.000   Median :-1.000   Median :0.0000   Median :2.000  \n Mean   : 0.392   Mean   : 2.466   Mean   :0.2557   Mean   :2.599  \n 3rd Qu.: 0.000   3rd Qu.: 5.000   3rd Qu.:0.0000   3rd Qu.:4.000  \n Max.   :10.000   Max.   :19.000   Max.   :4.0000   Max.   :9.000  \n     happy            health        hh_accom_type     hh_benefits   \n Min.   :-9.000   Min.   :-1.0000   Min.   :-1.000   Min.   :    0  \n 1st Qu.:-1.000   1st Qu.:-1.0000   1st Qu.: 1.000   1st Qu.:    0  \n Median : 5.000   Median : 1.0000   Median : 2.000   Median : 1768  \n Mean   : 2.197   Mean   : 0.9781   Mean   : 2.326   Mean   : 5670  \n 3rd Qu.: 8.000   3rd Qu.: 2.0000   3rd Qu.: 3.000   3rd Qu.:10192  \n Max.   :10.000   Max.   : 5.0000   Max.   : 7.000   Max.   :54080  \n hh_composition    hh_ctax_band   hh_housing_costs hh_income_gross  \n Min.   : 1.000   Min.   :1.000   Min.   :    -1   Min.   :-326092  \n 1st Qu.: 5.000   1st Qu.:2.000   1st Qu.:   572   1st Qu.:  22256  \n Median : 6.000   Median :3.000   Median :  2704   Median :  35984  \n Mean   : 5.474   Mean   :3.198   Mean   :  3820   Mean   :  46076  \n 3rd Qu.: 6.000   3rd Qu.:4.000   3rd Qu.:  5616   3rd Qu.:  57252  \n Max.   :10.000   Max.   :8.000   Max.   :135720   Max.   :1165216  \n hh_income_net        hh_size       hh_tenure      highest_qual     \n Min.   :-334776   Min.   :1.00   Min.   :1.000   Min.   :-9.00000  \n 1st Qu.:  20748   1st Qu.:2.00   1st Qu.:1.000   1st Qu.: 1.00000  \n Median :  31512   Median :3.00   Median :2.000   Median : 2.00000  \n Mean   :  37447   Mean   :2.96   Mean   :2.369   Mean   : 0.04098  \n 3rd Qu.:  47008   3rd Qu.:4.00   3rd Qu.:3.000   3rd Qu.: 3.00000  \n Max.   :1116596   Max.   :9.00   Max.   :5.000   Max.   : 5.00000  \n  income_gross       income_net           jobs         life_satisf   \n Min.   :-354848   Min.   :-358592   Min.   :-9.000   Min.   :-9.00  \n 1st Qu.:     52   1st Qu.:      0   1st Qu.: 0.000   1st Qu.:-1.00  \n Median :  12740   Median :  12012   Median : 0.000   Median : 6.00  \n Mean   :  17305   Mean   :  14204   Mean   :-1.644   Mean   : 2.23  \n 3rd Qu.:  23712   3rd Qu.:  20384   3rd Qu.: 1.000   3rd Qu.: 8.00  \n Max.   :1127360   Max.   :1110928   Max.   : 4.000   Max.   :10.00  \n     nssec         sic_chapter      sic_division      soc2010      \n Min.   : 1.000   Min.   :-9.000   Min.   :-9.00   Min.   :-9.000  \n 1st Qu.: 3.000   1st Qu.:-2.000   1st Qu.:-2.00   1st Qu.:-2.000  \n Median : 7.000   Median : 7.000   Median :47.00   Median : 3.000  \n Mean   : 6.707   Mean   : 5.351   Mean   :40.85   Mean   : 1.229  \n 3rd Qu.:10.000   3rd Qu.:15.000   3rd Qu.:84.00   3rd Qu.: 6.000  \n Max.   :12.000   Max.   :21.000   Max.   :99.00   Max.   : 9.000  \n   work_hours       workstatus      years_ft_work   survey_weight  \n Min.   : -9.00   Min.   :-9.0000   Min.   :-9.00   Min.   :  221  \n 1st Qu.: -2.00   1st Qu.: 1.0000   1st Qu.:-1.00   1st Qu.: 1097  \n Median : -2.00   Median : 1.0000   Median :11.00   Median : 1380  \n Mean   : 12.75   Mean   : 0.8902   Mean   :14.15   Mean   : 1459  \n 3rd Qu.: 37.00   3rd Qu.: 6.0000   3rd Qu.:30.00   3rd Qu.: 1742  \n Max.   :150.00   Max.   :11.0000   Max.   :73.00   Max.   :39675  \n\n\n\n\n1.4.2 Understanding the Structure of the FRS Datafile\nIn the FRS data structure, each row represents a person, but:\n\nEach person is nested within a family.\nEach family is nested within a household.\n\nBelow is an example dataset structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\nhousehold\nfamily\nperson\nregion\nage_group\nsex\nmarital_status\nrel_to_hrp\n\n\n\n\n1\n1\n1\nLondon\n40-44\nFemale\nMarried/Civil partnership\nSpouse\n\n\n1\n1\n2\nLondon\n40-44\nMale\nMarried/Civil partnership\nHousehold Representative\n\n\n1\n1\n3\nLondon\n5-10\nMale\nSingle\nSon/daughter (incl. adopted)\n\n\n1\n1\n4\nLondon\n5-10\nFemale\nSingle\nSon/daughter (incl. adopted)\n\n\n1\n1\n5\nLondon\n16-19\nMale\nSingle\nStep-son/daughter\n\n\n2\n1\n1\nScotland\n35-39\nMale\nSingle\nHousehold Representative\n\n\n3\n1\n1\nYorks and the Humber\n35-39\nFemale\nMarried/Civil partnership\nHousehold Representative\n\n\n3\n1\n2\nYorks and the Humber\n35-39\nMale\nMarried/Civil partnership\nSpouse\n\n\n3\n1\n3\nYorks and the Humber\n5-10\nMale\nSingle\nStep-son/daughter\n\n\n4\n1\n1\nWales\n0-4\nMale\nSingle\nSon/daughter (incl. adopted)\n\n\n4\n1\n2\nWales\n60-64\nMale\nMarried/Civil partnership\nHousehold Representative\n\n\n4\n1\n3\nWales\n55-59\nFemale\nMarried/Civil partnership\nSpouse\n\n\n4\n2\n3\nWales\n30-34\nFemale\nSingle\nSon/daughter (incl. adopted)\n\n\n\nThe first five people in the FRS all belong to the same household (household 1); they also all belong to the same family. This family comprises a married middle-aged couple plus their three children, one of whom is a stepson.\nThe second household (household 2) comprises only one person – a single middle-aged male.The third household comprises another married couple, this time with two children.\nSuperficially the fourth household looks similar to households 1 and 2: a married couple plus their daughter. The difference is that this particular married couple is nearing retirement age, and their daughter is middle-aged. Consequently, despite being a child of the married couple, the middle-aged daughter is treated as a separate ‘family’ (family 2 in the household). This is because the FRS (and Census) define a ‘family’ as a couple plus any ‘dependent’ children. A dependent child is defined as a child who is either` aged 0-15 or aged 16-19, unmarried and in full-time education. All children aged 16-19 who are married or no longer in full-time education are regarded as ‘independent’ adults who form their own family unit, as are all children aged 20+.\nThe inclusion of all persons in a household allows us more flexibility in the types of research question we can answer. For example, we could explore how the likelihood of a woman being in paid employment WorkStatus is influenced by the age of the youngest child still living in her family (if any) fam_youngest.\nIn the FRS (and Census), a “family” is defined as a couple and any “dependent” children. Dependent children are defined as those aged 0–15, or aged 16–19 if unmarried and in full-time education.\n\n\n1.4.3 Explore the Distribution of Your Outcome Variable\nBefore starting your analysis, it is critical to know the type of scale used to measure your outcome variable: is it categorical or continuous? Here we will start off by exploring a continuous variable which can then turn into a categorical variable (e.g. top earners: yes or no). We explore the income distribution in the UK by first looking at the low and high end of the distribution ie. What sorts of people have high (or low) incomes?\nIn the FRS each person’s annual income is recorded, both gross (pre-tax) and net (post-tax). This income includes all income sources, including earnings, profits, investment returns, state benefits, occupational pensions etc. As it is possible to make a loss on some of these activities, it is also possible (although unusual) for someone’s gross or net annual income in a given year to be negative (representing an overall loss).\n\nTask: Load the FRS dataset into your R environment, if it’s not already loaded, and inspect the data.\n\n\n# Load the dataset (replace 'frs_data.csv' with the actual file path)\nfrs_data &lt;- read.csv(\"../data/FamilyResourceSurvey/FRS16-17.csv\") \n\nOpen the dataset in RStudio’s Data Viewer to explore its structure, including the income_gross and income_net variables.\n\n # Open the data in the RStudio Viewer\n View(frs_data)\n\nin the Data Viewer tab, scroll horizontally to locate the income_gross and income_net columns. If columns are listed alphabetically, they will appear near other attributes that start with “income.”\nYou should notice two things:\n\nIncomes are recorded to the nearest £, NOT in income bands.\nDependent children almost all have a recorded income of £0.\n\nThis second observation highlights the somewhat loose wording of our question above (What sorts of people have high (or low) incomes?). To avoid reaching the somewhat banal conclusion that those with the lowest of all incomes are almost all children, we should re-frame the question more precisely as What sorts of people (excluding dependent children) have low incomes?\n\nTask: Determine the Scale of the Outcome Variable.\n\n**\n\n# Summarize income variables\nsummary(frs_data$income_gross)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-354848      52   12740   17305   23712 1127360 \n\n\n\n# Summarize income variables\nsummary(frs_data$income_net)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-358592       0   12012   14204   20384 1110928 \n\n\n\nTask: Exclude Dependent Children.\n\n**\nYou need to select all cases (persons) that are independent, that is where the variable dependent has value 2 (1 = dependent, 2 = independent).\n\n# Filter to include only independent persons\nfrs_independent &lt;- frs_data %&gt;% filter(dependent == \"2\")\n\n\nTask: Create a basic histogram (a visualisation lecture is scheduled later on).\n\nThe income variables in the FRS are all scale variables so a good starting point is to examine its distribution looking at a histogram of income_gross.\n\nlibrary(ggplot2)\n \n ggplot(frs_independent, aes(x = income_gross)) +\n   geom_histogram(binwidth = 5000, fill = \"blue\", color = \"black\") +\n   labs(\n     title = \"Distribution of Gross Household Income\",\n     x = \"Gross Income (£)\",\n     y = \"Frequency\"\n   ) +\n   xlim(0, 90000) +\n   theme_minimal()\n\n\n\n\n\n\n\n\nYou should see the histogram below. It reveals that the income distribution is very skewed with few people earning high salaries and the majority earning just over or less 35,000 annually.\n\nTask: Adopt a regrouping strategy.\n\n**\nYou can also cross-tabulate gross (or net) income with any of the other variables in the FRS to your heart’s content – or can you?\nAgain, here is important to recall that the income variables in the FRS are all ‘scale’ variables; in other words, they are precise measures rather than broad categories. Consequently, every single person in the FRS potentially has their own unique income value. That could make for a table c. 44,000 rows long (one row per person) if each person has their own unique value. The solution is to create a categorical version of the original income variable by assigning each person to one of a set of income categories (income bands). Having done this, cross-tabulation then becomes possible.\nBut which strategy to use? Equal intervals, percentiles or ‘ad hoc’. Here I would suggest that ‘ad hoc’ is best: all you want to do is to allocate each independent adult to one of three arbitrarily defined groups: ‘low’, ‘middle’ and ‘high’ income. Define Low and High Income Thresholds\nDefine thresholds for income categories:\n\nLow-income threshold: £________\nHigh-income threshold: £_______\n\n\nTask: Create a New Variable Based on Regrouping of Original Variable.\n\nRecode income_gross into categories based on the chosen thresholds.\n\n# Define thresholds for income categories \nLOW_THRESHOLD &lt;- 10000 # Replace with the upper limit for low income \nHIGH_THRESHOLD &lt;- 50000 # Replace with the lower limit for high income \n\n# Define income categories based on thresholds \nfrs_independent &lt;- frs_independent %&gt;% \n    mutate(income_category = case_when( \n        income_gross &lt;= LOW_THRESHOLD ~ \"Low\", \n        income_gross &gt;= HIGH_THRESHOLD ~ \"High\", \n        TRUE ~ \"Middle\" ))\n\nThe mutate() function in R, from the dplyr package, is used to add or modify columns in a data frame. It allows you to create new variables or transform existing ones by applying calculations or conditional statements directly within the function.\nExplanation of the code\n\nfrs_independent %&gt;%: The pipe operator %&gt;% sends frs_independent into mutate(), allowing us to apply transformations without reassigning it repeatedly.\nmutate(): Starts the transformation process by defining new or modified columns.\nincome_category = case_when(...):\n\nThis creates a new column named income_category.\nThe case_when() function defines conditions for assigning values to this new column.\n\ncase_when():\n\ncase_when() is used here to assign categorical labels based on conditions.\nincome_gross &lt;= LOW_THRESHOLD ~ \"Low\": If income_gross is less than or equal to LOW_THRESHOLD, income_category will be labeled “Low.”\nincome_gross &gt;= HIGH_THRESHOLD ~ \"High\": If income_gross is greater than or equal to HIGH_THRESHOLD, income_category will be labeled “High.”\nTRUE ~ \"Middle\": Any values not meeting the previous conditions are labeled “Middle.”\n\n\n\nTask: Add some Metadata.\n\nDefine metadata for the new variable by labeling income categories.\n\n# Add metadata by converting to a factor and defining labels\n\nfrs_independent$income_category &lt;- factor(frs_independent$income_category,\n                        levels = c(\"Low\", \"Middle\", \"High\"), labels = c(\"&lt;= £10,000\", \"£10,001 - £49,999\", \"&gt;= £50,000\"))\n\n\nTask: Check your work.\n\nExamine the frequency distribution of the variable you have just created. Both variables should have the same number of missing cases, unless:\n\nMissing cases in the old variable have been intentionally converted into valid cases in the new variable.\nYou forgot to allocate a new value to one of the old variable categories, in which case the new variable will have more missing cases than the old variable.\n\n\n# Frequency distribution of income categories\ntable(frs_independent$income_category)\n\n\n       &lt;= £10,000 £10,001 - £49,999        &gt;= £50,000 \n             8584             22981              2271 \n\n\nAfter preparing the data, use cross-tabulations to compare income levels across demographic groups.\n\n# Cross-tabulate income category by age group, nationality, etc.\ntable(frs_independent$income_category, frs_independent$age_group)\n\n                   \n                       4    5    6    7    8    9   10   11   12   13   14   15\n  &lt;= £10,000         373  680  492  558  474  511  554  652  781  826  773  744\n  £10,001 - £49,999  263 1241 1802 2056 2052 1948 1995 1967 1749 1772 2073 1554\n  &gt;= £50,000           1    8   59  186  314  331  334  356  237  177  144   56\n                   \n                      16\n  &lt;= £10,000        1166\n  £10,001 - £49,999 2509\n  &gt;= £50,000          68\n\n\nExplore income distribution across different regions.\n\n# Cross-tabulate income category by region\ntable(frs_independent$income_category, frs_independent$region) \n\n                   \n                       1    2    4    5    6    7    8    9   10   11   12   13\n  &lt;= £10,000         357  878  670  562  744  665  740  895  588  399 1212  874\n  £10,001 - £49,999  979 2347 1728 1550 1892 1855 1850 2563 1707  971 3234 2305\n  &gt;= £50,000          48  174  114  135  164  245  367  367  149   63  322  123\n\n\nTips for Cross-Tabulation\n\nPlace the income variable in the columns.\nAdd multiple variables in the rows to create simultaneous cross-tabulations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R for Statistics</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html",
    "href": "labs/02.MultipleLinear.html",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "2.1 Part I. Correlation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-i.-correlation",
    "href": "labs/02.MultipleLinear.html#part-i.-correlation",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "2.1.1 Data Overview: Descriptive Statistics:\nLet’s start by picking one dataset derived from the UK 2021 Census data. You can choose one dataset that aggregates data either at a) county, b) district, or c) ward-level. Lower Tier Local Authority-, Region-, and Country-level data is also available in the data folder.\nsee also: https://canvas.liverpool.ac.uk/courses/77895/pages/census-data-2021\n\n# Load necessary libraries \nlibrary(ggplot2) \nlibrary(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# load data\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\") # County level\n\nWe’re using a (district/ward/etc.)-level census dataset that includes:\n\n% of population with poor health (variable name: pct_Very_bad_health).\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nLet’s first get some descriptive statistics that help identify general trends and distributions in the data.\n\n# Summary statistics\nsummary_data &lt;- census %&gt;%\n  select(pct_Very_bad_health, pct_No_qualifications, pct_Males, pct_Higher_manager_prof) %&gt;%\n  summarise_all(list(mean = mean, sd = sd))\nsummary_data\n\n  pct_Very_bad_health_mean pct_No_qualifications_mean pct_Males_mean\n1                 1.173198                   17.89878       48.96817\n  pct_Higher_manager_prof_mean pct_Very_bad_health_sd pct_No_qualifications_sd\n1                     13.21677              0.3401808                 3.959094\n  pct_Males_sd pct_Higher_manager_prof_sd\n1     0.660313                   4.730412\n\n\nQ1. Complete the table by specifying the type (continuous or categorical) and reporting the mean and standard deviation for each variable.\n\n\n\n\n\n\n\n\n\nVariable Name\nType (Continuous or Categorical)\nMean\nStandard Deviation\n\n\n\n\npct_Very_bad_health\n\n\n\n\n\npct_No_qualifications\n\n\n\n\n\npct_Males\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\n\n2.1.2 Simple visualisation for continuous data\nThe relationship between two continuous variables may be visualised graphically using a scatter-plot. Using the chosen census datasets, visualise the association between the % of population with bad health (pct_Very_bad_health) and each of the following:\n\nthe % of population with no qualifications (pct_No_qualifications);\nthe % of population aged 65 to 84 (pct_Age_65_to_84);\nthe % of population in a married couple (pct_Married_couple);\nthe % of population in a Higher Managerial or Professional occupation (pct_Higher_manager_prof).\n\n\n# Scatterplot for each variable variables \nvariables &lt;- c(\"pct_No_qualifications\", \"pct_Age_65_to_84\", \"pct_Married_couple\", \"pct_Higher_manager_prof\")\n\n# Loop to create scatterplots and calculate correlations \n\nfor (var in variables) { \n  # Scatterplot \n  ggplot(census, aes_string(x = var, y = \"pct_Very_bad_health\")) +\n    geom_point() + \n    labs(title = paste(\"Scatterplot of pct_Very_bad_health vs\", var), \n         x = var, y = \"pct_Very_bad_health\") +\n    theme_minimal()\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nQ2. Which of the associations do you think is strongest, which one is the weakest?\nAs noted, before, an observed association between two variables is no guarantee of causation. It could be that the observed association is:\n\nsimply a chance one due to sampling uncertainty;\ncaused by some third underlying variable which explains the spatial variation of both of the variables in the scatterplot;\ndue to the inherent arbitrariness of the boundaries used to define the areas being analysed (the ‘Modifiable Area Unit Problem’).\n\nQ3. Setting these caveats to one side, are the associations observed in the scatter-plots suggestive of any causative mechanisms of bad health?\nRather than relying upon an impressionistic view of the strength of the association between two variables, we can measure that association by calculating the relevant correlation coefficient. The Table below identifies the statistically appropriate measure of correlation to use between two continuous variables.\n\n\n\n\n\n\n\n\nVariable Data Type\nMeasure of Correlation\nRange\n\n\n\n\nBoth symmetrically distributed\nPearson’s\n-1 to +1\n\n\nOne or both with a skewed distribution\nSpearman’s Rank\n-1 to +1\n\n\n\nDifferent Calculation Methods: Pearson’s correlation assumes linear relationships and is suitable for symmetrically distributed (normally distributed) variables, measuring the strength of the linear relationship. Spearman’s rank correlation, however, works on ranked data, so it’s more suitable for skewed data or variables with non-linear relationships, measuring the strength and direction of a monotonic relationship.\nIf you’re calculating the correlation for a single pair of variables, pick the method that best matches the data distribution for that specific pair:\n-   Use **Pearson’s** if both variables are symmetrically distributed.\n-   Use **Spearman’s** if one or both variables are skewed.\n\n\n\n\n\n\n\n\n\nYou can check the distribution of a variable (e.g. pct_No_qualifications like this):\n\n# Plot histogram with density overlay for a chosen variable (e.g., 'pct_No_qualifications')\nggplot(census, aes(x = pct_No_qualifications)) + \n    geom_histogram(aes(y = ..density..), bins = 30, color = \"black\", fill = \"skyblue\", alpha = 0.7) +\n    geom_density(color = \"darkblue\", size = 1) +\n    labs(title = \"Distribution of pct_No_qualifications\", x = \"Value\", y =  \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nWhen analyzing multiple pairs of variables, using different measures (Pearson for some pairs, Spearman for others) creates inconsistencies since Pearson and Spearman values aren’t directly comparable in size due to their different calculation methods. To maintain consistency across comparisons, calculate both Pearson’s and Spearman’s correlations for each pair. Check if the trends align (both showing strong, weak, or moderate correlation in the same direction). This consistency check can give confidence that the relationships observed are not dependent on the correlation method chosen. While in a report, you’d typically include only one set of correlations (usually Pearson’s if the relationships appear linear), calculating both can validate that your observations aren’t an artifact of the correlation method.\n\nResearch Question 1: Which of our selected variables are most strongly correlated with % of population with bad health?\n\nTo answer this question, complete the Table below by editing/running this code:.\nPearson correlations\n\npearson_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications,use = \"complete.obs\", method = \"pearson\")\n    \n# Display the results\ncat(\"Pearson Correlation:\", pearson_correlation, \"\\n\")\n\nPearson Correlation: 0.7620569 \n\n\nSpearman correlations:\n\nspearman_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications, use = \"complete.obs\", method = \"spearman\")\n\ncat(\"Spearman Correlation:\", spearman_correlation, \"\\n\")\n\nSpearman Correlation: 0.7785197 \n\n\n\n\n\nCovariates\nPearson\nSpearman\n\n\n\n\npct_Very_bad_health - pct_No_qualifications\n\n\n\n\npct_Very_bad_health - pct_Age_65_to_84\n\n\n\n\npct_Very_bad_health - pct_Married_couple\n\n\n\n\npct_Very_bad_health - pct_Higher_manager_prof\n\n\n\n\n\nWhat can you make of this numbers?\nIf you think you have found a correlation between two variables in our dataset, this doesn’t mean that an association exists between these two variables in the population at large. The uncertainty arises because, by chance, the random sample included in our dataset might not be fully representative of the wider population.\nFor this reason, we need to verify whether the correlation is statistically significant,\n\n# significance test for pearson, for example\npearson_test &lt;- cor.test(census$pct_Very_bad_health,\n    census$pct_No_qualifications, method = \"pearson\", use = \"complete.obs\")\npearson_test\n\n\n    Pearson's product-moment correlation\n\ndata:  census$pct_Very_bad_health and census$pct_No_qualifications\nt = 21.347, df = 329, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7128142 0.8038232\nsample estimates:\n      cor \n0.7620569 \n\n\nLook at https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor.test for details about the function. But in general, when calculating the correlation between two variables, a p-value accompanies the correlation coefficient to indicate the statistical significance of the observed association. This p-value tests the null hypothesis that there is no association between the two variables (i.e., that the correlation is zero).\nIn interpreting p-values, certain thresholds denote different levels of confidence. A p-value less than 0.05 is generally considered statistically significant at the 95% confidence level, suggesting that we can be 95% confident there is an association between the variables in the broader population. When the p-value is below 0.01, the result is significant at the 99% confidence level, meaning we have even greater confidence (99%) that an association exists. Sometimes, on reasearch papers or tables significance levels are denoted with asterisks: one asterisk (*) typically indicates significance at the 95% level (p &lt; 0.05), while two asterisks (**) denote significance at the 99% level (p &lt; 0.01).\nTypically, p-values are reported under labels such as “Sig (2-tailed),” where “2-tailed” refers to the fact that the test considers both directions (positive and negative correlations). Reporting the exact p-value (e.g., p = 0.001) is more informative than using thresholds alone, as it gives a clearer picture of how strongly the data contradicts the null hypothesis of no association.\nIn a nutshell, lower p-values suggest a stronger statistical basis for believing that an observed correlation is not due to random chance. A statistically significant p-value reinforces confidence that an association likely exists in the wider population, though it does not imply causation.\n\n\n2.1.3 Part. 2: Implementing a Linear Regression Model\nA key goal of data analysis is to explore the potential factors of health at the local district level. So far, we have used cross-tabulations and various bivariate correlation analysis methods to explore the relationships between variables. One key limitation of standard correlation analysis is that it remains hard to look at the associations of an outcome/dependent variable to multiple independent/predictor variables at the same time. Regression analysis provides a very useful and flexible methodological framework for such a purpose. Therefore, we will investigate how various local factors impact residents’ health by building a multiple linear regression model in R.\nWe use pct_Very_bad_health as a proxy for residents’ health.\n\nResearch Question 2: How do local factors affect residents’ health?\n\nDependent (or Outcome) Variable:\n\n% of population with bad health (pct_Very_bad_health).\n\nIndependent (or Predictor) Variables:\n\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nLoad some other Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\n\nand the data (if not loaded):\n\n# Load dataset\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\")\n\nRegression models are the standard method for constructing predictive and explanatory models. They tell us how changes in one variable (the target variable or independent variable, \\(y\\)) are associated with changes in explanatory variables, or dependent variables, \\(x_1, x_2, x_3\\), etc. Classic linear regression is referred to Ordinary least squares (OLS) regression because they estimate the relationship between one or more independent variables and a dependent variable \\(y\\) using a hyperplane (i.e. a multi-dimensional line) that minimises the sum of the squared difference between the observed values of y and the values predicted by the model (denoted as \\(\\hat{y}\\), \\(y\\)-hat).\nHaving seen Single Linear Regression in class - where the relationship between one independent variable and a dependent variable is modeled - we can extend this concept to situations where more than one predictor might influence the outcome. While single linear regression helps us understand the effect of ONE variable in isolation, real-world phenomena are often influenced by multiple factors simultaneously. Multiple linear regression addresses this complexity by allowing us to model the relationship between a dependent variable and multiple independent variables, providing a more comprehensive view of how various predictors contribute to changes in the outcome.\nHere, regression allows us to examine the relationship between people’s health rates and multiple predictors.\n\n\n2.1.4 Model fit\n\n# Linear regression model\nmodel &lt;- lm(pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof, data = census)\nsummary(model)\n\n\nCall:\nlm(formula = pct_Very_bad_health ~ pct_No_qualifications + pct_Males + \n    pct_Higher_manager_prof, data = census)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.49028 -0.13686 -0.03516  0.09834  0.76575 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)              4.017987   0.880039   4.566 7.06e-06 ***\npct_No_qualifications    0.052959   0.005911   8.959  &lt; 2e-16 ***\npct_Males               -0.073920   0.017850  -4.141 4.40e-05 ***\npct_Higher_manager_prof -0.013088   0.004938  -2.650  0.00843 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2133 on 327 degrees of freedom\nMultiple R-squared:  0.6105,    Adjusted R-squared:  0.6069 \nF-statistic: 170.8 on 3 and 327 DF,  p-value: &lt; 2.2e-16\n\n\nCode explanation\nlm() Function:\n\nlm() stands for “linear model” and is used to fit a linear regression model in R.\nThe formula syntax pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof specifies a relationship between:\n\nDependent Variable: pct_Very_bad_health.\nIndependent Variables: pct_No_qualifications, pct_Males, and pct_Higher_manager_prof. The model is trained on the data dataset.\n\n\nStoring the Model: The model &lt;- syntax stores the fitted model in an object called model.\nsummary(model) provides a detailed output of the model’s results, including:\n\nCoefficients: Estimates of the regression slopes (i.e., how each predictor affects pct_Very_bad_health).\nStandard Errors: The variability of each coefficient estimate.\nt-values and p-values: Indicate the statistical significance of each predictor.\n\nR-squared and Adjusted R-squared: Show how well the independent variables explain the variance in the dependent variable.\nF-statistic: Tests the overall significance of the model.\n\nRegression Coefficients\n\n# Regression coefficients\ncoefficients &lt;- tidy(model)\ncoefficients\n\n# A tibble: 4 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.02     0.880        4.57 7.06e- 6\n2 pct_No_qualifications     0.0530   0.00591      8.96 2.54e-17\n3 pct_Males                -0.0739   0.0179      -4.14 4.40e- 5\n4 pct_Higher_manager_prof  -0.0131   0.00494     -2.65 8.43e- 3\n\n\nWe focus on key output metrics:\n\nAdjusted R-squared: Indicates how well the model explains the variance in the outcome variable.\nCoefficients: Estimate the average change in bad health rates for a one-unit increase in each predictor, assuming other variables are held constant.\n\n\n\n2.1.5 Interpreting the Results\nThe t tests of regression coefficients are used to judge the statistical inferences on regression coefficients, i.e. associations between independent variables and the outcome variable. For a t-statistic of a predictor variable there is a corresponding p-value. It is much easier to judge the statistical inference on regression coefficient by comparing the p value to the conventional level of significance of 0.05; as discussed above:\n\nIf the p-value of a coefficient is smaller than 0.05, the coefficient is statistically significant. In this case, you can say that the relationship between this independent variable and the outcome variable is statistically significant.\nIf the p-value of a coefficient is larger than 0.05, the coefficient is not statistically significant. In this case, you can say or conclude that there is no evidence of an association or relationship between this independent variable and the outcome variable.\n\n\n# Format regression results for reporting\nresults_table &lt;- coefficients %&gt;%\n  select(term, estimate, statistic, p.value)\nresults_table\n\n# A tibble: 4 × 4\n  term                    estimate statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.02        4.57 7.06e- 6\n2 pct_No_qualifications     0.0530      8.96 2.54e-17\n3 pct_Males                -0.0739     -4.14 4.40e- 5\n4 pct_Higher_manager_prof  -0.0131     -2.65 8.43e- 3\n\n\nQ4. Complete the table above by filling in the coefficients, t-values, p-values, and indicating if each variable is statistically significant.\n\n\n\n\n\n\n\n\n\n\nVariable Name\nCoefficients\nt-values\np-values\nSignificant?\n\n\n\n\npct_No_qualifications\n\n\n\n\n\n\npct_Males\n\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\n\n\n2.1.6 Interpretation of regression coefficients or covariate effects\n\n2.1.6.1 Interpretation of the Intercept or Constant term\nFrom the lecture notes, you know that the Intercept or Constant represents the estimated average value of the outcome variable when the values of all independent variables are equal to zero.\nQ5. When values of pct_Males, pct_No_qualifications and pct_Higher_manager_prof are all \\(zero\\), what is the % of population with very bad health? Is the intercept term meaningful? Think about whether there are districts with zero percentages of persons with no qualification. Are there any districts (or units, depending on the dataset you chose) with zero percentages of persons with no qualification in your data set?\n\n\n2.1.6.2 Interpretation of regression slopes\nThe regression coefficient of an independent variable is the estimated average change in \\(Y\\) for a one unit change in \\(X\\), when all other explanatory variables are held constant. There are two key points worth mentioning:\n\nThe unit of \\(X\\) and \\(Y\\): you need to know what the units are of the independent and dependent variables. For instance, one unit could be one year if you have an age variable, or a one percentage point if the variable is measured in percentages (all the variables in this week’s practical).\nAll the other explanatory variables are held constant. It means that the coefficient of an explanatory variable \\(x_1\\) (e.g. \\(b_1\\)) should be interpreted as: a one unit change in\\(x_1\\) is associated with b1 units change in \\(Y\\), keeping other values of explanatory variables (e.g. \\(x_2\\), \\(x_3\\)) constant – for instance, \\(x_2\\)= 0.1 or \\(x_3\\)= 0.4.\n\nQ6. Interpret the regression coefficients of pct_Males, pct_No_qualifications and pct_Higher_manager_prof. Do they make sense?\n\n\n\n2.1.7 Identify factors of % bad health\nNow combine the above two sections and identify factors affecting the percentage of population with very bad health. Fill in each row for the direction (positive or negative) and significance level of each variable.\n\n\n\n\n\n\n\n\nVariable Name\nPositive or Negative\nStatistical Significance\n\n\n\n\npct_No_qualifications\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\npct_Males\n\n\n\n\n\nQ5. Think about the potential conclusions that can be drawn from the above analyses. Try to answer the research question of this practical: How do local factors affect residents’ health?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "href": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "title": "2  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "2.2 Part C: Practice and Extension",
    "text": "2.2 Part C: Practice and Extension\nIf you haven’t understood something, if you have doubts, even if they seem silly, ask.\n\nFinish working through the practical.\nRevise the material.\nExtension activities (optional): Think about other potential factors of long-term illness and test your ideas with new linear regression models.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  }
]