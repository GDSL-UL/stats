[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the Social World - Quantitative Block: Statistics",
    "section": "",
    "text": "Welcome\nThis is the website for “Exploring the Social World - Quantitative Block: Statistics” (module ENVS225) at the University of Liverpool. This block of the module is designed and delivered by Dr. Gabriele Filomena and Dr. Zi Ye from the Geographic Data Science Lab at the University of Liverpool. The module seeks to provide hands-on experience and training in introductory statistics for human geographers.\nThe website is free to use and is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International. A compilation of this web course is hosted as a GitHub repository that you can access:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Exploring the Social World - Quantitative Block: Statistics",
    "section": "Contact",
    "text": "Contact\n\nGabriele Filomena - gfilo [at] liverpool.ac.uk Lecturer in Geographic Data Science Office 1xx, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.\n\n\nZi Ye - zi.ye [at] liverpool.ac.uk Lecturer in Geographic Information Science Office 107, Roxby Building, University of Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "general/overview.html",
    "href": "general/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Aim and Learning Objectives\nThis sub-module aims to provide training and skills on a set of basic quantitative research methods for data collection, analysis, and interpretation. You will learn how to define coherent, relevant research questions, utilise various research quantitative methods, and identify appropriate methodologies to tackle your research questions. This block serves as the foundation for the dissertation and fieldwork modules.\nBackground\nData and research are key pillars of the global economy and society today. We need rigorous approaches to collecting and analysing both the statistics that can tell us ‘how much’ and if there are observable relationships between phenomena; and the information gives us a nuanced understanding of cultural contexts and human dynamics. Quantitative skills enable us to explore and measure socio-economic activities and processes at large scales, while qualitative skills enable understanding of social, cultural, and political contexts and diverse lived experiences. Rather than being in opposition, qualitative and quantitative research can complement one another in the investigation of today’s pressing research questions.\nTo these ends, this block will help you develop your quantitative (statistical) skills, as critical tools. This course will help you understand what quantitative statistical researchers use and develop a set of research techniques that can be used in your field classes and dissertations.\nLearning objectives:",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#aim-and-learning-objectives",
    "href": "general/overview.html#aim-and-learning-objectives",
    "title": "Overview",
    "section": "",
    "text": "Understand how to explore a dataset, containing a number of observations described by a set of variables.\nDemonstrate an understanding in the application and interpretation of commonly used quantitative research methods.\nDemonstrate an understanding of how to work with quantitative data to address real-world research questions.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#module-structure",
    "href": "general/overview.html#module-structure",
    "title": "Overview",
    "section": "Module Structure",
    "text": "Module Structure\nStaff: Dr Zi Ye and Dr Gabriele Filomena\nWhere and When\nWeek 1, 2, 4, 6 Tuesday @ Central Teaching Hub PCTC\n\nLecture (10 – 11 am).\nPractical PC session (11 am – 1 pm).\n\nWeek 3\n\nLecture (3 – 4 pm) Thursday @ Central Teaching Hub – Lect. Theatre C.\nPractical PC session (9 – 11 am) Friday @ Central Teaching Hub PCTC.\n\nWeek 5\n\nLecture (3 – 4 pm) Thursday @ Central Teaching Hub – Lect. Theatre C.\nPractical PC session (10 – 12 am) Friday @ Central Teaching Hub PCTC.\n\nLectures will introduce and explain the fundamentals of quantitative methods, with the opportunity to apply the method introduced in the labs later in the week.\nThe computer practical sessions, will give you the chance to use and apply quantitative methods to real-world data. These are primarily self-directed sessions, but with support on hand if you get stuck. Support and training in R will be provided through these sessions. Weekly sessions will be driven by empirical research questions.\n\n\n\n\n\n\n\n\n\nWeek\nTopic\nFormat\nStaff\n\n\n\n\n1\nIntroduction & Review\nLecture and Computer Lab Practical\nGF\n\n\n2\nSingle & Multiple Linear Regression\nLecture and Computer Lab Practical\nGF\n\n\n3\nMultiple Linear Regression with Categorical Variables\nLecture and Computer Lab Practical\nZY\n\n\n4\nLogistic Regression\nLecture and Computer Lab Practical\nZY\n\n\n5\nData Visualisation\nLecture and Computer Lab Practical\nGF\n\n\n6\nSummary and Assessment Support\nLecture and Computer Lab Practical\nZY",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/overview.html#software-and-data",
    "href": "general/overview.html#software-and-data",
    "title": "Overview",
    "section": "Software and Data",
    "text": "Software and Data\nFor quantitative training sessions, ensure you have installed and/or have access to RStudio. To run the analysis and reproduce the code in R, you need the following software installed on your machine:\n\nR-4.2.2 (or later)\nRStudio 2022.12.0-353 (or later)\n\nTo install and update:\n\nR, download the appropriate version from The Comprehensive R Archive Network (CRAN).\nRStudio, download the appropriate version from here.\n\nThis software is already installed on University Machines. But you will need it to run the analysis on your personal devices.\nData\nExample datasets could be accessed through Canvas or (some) on GitHub Repository of the module. These include:\n\n2021 UK Census Data.\n2021 Annulation Population Survey (APS) - only on Canvas.\n2016 Family Resource Survey (FRS) - only on Canvas.\n2011 Sample of Anonymised Records (SAR).\n\nNote: The Annual Population Survey requires the completion of a quiz prior to its usage, as it is licensed.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "general/assessment.html",
    "href": "general/assessment.html",
    "title": "Assessment",
    "section": "",
    "text": "Required Report Structure\nDeadline: Monday 3rd November 2025 - 2pm. Word count: 2000 words - including tables, excluding references.\nThe assignment Data Exploration and Analysis consists of writing a research report using one of the regression techniques learned during the module. The basic idea is to put in practice the methods learned during the quantitative block of the module. You are required to apply a linear or logistic regression model to the data provided for the module. The report needs to include the following sections (in brackets, % of the whole lenght):\nFollow this structure and include ALL these points, do not make your life harder.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#required-report-structure",
    "href": "general/assessment.html#required-report-structure",
    "title": "Assessment",
    "section": "",
    "text": "Introduction\n\nContext: Why is the topic relevant or worth being investigated?\nBrief discussion of existing literature.\nKnowledge gap and Aim.\n1 Research question.\n\nLiterature review\n\nMore detailed Literature review, i.e. what do we already know about this subject\nRationale for including certain predictor variables in the model.\nWhat knowledge gap remains that this article will address? (includes “not studied before in this area”). Note: there is no expectation on totally original research. The focus is on a clean, sensible, data analysis situated in existing ideas.\n\nMethodology:\n\nA brief introduction to the dataset being analysed (who collected it? When? How many responses? etc.)\nA description of the variables chosen to be analysed.\nA description of any transformation made to the original data, i.e. turning a continuous variable of income into intervals, or reducing the number of age groups from 11 to 3.\nA description and justification of the statistical techniques used in the subsequent analysis (i.e. the Multivariate regression model: Multiple or Logistic Linear Regression).\n\nResults and Discussion\n\nDescriptive statistics and summary of the variables employed.\nCorrect interpretation of correlation coefficients.\nUsage and results of an appropriate multivariate regression model.\nInterpretation of the results, including links and contrasts to existing literature.\nSelective illustrations (graphs and tables) to make your findings as clear as possible.\n\nConclusion\n\nSummary of main findings.\nLimitations of study (self-critique).\nHighlight any implications derived from the study.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-to-get-there",
    "href": "general/assessment.html#how-to-get-there",
    "title": "Assessment",
    "section": "How to get there?",
    "text": "How to get there?\nThe first stage is to identify ONE a relevant research question to be addressed. Based on the chosen question, you will need to identify a dependent (or outcome) variable which you want to explain, and at least two relevant independent variables that you can use to explain the chosen dependent variable. The selection of variables should be informed by the literature and empirical evidence.\nTo detail in the Methods Section: Once the variables have been chosen, you will need to describe the data and appropriate type of regression to be used for the analysis. You need to explain any transformation done to the original data source, such as reclassifying variables, or changing variables from continuous to nominal scales. You also need to briefly describe the data use: source of data, year of data collection, indicate the number of records used, state if you are using individual records or geographical units, explain if you are selecting a sample, and any relevant details. You also need to identify type of regression to be used and why.\nTo detail in the Results and Discussion Section: Firstly, you need to provide two types of analyses. First, you need to provide a descriptive analysis of the data. Here you could use tables and/or plots reporting relevant descriptive statistics, such as the mean, median and standard deviation; variable distributions using histograms; and relationships between variables using correlation matrices or scatter plots. Secondly, you need to present an estimated regression model or models and the interpretation of the estimated coefficients. You need a careful and critical analysis of the regression estimates. You should think that you intend to use your regression models to advice your boss who is expecting to make some decisions based on the information you will provide. As part of this process, you need to discuss the model assessment results for the overall model and regression coefficients. Remember to substantiate your arguments using relevant literature and evidence, and present results clearly in tables and graphs.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/assessment.html#how-is-it-graded",
    "href": "general/assessment.html#how-is-it-graded",
    "title": "Assessment",
    "section": "How is it graded?",
    "text": "How is it graded?\n\n\n\n\n\n\n\n\n\n\nGrade\nScore Range\nUG\nDescriptor\nAssignment Expectations\n\n\n\n\nFail\n0-34%\nFail\nInadequate\nLiterature Review: Lacks relevance and fails to justify variable choice. Evidence is irrelevant or missing, providing no support to the research question. Methods: Data is not described, and the regression model is entirely missing. No appropriate statistical method is applied. Results and Discussion: No descriptive statistics, graphs, or tables are provided. Model results and interpretation are absent. Structure and References: Report is disorganized with significant referencing and citation errors throughout.\n\n\nNarrow Fail\n35-39%\nFail\nHighly Deficient\nLiterature Review: Review is present but lacks coherence and fails to justify variable choice. Evidence is poorly aligned with the research question and mostly irrelevant. Methods: Minimal data description; the regression model is missing but some statistical methods are mentioned. Results and Discussion: Few or no descriptive statistics or visuals are present. Statistical methods are unclear or incorrectly applied. Results are vague and lack meaningful interpretation. Structure and References: Report structure is poor, with referencing errors in multiple sections.\n\n\nThird / Fail\n40-49%\nThird (UG)\nDeficient\nLiterature Review: Relevant literature is partially addressed but lacks depth, with limited justification for variable choice. Evidence is minimally aligned with the research question. Methods: A very basic data description is provided, but the selected regression model is deeply inadequate or incorrect (e.g., multiple linear regression for a categorical outcome; logistic regression for a continuous outcome). Results and Discussion: Descriptive statistics or visuals may be present but insufficient. Model results are presented with little to no interpretation. Structure and References: Report structure is present but lacks clarity, with inconsistencies in citations and citation style.\n\n\n2.2 / Pass\n50-59%\n2.2 (UG)\nAdequate\nLiterature Review: Addresses relevant literature but with limited justification of variable choices. Evidence generally supports the research question but lacks detail. Methods: Data description is present but brief; a regression model is included but applied illogically or incorrectly (e.g., multiple linear regression for a categorical outcome; logistic regression for a continuous outcome) and with little explanation. Results and Discussion: Basic descriptive statistics, graphs, or tables are presented; the regression model is applied with some inaccuracies and/or interpretation is minimal. Structure and References: Report is mostly organized, though with referencing inconsistencies.\n\n\n2.1 / Merit\n60-69%\n2.1 (UG)\nGood\nLiterature Review: Relevant literature is discussed, with some justification for variable choice. Evidence supports the research question well. Methods: Data is described with some detail, though potential data transformations are under-explored. The regression model is appropriate for the selected variable types. Results and Discussion: Descriptive statistics and visuals are provided. Model results are discussed, though interpretation lacks depth. Findings are compared to existing literature. Structure and References: Report is logically structured and clear, with mostly correct citations.\n\n\nFirst / Distinction\n70-79%\nFirst (UG)\nVery Good\nLiterature Review: Strong grasp of relevant literature, with well-justified variable selection. Evidence aligns well with the research question. Methods: Data is comprehensively described with consideration of relevant transformations. The regression model is appropriate and well-justified. Results and Discussion: Descriptive statistics and clear visuals support findings. Model results are accurately interpreted with strong connections to existing literature. Structure and References: Report has a coherent, professional structure with only minor referencing errors.\n\n\nHigh First / High Distinction\n80-100%\nHigh First (UG)\nExcellent to Outstanding\nLiterature Review: Critical and thorough literature review with strong, well-justified variable selection. Evidence fully supports the research question with insightful connections. Methods: Detailed data description and transformation steps are clearly articulated. Regression model is expertly applied and justified. Results and Discussion: Comprehensive descriptive statistics, graphs, and tables are provided. Model results are innovatively interpreted with strong links to existing research. Structure and References: Report is professionally structured, with flawless citations and a high standard of organization.\n\n\n\n\nIn summary:\n\nIntroduction: Should establish the topic’s relevance, present a concise literature overview, identify a knowledge gap, and outline research questions.\nLiterature Review: Requires an in-depth review of relevant studies, justification for chosen independent variables, and identification of a potential knowledge gap or unexplored area aligned with the chosen research question.\nMethods and Data: Should describe the dataset, variable transformations, and justify the regression technique. Key transformations, such as reclassifying variables, should be explained with clarity and relevance.\nResults and Discussion: Involves presenting descriptive statistics, followed by a clear regression analysis. Discussion should interpret results, compare findings with existing literature, and include meaningful tables and graphs.\nConclusion: Summarize findings, discuss limitations, and suggest future directions.\nReferencing: Requires correct and consistent citations and a well-structured reference list.\n\nEmploying a novel dataset, i.e. not employed during the practical sessions, for the assignment will be awarded with a higher grade. For example, see other quantitative dataset from Secondary datasets for Human Geography.",
    "crumbs": [
      "Assessment"
    ]
  },
  {
    "objectID": "general/howSubmit.html",
    "href": "general/howSubmit.html",
    "title": "Assessment: How to submit",
    "section": "",
    "text": "You must submit a .pdf file, that is a rendered version of a Quarto Markdown file (qmd file). This will allow you to write a research paper that also includes your working code, without the need of including the data (rendered .qmd files are executed before being converted to R).\nHow to get a PDF?\n\nInstall Quarto: Make sure you have Quarto installed. You can download it from quarto.org.\nLaTeX Installation: For PDF output, you’ll need a LaTeX distribution like TinyTeX from R, by executing this in the R console:\n\ninstall.packages(\"tinytex\")\ntinytex::install_tinytex()\n\nOpen the Quarto File: Open your .qmd file in RStudio.\nSet Output Format: In the YAML header at the top of your Quarto file, specify pdf under format:\n\n\n\n\n\n\n\n\n\n\n    title: \"Your Document Title\"\n    author: \"Anonymous\" # do not change\n    format: pdf\n\nClick the Render button in the RStudio toolbar (next to the Knit button).",
    "crumbs": [
      "Assessment: How to submit"
    ]
  },
  {
    "objectID": "general/reportTemplate.html",
    "href": "general/reportTemplate.html",
    "title": "Assessment Template",
    "section": "",
    "text": "Setup\nLoad commonly used libraries for statistical analysis and data manipulation.\nCode\n# Core tidyverse\nlibrary(tidyverse)    # includes ggplot2, dplyr, tidyr, readr, purrr, tibble, stringr, forcats\n\n# Tables & reporting\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Stats helpers\nlibrary(broom)\n\n## These need to be installed on your PC.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#literature-review",
    "href": "general/reportTemplate.html#literature-review",
    "title": "Assessment Template",
    "section": "Literature Review",
    "text": "Literature Review\nWhat we know. Summarise key findings from prior studies relevant to your RQ.\nPredictor rationale. Justify the inclusion of variables (theory-driven, prior empirical evidence).\nRemaining gap. Specify precisely the gap this report addresses (e.g., population, geography, time period, variable, method).\nNote: The goal is a clean, sensible analysis grounded in existing ideas—not necessarily novel theory.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#methodology",
    "href": "general/reportTemplate.html#methodology",
    "title": "Assessment Template",
    "section": "Methodology",
    "text": "Methodology\nData. Briefly describe the dataset (who collected it, when, sample size, key variables).\nTransformations. Describe any recoding/aggregation (e.g., bin income into bands; reduce age groups from 11 to 3). Provide justification.\nTechniques. Outline and justify the statistical methods used (e.g., GLM/LM, logistic regression, mixed models, matching).\nImportant: do not include statistics here, or graphs, or tables.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#results-and-discussion",
    "href": "general/reportTemplate.html#results-and-discussion",
    "title": "Assessment Template",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\n\nCode\n# summary variables, distribution, plots, etc\n\n\nPresent the variables Start with descriptive statistics for your variables and distribution. If relevant, include discuss one-to-one associations (briefly, e.g. correlation plots). You can provide correlation plots and/or boxplots but do not overload the report with graphs.***\n\n\nCode\n# run the model, show tables summarising the Regression model.\n\n\nResults + interpretation. Present estimates and interpret them in plain language, tying back to the RQ. The Regression model should the core of this section!! Link to literature. Compare/contrast with prior findings. Selective visuals. Use clear charts/tables to highlight key results only (avoid clutter). ————————————————————————",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#conclusion",
    "href": "general/reportTemplate.html#conclusion",
    "title": "Assessment Template",
    "section": "Conclusion",
    "text": "Conclusion\nSummary. Recap the main findings vis‑à‑vis the RQs (do not introduce new results).\nLimitations. Offer a brief, honest self‑critique (data, measurement, design, external validity).\nImplications. Indicate what the findings suggest for practice, policy, or future research.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/wdPaths.html",
    "href": "general/wdPaths.html",
    "title": "Working Directories and Paths",
    "section": "",
    "text": "Start clean (optional but handy)\n# Clear the environment (objects in memory)\nrm(list = ls())",
    "crumbs": [
      "Working Directories and Paths"
    ]
  },
  {
    "objectID": "general/wdPaths.html#know-where-you-are-working-directory",
    "href": "general/wdPaths.html#know-where-you-are-working-directory",
    "title": "Working Directories and Paths",
    "section": "Know where you are (working directory)",
    "text": "Know where you are (working directory)\nYour working directory (WD) is the folder R uses as the starting point for relative paths.\n\n# Show current working directory\ngetwd()\n\n[1] \"C:/Users/gfilo/OneDrive - The University of Liverpool/Teaching/stats/general\"\n\n\nA relative path is specified from the working directory (e.g., data/myfile.csv).\nAn absolute path starts at the drive root (e.g., C:/Users/you/Documents/stats/data/myfile.csv).",
    "crumbs": [
      "Working Directories and Paths"
    ]
  },
  {
    "objectID": "general/wdPaths.html#recommended-folder-layout-envs225",
    "href": "general/wdPaths.html#recommended-folder-layout-envs225",
    "title": "Working Directories and Paths",
    "section": "Recommended folder layout (ENVS225)",
    "text": "Recommended folder layout (ENVS225)\nDownload the module materials, unzip them them wherever you prefer. Then un-nest the folder with the material (you will have one folder called stats-main containing another folder inside, with the same name. Aim for:\nstats-main/\n├── data/\n├── labs_img/\n└── labs/\nYou can delete other sub-folders (e.g., docs) if you don’t need them. It is strongly advised to move this folder into your M: drive on university machines, so it’s accessible from every other PC on campus. Go to This PC and access M:\\(UoL userName)",
    "crumbs": [
      "Working Directories and Paths"
    ]
  },
  {
    "objectID": "general/wdPaths.html#set-the-working-directory-in-rstudio-every-time-you-use-a-new-pc",
    "href": "general/wdPaths.html#set-the-working-directory-in-rstudio-every-time-you-use-a-new-pc",
    "title": "Working Directories and Paths",
    "section": "Set the working directory in RStudio (every time you use a new PC)",
    "text": "Set the working directory in RStudio (every time you use a new PC)\nWindows: RStudio &gt; Tools &gt; Global Options &gt; General &gt; Default working directory &gt; Browse to your stats-main folder.\nmacOS: RStudio &gt; Preferences &gt; General(older versions: under Appearance/Pane Layout) set Default working directory to stats-main/, this is the folder with all the materials inside.\nThen restart RStudio and verify:\n\ngetwd()\n\n[1] \"C:/Users/gfilo/OneDrive - The University of Liverpool/Teaching/stats/general\"\n\n\nTip: In addition always save your projects inside the folder stats-main. This keeps paths stable and prevents issues.",
    "crumbs": [
      "Working Directories and Paths"
    ]
  },
  {
    "objectID": "general/wdPaths.html#loading-data-csv-excel-with-relative-paths",
    "href": "general/wdPaths.html#loading-data-csv-excel-with-relative-paths",
    "title": "Working Directories and Paths",
    "section": "Loading data (CSV, Excel) with relative paths",
    "text": "Loading data (CSV, Excel) with relative paths\nOption a) Your .qmd project is in stats-main\n\nlibrary(readr)\nlibrary(readxl)\ndf &lt;- read_csv(\"data/survey.csv\")\nxls &lt;- read_excel(\"data/survey.xlsx\", sheet = 1)\n\nOption b) Your .qmd project is in stats-main\\subfolder\n\ndf &lt;- read_csv(\"../data/survey.csv\")\nxls &lt;- read_excel(\"../data/survey.xlsx\", sheet = 1)\n\nIn summary:\n\nread_csv(MyFile.csv) RStudio looks in the working directory for the file MyFile.csv.\nread_csv(MyFolder/MyFile.csv) RStudio looks inside the Working Directory (WD), for the folder MyFolder, then the file MyFile.csv.\nread_csv(data/survey.csv) RStudio looks inside the WD, looks inside data, then survey.csv.\nread_csv(``../data/survey.csv RStudio goes up one folder from the WD, then looks for the folder dataand the file surve.csv.\n\nYou can always inspect where you are by\n\ngetwd()\n\n[1] \"C:/Users/gfilo/OneDrive - The University of Liverpool/Teaching/stats/general\"\n\nlist.files()        # What’s in the working directory?\n\n [1] \"about.qmd\"           \"assessment.html\"     \"assessment.qmd\"     \n [4] \"howSubmit.html\"      \"howSubmit.qmd\"       \"overview.html\"      \n [7] \"overview.qmd\"        \"reportTemplate.html\" \"reportTemplate.qmd\" \n[10] \"setup.qmd\"           \"wdPaths.qmd\"         \"wdPaths.rmarkdown\"  \n\nlist.files(\"data\")  # Do we see the data folder?\n\ncharacter(0)",
    "crumbs": [
      "Working Directories and Paths"
    ]
  },
  {
    "objectID": "labs/00.introR.html",
    "href": "labs/00.introR.html",
    "title": "1  Lab: Introduction to R",
    "section": "",
    "text": "1.1 R?\nThe following material has been readapted from:\nTo install and update:\nR is an open-source program that is commonly used in Statistics. It runs on almost every platform and is completely free and is available at www.r-project.org. Most of the cutting-edge statistical research is first available on R.\nR is a script based language, so there is no point and click interface. While the initial learning curve will be steeper, understanding how to write scripts will be valuable because it leaves a clear description of what steps you performed in your data analysis. Typically you will want to write a script in a separate file and then run individual lines. This saves you from having to retype a bunch of commands and speeds up the debugging process.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R</span>"
    ]
  },
  {
    "objectID": "labs/00.introR.html#rstudio-basics",
    "href": "labs/00.introR.html#rstudio-basics",
    "title": "1  Lab: Introduction to R",
    "section": "1.2 R(Studio) Basics",
    "text": "1.2 R(Studio) Basics\nWe will be running R through the program RStudio which is located at rstudio.com. When you first open up RStudio the console window gives you some information about the version of R you are running and then it gives the prompt &gt;. This prompt is waiting for you to input a command. The prompt + tells you that the current command is spanning multiple lines. In a script file you might have typed something like this:\nfor( i in 1:5 ){\n    print(i)\n}\nFinding help about a certain function is very easy. At the prompt, just type help(function.name) or ?function.name. If you don’t know the name of the function, your best bet is to go the the web page www.rseek.org which will search various R resources for your keyword(s). Another great resource is the coding question and answer site stackoverflow.\n\n1.2.1 Starting a session in RStudio\nUpon startup, RStudio will look something like this.\n\n\n\n\n\n\n\n\n\nNote: the Pane Layout and Appearance settings can be altered:\n\non Windows by clicking RStudio&gt;Tools&gt;Global Options&gt;Appearance or Pane Layout\non Mac OS by clicking RStudio&gt;Preferences&gt;Appearance or Pane Layout.\n\nYou will also have a standard white background; but you can choose specific themes.\nSource Panel (Top-Left)\nThis is where you write, edit, and view scripts, R Markdown/Quarto documents, or R scripts. It allows:\n\nEditing Scripts: Write and edit R scripts or documents (.R, .Rmd, .qmd).\nExecuting the Code: Run lines, blocks, or the entire script directly from the editor.\n\nConsole Panel (Bottom-Left)\nThe Console is the main place to run R commands interactively. It allows:\n\nExecuting the Code: Type and run R commands directly.\nViewing outputs, warnings, and errors for immediate feedback.\nBrowsing and reusing past commands (History Tab).\nToggling between the R Console, and the Terminal (yuo don’t really need the latter).\n\nEnvironment Panel (Top-Right)\nThis panel helps track variables, functions, and the history of commands used. It contains:\n\nEnvironment Tab: Shows all current variables, datasets, and objects in your session, including their structure and values.\nHistory Tab: Provides a record of past commands. You can re-run or move commands to the console or script.\n\nFiles / Plots / Packages / Help Panel (Bottom-Right)\nThis multifunctional panel is for file navigation, plotting, managing packages, viewing help, and managing jobs. It contains:\n\nFiles Tab: Navigate, open, and manage files and directories within your project.\nPlots Tab: Displays plots generated in your session. You can export or navigate through multiple plots here.\nPackages Tab: Lists installed packages and allows you to install, load, and update packages.\nHelp Tab: Displays help documentation for R functions, packages, and other resources. You can search for documentation by typing a function or package name.\n\nAt the start of a session, it’s good practice clearing your R environment (console):\n\nrm(list = ls())\n\nIn R, we are going to workin with relative paths. With the command getwd(), you can see where your working directory is currently set.\n\ngetwd() \n\n\n\n1.2.2 Using the console\nWe can use the console to perform few operations. For example type in:\n\n1+1\n\n[1] 2\n\n\nSlightly more complicated:\n\nprint(\"hello world\")\n\n[1] \"hello world\"\n\n\nIf you are unsure about what a command does, use the “Help” panel in your Files pane or type ?function in the console. For example, to see how the dplyr::rename() function works, type in ?dplyr::rename. When you see the double colon syntax like in the previous command, it’s a call to a package without loading its library.\n\n\n1.2.3 R as a simple calculator\nYou can use R as a simple calculator. At the prompt, type 2+3 and hit enter. What you should see is the following\n\n# Some simple addition\n2+3\n\n[1] 5\n\n\nIn this fashion you can use R as a very capable calculator.\n\n6*8\n\n[1] 48\n\n4^3\n\n[1] 64\n\nexp(1)   # exp() is the exponential function\n\n[1] 2.718282\n\n\nR has most constants and common mathematical functions you could ever want. For example, the absolute value of a number is given by abs(), and round() will round a value to the nearest integer.\n\npi     # the constant 3.14159265...\n\n[1] 3.141593\n\nabs(1.77) \n\n[1] 1.77\n\n\nWhenever you call a function, there will be some arguments that are mandatory, and some that are optional and the arguments are separated by a comma. In the above statements the function abs() requires at least one argument, and that is the number you want the absolute value of.\nWhen functions require more than one argument, arguments can be specified via the order in which they are passed or by naming the arguments. So for the log() function, for example, which calculates the logarithm of a number, one can specify the arguments using the named values; the order woudn’t matter:\n\n# Demonstrating order does not matter if you specify\n# which argument is which\nlog(x=5, base=10)   \n\n[1] 0.69897\n\nlog(base=10, x=5)\n\n[1] 0.69897\n\n\nWhen we don’t specify which argument is which, R will decide that x is the first argument, and base is the second.\n\n# If not specified, R will assume the second value is the base...\nlog(5, 10)\n\n[1] 0.69897\n\nlog(10, 5)\n\n[1] 1.430677\n\n\nWhen we want to specify the arguments, we can do so using the name=value notation.\n\n\n1.2.4 Variables Assignment\nWe need to be able to assign a value to a variable to be able to use it later. R does this by using an arrow &lt;- or an equal sign =. While R supports either, for readability, I suggest people pick one assignment operator and stick with it.\nVariable names cannot start with a number, include spaces, and they are case sensitive.\n\nvar &lt;- 2*7.5       # create two variables\nanother_var = 5   # notice they show up in 'Environment' tab in RStudio!\nvar \n\n[1] 15\n\nvar * another_var \n\n[1] 75\n\n\nAs your analysis gets more complicated, you’ll want to save the results to a variable so that you can access the results later. if you don’t assign the result to a variable, you have no way of accessing the result.\n\n\n1.2.5 Working with Scripts\nNormally you would use the Console for quick calculations or executions. In this module, though, we are going to work with Quarto Markdown Scripts (.qmd files).\nThe R Markdown is an implementation of the Markdown syntax that makes it extremely easy to write webpages or scientific documents that include code. This syntax was extended to allow users to embed R code directly into more complex documents. Perhaps the easiest way to understand the syntax is to look at an at the RMarkdown website. The R code in a R Markdown document (.rmd file extension) can be nicely separated from regular text using the three backticks (3 times `, see below) and an instruction that it is R code that needs to be evaluated. A code chunk will look like:\n\n    for (i in 1:5) {print(i)}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n** .qmd - the type of scripts we use - are just a more flexible development of .rmd files.**\n\nMarkdown files present several advantages compared to writing your code in the console or just using scripts. You’ll save yourself a huge amount of work by embracing Markdown files from the beginning; you will keep track of your code and your steps, be able to document and present how you did your analysis (helpful when writing the methods section of a paper), and it will make it easier to re-run an analysis after a change in the data (such as additional data values, transformed data, or removal of outliers) or once you spot an error. Finally, it makes the script more readable.\n\n\n1.2.6 R Packages\nOne of the greatest strengths about R is that so many people have developed add-on packages to do some additional function. To download and install the package from the Comprehensive R Archive Network (CRAN), you just need to ask RStudio it to install it via the menu Tools -&gt; Install Packages.... Once there, you just need to give the name of the package and RStudio will download and install the package on your computer.\nOnce a package is downloaded and installed on your computer, it is available, but it is not loaded into your current R session by default. To improve overall performance only a few packages are loaded by default and the you must explicitly load packages whenever you want to use them. You only need to load them once per session/script.\n\nlibrary(dplyr)   # load the dplyr library, will be useful later\n\nThis is just a quick intro to R, now move to the actual practical of week 1.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Lab: Introduction to R</span>"
    ]
  },
  {
    "objectID": "labs/01.DataExploration.html",
    "href": "labs/01.DataExploration.html",
    "title": "2  Lab: Exploring a Dataset",
    "section": "",
    "text": "2.1 Practice: Dataset and Dataframes\nWithin this module we will be working with data stored in so-called datasets. A dataset is a structured collection of data points that represent various measurements or observations, often organized in a tabular format with rows and columns. A dataset might contain information about different locations, such as neighborhoods or cities, with each row representing a place and each column detailing characteristics like population density, average income, or number of green parks. For example, a dataset could be compiled to study patterns in urban mobility, where the data includes the number of daily commuters, the distance they travel, and the mode of transport they use. Datasets provide the essential building blocks for statistical analysis; they enable exploring relationships, identifying patterns, and drawing conclusions about certain phenomena.\nExamples of everyday datasets:\nUsually, data is organized in\nFor example, in a grade book for recording students scores throughout the semester, their is one row for every student and columns for each assignment. A greenhouse experiment dataset will have a row for every plant and columns for treatment type and biomass.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploring a Dataset</span>"
    ]
  },
  {
    "objectID": "labs/01.DataExploration.html#practice-dataset-and-dataframes",
    "href": "labs/01.DataExploration.html#practice-dataset-and-dataframes",
    "title": "2  Lab: Exploring a Dataset",
    "section": "",
    "text": "Premier League Standings: Each row represents a team, with columns for points, games played, wins, draws, and losses.\nMovie Dataset: Each row represents a movie, with columns showing its title, genre, release year, director, and rating.\nWeather Dataset: Each row shows a day’s weather in a city, with columns for temperature, humidity, wind speed, and precipitation.\n\n\n\nColumns of data representing some trait or variable that we might be interested in. In general, we might wish to investigate the relationship between variables.\nRows represent a single object on which the column traits are measured.\n\n\n\n2.1.1 Datasets in R\nIn R, we want a way of storing data where it feels just as if we had an Excel Spreadsheet where each row represents an observation and each column represents some information about that observation. We will call this object a data.frame, an R represention of a data set. The easiest way to understand data frames is to create one.\n\nTask: Copy the code below in your markdown. Create a data.frame that represents an instructor’s grade book, where each row is a student, and each column represents some sort of assessment.\n\n\nlibrary(dplyr)\n\nGrades &lt;- data.frame(\n  Name  = c('Bob','Jeff','Mary','Valerie'),     \n  Exam.1 = c(90, 75, 92, 85),\n  Exam.2 = c(87, 71, 95, 81)\n)\n# Show the data.frame \n# View(Grades)  # show the data in an Excel-like tab.  Doesn't work when knitting \nGrades          # show the output in the console. This works when knitting\n\n     Name Exam.1 Exam.2\n1     Bob     90     87\n2    Jeff     75     71\n3    Mary     92     95\n4 Valerie     85     81\n\n\nTo execute just one chunk of code press the green arrow top-right of the chunk:\n\n\n\n\n\n\n\n\n\nR allows two differnt was to access elements of the data.frame. First is a matrix-like notation for accessing particular values.\n\n\n\nFormat\nResult\n\n\n\n\n[a,b]\nElement in row a and column b\n\n\n[a,]\nAll of row a\n\n\n[,b]\nAll of column b\n\n\n\nBecause the columns have meaning and we have given them column names, it is desirable to want to access an element by the name of the column as opposed to the column number.\n\nTask: Copy and Run:\n\n\nGrades[, 2]       # print out all of column 2 \n\n[1] 90 75 92 85\n\nGrades$Name       # The $-sign means to reference a column by its label\n\n[1] \"Bob\"     \"Jeff\"    \"Mary\"    \"Valerie\"\n\n\n\n\n2.1.2 Importing Data in R\nUsually we won’t type the data in by hand, but rather load the data from some package. Reading data from external sources is a necessary skill.\nComma Separated Values Data\nTo consider how data might be stored, we first consider the simplest file format: the comma separated values file (.csv). In this file type, each of the “cells” of data are separated by a comma. For example, the data file storing scores for three students might be as follows:\nAble, Dave, 98, 92, 94\nBowles, Jason, 85, 89, 91\nCarr, Jasmine, 81, 96, 97\nTypically when you open up such a file on a computer with MS Excel installed, Excel will open up the file assuming it is a spreadsheet and put each element in its own cell. However, you can also open the file using a more primitive program (say Notepad in Windows, TextEdit on a Mac) you’ll see the raw form of the data.\nHaving just the raw data without any sort of column header is problematic (which of the three exams was the final??). Ideally we would have column headers that store the name of the column.\nLastName, FirstName, Exam1, Exam2, FinalExam\nAble, Dave, 98, 92, 94\nBowles, Jason, 85, 89, 91\nCarr, Jasmine, 81, 96, 97\nReading (.csv) files\nTo make R read in the data arranged in this format, we need to tell R three things:\n\nWhere does the data live? Often this will be the name of a file on your computer, but the file could just as easily live on the internet (provided your computer has internet access).\nIs the first row data or is it the column names?\nWhat character separates the data? Some programs store data using tabs to distinguish between elements, some others use white space. R’s mechanism for reading in data is flexible enough to allow you to specify what the separator is.\n\nThe primary function that we’ll use to read data from a file and into R is the function read.csv(). This function has many optional arguments but the most commonly used ones are outlined in the table below.\n\n\n\n\n\n\n\n\nArgument\nDefault\nDescription\n\n\n\n\nfile\nRequired\nA character string denoting the file location.\n\n\nheader\nTRUE\nSpecifies whether the first line contains column headers.\n\n\nsep\n\",\"\nSpecifies the character that separates columns. For read.csv(), this is usually a comma.\n\n\nskip\n0\nThe number of lines to skip before reading data; useful for files with descriptive text before the actual data.\n\n\nna.strings\n\"NA\"\nValues that represent missing data; multiple values can be specified, e.g., c(\"NA\", \"-9999\").\n\n\nquote\n\"\nSpecifies the character used to quote character strings, typically \" or '.\n\n\nstringsAsFactors\nFALSE\nControls whether character strings are converted to factors; FALSE means they remain as character data.\n\n\nrow.names\nNULL\nAllows specifying a column as row names, or assigning NULL to use default indexing for rows.\n\n\ncolClasses\nNULL\nSpecifies the data type for each column to speed up reading for large files, e.g., c(\"character\", \"numeric\").\n\n\nencoding\n\"unknown\"\nSets the text encoding of the file, which can be useful for files with special or international characters.\n\n\n\nMost of the time you just need to specify the file. |\n\nTask: Let’s read in a dataset of terrorist attacks that have taken place in the UK:\n\n\nattacks &lt;- read.csv(file   = '../data/attacksUK.csv')  # where the data lives                                 \nView(attacks)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploring a Dataset</span>"
    ]
  },
  {
    "objectID": "labs/01.DataExploration.html#practice-descriptive-statistics",
    "href": "labs/01.DataExploration.html#practice-descriptive-statistics",
    "title": "2  Lab: Exploring a Dataset",
    "section": "2.2 Practice: Descriptive Statistics",
    "text": "2.2 Practice: Descriptive Statistics\n\n2.2.1 Summarizing Data\nIt is very important to be able to take a data set and produce summary statistics such as the mean and standard deviation of a column. For this sort of manipulation, we use the package dplyr. This package allows chaining together many common actions to form a particular task.\nThe fundamental operations to perform on a data set are:\n\nSubsetting - Returns a dataframe with only particular columns or rows\n– select - Selecting a subset of columns by name or column number.\n– filter - Selecting a subset of rows from a data frame based on logical expressions.\n– slice - Selecting a subset of rows by row number.\narrange - Re-ordering the rows of a data frame.\nmutate - Add a new column that is some function of other columns.\nsummarise - calculate some summary statistic of a column of data. This collapses a set of rows into a single row.\n\nEach of these operations is a function in the package dplyr. These functions all have a similar calling syntax:\n\nThe first argument is a data set.\nSubsequent arguments describe what to do with the input data frame and you can refer to the columns without using the df$column notation.\n\nAll of these functions will return a data set.\nLet’s consider the summarize function to calculate the mean score for Exam.1. Notice that this takes a data frame of four rows, and summarizes it down to just one row that represents the summarized data for all four students.\n\nlibrary(dplyr) # load the library\nGrades %&gt;%\n  summarize( Exam.1.mean = mean( Exam.1 ) )\n\n  Exam.1.mean\n1        85.5\n\n\nSimilarly you could calculate the standard deviation for the exam as well.\n\nGrades %&gt;%\n  summarize( Exam.1.mean = mean( Exam.1 ),\n             Exam.1.sd   = sd(   Exam.1   ) )\n\n  Exam.1.mean Exam.1.sd\n1        85.5  7.593857\n\n\n\nTask: Write the code above in your markdown file and run it. Do not to copy it this time.\n\nThe %&gt;% operator works by translating the command a %&gt;% f(b) to the expression f(a,b). This operator works on any function f. This is useful when we want to start with x, and first apply a function f(), then g(), and then h(); the usual R command would be h(g(f(x))) which is hard to read. Using the pipe command %&gt;%, this sequence of operations becomes x %&gt;% f() %&gt;% g() %&gt;% h().\nBelow, the code takes the Grades dataframe and calculates a column for the average exam score, and then sorts the data according to the that average score\n\nGrades %&gt;%   mutate( Avg.Score = (Exam.1 + Exam.2) / 2 ) %&gt;%   arrange( Avg.Score )\n\n     Name Exam.1 Exam.2 Avg.Score\n1    Jeff     75     71      73.0\n2 Valerie     85     81      83.0\n3     Bob     90     87      88.5\n4    Mary     92     95      93.5\n\n\nYou don’t have to memorise this.\nLet’s go back to the terrorist attacks dataframe. There are attacks perpetrated by several different groups. Each record is a single attack and contains information about who perpetrated the attack, what year, how many were killed and how many were wounded. You can get a glimpse of the dataframe with the function head\n\nhead(attacks, n = 10)\n\n   nrKilled nrWound year        country                        group\n1         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n2         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n3         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n4         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades\n5         0       1 1982 United Kingdom Abu Nidal Organization (ANO)\n6         0       0 2014 United Kingdom                   Anarchists\n7         0       0 2014 United Kingdom                   Anarchists\n8         0       0 2014 United Kingdom                   Anarchists\n9         0       0 2014 United Kingdom                   Anarchists\n10        0       0 2014 United Kingdom                   Anarchists\n                           attack                      target\n1               Bombing/Explosion              Transportation\n2               Bombing/Explosion              Transportation\n3               Bombing/Explosion              Transportation\n4               Bombing/Explosion              Transportation\n5                   Assassination     Government (Diplomatic)\n6  Facility/Infrastructure Attack                    Business\n7  Facility/Infrastructure Attack                    Business\n8  Facility/Infrastructure Attack                    Business\n9  Facility/Infrastructure Attack Private Citizens & Property\n10 Facility/Infrastructure Attack                      Police\n                      weapon\n1  Explosives/Bombs/Dynamite\n2  Explosives/Bombs/Dynamite\n3  Explosives/Bombs/Dynamite\n4  Explosives/Bombs/Dynamite\n5                   Firearms\n6                 Incendiary\n7                 Incendiary\n8                 Incendiary\n9                 Incendiary\n10                Incendiary\n\n\nWe might want to compare different actors and see the mean and standard deviation of the number of people wound, by each group’s attack, across time. To do this, we are still going to use the summarize, but we will precede that with group_by(group) to tell the subsequent dplyr functions to perform the actions separately for each breed.\n\nattacks %&gt;%\n  group_by( group) %&gt;%\n  summarise( Mean = mean(attacks$nrWound), \n             Std.Dev = sd(attacks$nrWound))\n\n# A tibble: 38 × 3\n   group                                               Mean Std.Dev\n   &lt;chr&gt;                                              &lt;dbl&gt;   &lt;dbl&gt;\n 1 Abu Hafs al-Masri Brigades                         0.963    7.22\n 2 Abu Nidal Organization (ANO)                       0.963    7.22\n 3 Anarchists                                         0.963    7.22\n 4 Animal Liberation Front (ALF)                      0.963    7.22\n 5 Animal Rights Activists                            0.963    7.22\n 6 Armenian Secret Army for the Liberation of Armenia 0.963    7.22\n 7 Black September                                    0.963    7.22\n 8 Continuity Irish Republican Army (CIRA)            0.963    7.22\n 9 Dissident Republicans                              0.963    7.22\n10 Informal Anarchist Federation                      0.963    7.22\n# ℹ 28 more rows\n\n\n\nTask: Write the code above in your markdown file and run it. Try out another categorical variable instead of group (e.g. year) and nrKilled instead of nrWound.\n\nLet’s now move to another dataset to address a research question.\nFor illustration purposes, we will use the Family Resources Survey (FRS). The FRS is an annual survey conducted by the UK government that collects detailed information about the income, living conditions, and resources of private households across the United Kingdom. Managed by the Department for Work and Pensions (DWP), the FRS provides data that is essential for understanding the economic and social conditions of households and informing public policy.\nConsider questions such as:\n\nHow many respondents (persons) are there in the 2016-17 FRS?\nHow many variables (population attributes) are there?\nWhat types of variables are present in the FRS?\nWhat is the most detailed geography available in the FRS?\n\n\nTask: To answer these questions, download from Canvas, save in the data folder, load and inspect the dataset.\n\n\n# the FRS dataset should be already loaded, otherwise\nfrs_data &lt;- read.csv(\"../data/FRS/FRS16-17_labels.csv\") \n\n# Display basic structure \nglimpse(frs_data)\n\nRows: 44,145\nColumns: 45\n$ household        &lt;int&gt; 6087, 6101, 6103, 6122, 6134, 6136, 6138, 6140, 6143,…\n$ family           &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ person           &lt;int&gt; 5, 3, 3, 3, 2, 4, 4, 3, 3, 4, 4, 3, 4, 2, 5, 3, 4, 3,…\n$ country          &lt;chr&gt; \"England\", \"England\", \"England\", \"Northern Ireland\", …\n$ region           &lt;chr&gt; \"London\", \"South East\", \"Yorks and the Humber\", \"Nort…\n$ age_group        &lt;chr&gt; \"05-10\", \"05-10\", \"05-10\", \"05-10\", \"05-10\", \"05-10\",…\n$ sex              &lt;chr&gt; \"Female\", \"Male\", \"Male\", \"Female\", \"Female\", \"Female…\n$ marital_status   &lt;chr&gt; \"Single\", \"Single\", \"Single\", \"Single\", \"Single\", \"Si…\n$ ethnicity        &lt;chr&gt; \"Mixed / multiple ethnic groups\", \"White\", \"White\", \"…\n$ hrp              &lt;chr&gt; \"Not HRP\", \"Not HRP\", \"Not HRP\", \"Not HRP\", \"Not HRP\"…\n$ rel_to_hrp       &lt;chr&gt; \"Son/daughter (incl. adopted)\", \"Son/daughter (incl. …\n$ lifestage        &lt;chr&gt; \"Child (0-17)\", \"Child (0-17)\", \"Child (0-17)\", \"Chil…\n$ dependent        &lt;chr&gt; \"Dependent\", \"Dependent\", \"Dependent\", \"Dependent\", \"…\n$ arrival_year     &lt;chr&gt; \"UK Born\", \"UK Born\", \"UK Born\", \"UK Born\", \"UK Born\"…\n$ birth_country    &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ care_hours       &lt;chr&gt; \"0 hours per week\", \"0 hours per week\", \"0 hours per …\n$ educ_age         &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ educ_type        &lt;chr&gt; \"School (full-time)\", \"School (full-time)\", \"School (…\n$ fam_youngest     &lt;chr&gt; \"7\", \"4\", \"0\", \"7\", \"0\", \"9\", \"10\", \"0\", \"3\", \"10\", \"…\n$ fam_toddlers     &lt;int&gt; 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,…\n$ fam_size         &lt;int&gt; 4, 4, 4, 3, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 4, 4,…\n$ happy            &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ health           &lt;chr&gt; \"Not known\", \"Not known\", \"Not known\", \"Not known\", \"…\n$ hh_accom_type    &lt;chr&gt; \"Terraced house/bungalow\", \"Detached house/bungalow\",…\n$ hh_benefits      &lt;int&gt; 10868, 0, 1768, 8632, 8372, 1768, 1768, 1768, 0, 0, 1…\n$ hh_composition   &lt;chr&gt; \"Three or more adults, 1+ children\", \"One adult femal…\n$ hh_ctax_band     &lt;chr&gt; \"Band D\", \"Band F\", \"Band A\", \"Band B\", \"Band A\", \"Ba…\n$ hh_housing_costs &lt;chr&gt; \"4316\", \"10296\", \"5408\", \"Northern Ireland\", \"5720\", …\n$ hh_income_gross  &lt;int&gt; 54236, 180804, 26936, 19968, 17992, 76596, 31564, 366…\n$ hh_income_net    &lt;int&gt; 44668, 120640, 23556, 19968, 17992, 62868, 29744, 287…\n$ hh_size          &lt;int&gt; 5, 4, 4, 3, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4,…\n$ hh_tenure        &lt;chr&gt; \"Mortgaged (including part rent / part own)\", \"Mortga…\n$ highest_qual     &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ income_gross     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ income_net       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ jobs             &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ life_satisf      &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ nssec            &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ sic_chapter      &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ sic_division     &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ soc2010          &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ work_hours       &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ workstatus       &lt;chr&gt; \"Dependent Child\", \"Dependent Child\", \"Dependent Chil…\n$ years_ft_work    &lt;chr&gt; \"Dependent child\", \"Dependent child\", \"Dependent chil…\n$ survey_weight    &lt;int&gt; 2315, 1317, 2449, 427, 1017, 1753, 1363, 1344, 828, 1…\n\n\nand summary:\n\nsummary(frs_data)\n\n   household         family          person       country         \n Min.   :    1   Min.   :1.000   Min.   :1.00   Length:44145      \n 1st Qu.: 4816   1st Qu.:1.000   1st Qu.:1.00   Class :character  \n Median : 9673   Median :1.000   Median :2.00   Mode  :character  \n Mean   : 9677   Mean   :1.106   Mean   :1.98                     \n 3rd Qu.:14553   3rd Qu.:1.000   3rd Qu.:3.00                     \n Max.   :19380   Max.   :6.000   Max.   :9.00                     \n    region           age_group             sex            marital_status    \n Length:44145       Length:44145       Length:44145       Length:44145      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  ethnicity             hrp             rel_to_hrp         lifestage        \n Length:44145       Length:44145       Length:44145       Length:44145      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  dependent         arrival_year       birth_country       care_hours       \n Length:44145       Length:44145       Length:44145       Length:44145      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   educ_age          educ_type         fam_youngest        fam_toddlers   \n Length:44145       Length:44145       Length:44145       Min.   :0.0000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.0000  \n                                                          Mean   :0.2557  \n                                                          3rd Qu.:0.0000  \n                                                          Max.   :4.0000  \n    fam_size        happy              health          hh_accom_type     \n Min.   :1.000   Length:44145       Length:44145       Length:44145      \n 1st Qu.:2.000   Class :character   Class :character   Class :character  \n Median :2.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2.599                                                           \n 3rd Qu.:4.000                                                           \n Max.   :9.000                                                           \n  hh_benefits    hh_composition     hh_ctax_band       hh_housing_costs  \n Min.   :    0   Length:44145       Length:44145       Length:44145      \n 1st Qu.:    0   Class :character   Class :character   Class :character  \n Median : 1768   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 5670                                                           \n 3rd Qu.:10192                                                           \n Max.   :54080                                                           \n hh_income_gross   hh_income_net        hh_size      hh_tenure        \n Min.   :-326092   Min.   :-334776   Min.   :1.00   Length:44145      \n 1st Qu.:  22256   1st Qu.:  20748   1st Qu.:2.00   Class :character  \n Median :  35984   Median :  31512   Median :3.00   Mode  :character  \n Mean   :  46076   Mean   :  37447   Mean   :2.96                     \n 3rd Qu.:  57252   3rd Qu.:  47008   3rd Qu.:4.00                     \n Max.   :1165216   Max.   :1116596   Max.   :9.00                     \n highest_qual        income_gross       income_net          jobs          \n Length:44145       Min.   :-354848   Min.   :-358592   Length:44145      \n Class :character   1st Qu.:     52   1st Qu.:      0   Class :character  \n Mode  :character   Median :  12740   Median :  12012   Mode  :character  \n                    Mean   :  17305   Mean   :  14204                     \n                    3rd Qu.:  23712   3rd Qu.:  20384                     \n                    Max.   :1127360   Max.   :1110928                     \n life_satisf           nssec           sic_chapter        sic_division      \n Length:44145       Length:44145       Length:44145       Length:44145      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   soc2010           work_hours         workstatus        years_ft_work     \n Length:44145       Length:44145       Length:44145       Length:44145      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n survey_weight  \n Min.   :  221  \n 1st Qu.: 1097  \n Median : 1380  \n Mean   : 1459  \n 3rd Qu.: 1742  \n Max.   :39675  \n\n\n\n\n2.2.2 Understanding the Structure of the FRS Datafile\nIn the FRS data structure, each row represents a person, but:\n\nEach person is nested within a family.\nEach family is nested within a household.\n\nBelow is an example dataset structure:\n\n\n\n\n\n\n\n\n\n\n\n\n\nhousehold\nfamily\nperson\nregion\nage_group\nsex\nmarital_status\nrel_to_hrp\n\n\n\n\n1\n1\n1\nLondon\n40-44\nFemale\nMarried/Civil partnership\nSpouse\n\n\n1\n1\n2\nLondon\n40-44\nMale\nMarried/Civil partnership\nHousehold Representative\n\n\n1\n1\n3\nLondon\n5-10\nMale\nSingle\nSon/daughter (incl. adopted)\n\n\n1\n1\n4\nLondon\n5-10\nFemale\nSingle\nSon/daughter (incl. adopted)\n\n\n1\n1\n5\nLondon\n16-19\nMale\nSingle\nStep-son/daughter\n\n\n2\n1\n1\nScotland\n35-39\nMale\nSingle\nHousehold Representative\n\n\n3\n1\n1\nYorks and the Humber\n35-39\nFemale\nMarried/Civil partnership\nHousehold Representative\n\n\n3\n1\n2\nYorks and the Humber\n35-39\nMale\nMarried/Civil partnership\nSpouse\n\n\n3\n1\n3\nYorks and the Humber\n5-10\nMale\nSingle\nStep-son/daughter\n\n\n4\n1\n1\nWales\n0-4\nMale\nSingle\nSon/daughter (incl. adopted)\n\n\n4\n1\n2\nWales\n60-64\nMale\nMarried/Civil partnership\nHousehold Representative\n\n\n4\n1\n3\nWales\n55-59\nFemale\nMarried/Civil partnership\nSpouse\n\n\n4\n2\n3\nWales\n30-34\nFemale\nSingle\nSon/daughter (incl. adopted)\n\n\n\nThe first five people in the FRS all belong to the same household (household 1); they also all belong to the same family. This family comprises a married middle-aged couple plus their three children, one of whom is a stepson.\nThe second household (household 2) comprises only one person – a single middle-aged male.The third household comprises another married couple, this time with two children.\nSuperficially the fourth household looks similar to households 1 and 2: a married couple plus their daughter. The difference is that this particular married couple is nearing retirement age, and their daughter is middle-aged. Consequently, despite being a child of the married couple, the middle-aged daughter is treated as a separate ‘family’ (family 2 in the household). This is because the FRS (and Census) define a ‘family’ as a couple plus any ‘dependent’ children. A dependent child is defined as a child who is either` aged 0-15 or aged 16-19, unmarried and in full-time education. All children aged 16-19 who are married or no longer in full-time education are regarded as ‘independent’ adults who form their own family unit, as are all children aged 20+.\nThe inclusion of all persons in a household allows us more flexibility in the types of research question we can answer. For example, we could explore how the likelihood of a woman being in paid employment WorkStatus is influenced by the age of the youngest child still living in her family (if any) fam_youngest.\nIn the FRS (and Census), a “family” is defined as a couple and any “dependent” children. Dependent children are defined as those aged 0–15, or aged 16–19 if unmarried and in full-time education.\n\n\n2.2.3 Explore the Distribution of Your Outcome Variable\nBefore starting your analysis, it is critical to know the type of scale used to measure your outcome variable: is it categorical or continuous? Here we will start off by exploring a continuous variable which can then turn into a categorical variable (e.g. top earners: yes or no). We explore the income distribution in the UK by first looking at the low and high end of the distribution ie. What sorts of people have high (or low) incomes?\nIn the FRS each person’s annual income is recorded, both gross (pre-tax) and net (post-tax). This income includes all income sources, including earnings, profits, investment returns, state benefits, occupational pensions etc. As it is possible to make a loss on some of these activities, it is also possible (although unusual) for someone’s gross or net annual income in a given year to be negative (representing an overall loss).\n\nTask: Load the FRS dataset into your R environment, if it’s not already loaded, and inspect the data.\n\nOpen the dataset in RStudio’s Data Viewer to explore its structure, including the income_gross and income_net variables.\n\n # Open the data in the RStudio Viewer\nView(frs_data)\n\nin the Data Viewer tab, scroll horizontally to locate the income_gross and income_net columns. If columns are listed alphabetically, they will appear near other attributes that start with “income.”\nYou should notice two things:\n\nIncomes are recorded to the nearest £, NOT in income bands.\nDependent children almost all have a recorded income of £0.\n\nThis second observation highlights the somewhat loose wording of our question above (What sorts of people have high (or low) incomes?). To avoid reaching the somewhat banal conclusion that those with the lowest of all incomes are almost all children, we should re-frame the question more precisely as What sorts of people (excluding dependent children) have low incomes?\n\nTask: Determine the Scale of the Outcome Variable.\n\n\n# Summarize income variables\nsummary(frs_data$income_gross)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-354848      52   12740   17305   23712 1127360 \n\n\n\n# Summarize income variables\nsummary(frs_data$income_net)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-358592       0   12012   14204   20384 1110928 \n\n\n\nTask: Exclude Dependent Children.\n\nYou need to select all cases (persons) that are independent, that is where the variable dependent has values different from != “Dependent” or equal == “Independent”.\n\n# Filter to include only independent persons\nfrs_independent &lt;- frs_data %&gt;% filter(dependent != \"Dependent\")\n\n\nTask: Create a basic histogram (a visualisation lecture is scheduled later on).\n\nThe income variables in the FRS are all scale variables so a good starting point is to examine its distribution looking at a histogram of income_gross.\n\nlibrary(ggplot2)\n \n ggplot(frs_independent, aes(x = income_gross)) +\n   geom_histogram(binwidth = 5000, fill = \"blue\", color = \"black\") +\n   labs(\n     title = \"Distribution of Gross Household Income\",\n     x = \"Gross Income (£)\",\n     y = \"Frequency\"\n   ) +\n   xlim(0, 90000) +\n   theme_minimal()\n\n\n\n\n\n\n\n\nYou should see the histogram below. It reveals that the income distribution is very skewed with few people earning high salaries and the majority earning just over or less 35,000 annually.\n\nTask: Adopt a regrouping strategy.\n\nYou can also cross-tabulate gross (or net) income with any of the other variables in the FRS to your heart’s content – or can you?\nAgain, here is important to recall that the income variables in the FRS are all ‘scale’ variables; in other words, they are precise measures rather than broad categories. Consequently, every single person in the FRS potentially has their own unique income value. That could make for a table c. 44,000 rows long (one row per person) if each person has their own unique value. The solution is to create a categorical version of the original income variable by assigning each person to one of a set of income categories (income bands). Having done this, cross-tabulation then becomes possible.\nBut which strategy to use? Equal intervals, percentiles or ‘ad hoc’. Here I would suggest that ‘ad hoc’ is best: all you want to do is to allocate each independent adult to one of three arbitrarily defined groups: ‘low’, ‘middle’ and ‘high’ income. Define Low and High Income Thresholds\nDefine thresholds for income categories:\n\nLow-income threshold: £________\nHigh-income threshold: £_______\n\n\nTask: Create a New Variable Based on Regrouping of Original Variable.\n\nRecode income_gross into categories based on the chosen thresholds.\n\n# Define thresholds for income categories \nLOW_THRESHOLD &lt;- 10000 # Replace with the upper limit for low income \nHIGH_THRESHOLD &lt;- 50000 # Replace with the lower limit for high income \n\n# Define income categories based on thresholds \nfrs_independent &lt;- frs_independent %&gt;% \n    mutate(income_category = case_when( \n        income_gross &lt;= LOW_THRESHOLD ~ \"Low\", \n        income_gross &gt;= HIGH_THRESHOLD ~ \"High\", \n        TRUE ~ \"Middle\" ))\n\nThe mutate() function in R, from the dplyr package, is used to add or modify columns in a data frame. It allows you to create new variables or transform existing ones by applying calculations or conditional statements directly within the function.\nExplanation of the code\n\nfrs_independent %&gt;%: The pipe operator %&gt;% sends frs_independent into mutate(), allowing us to apply transformations without reassigning it repeatedly.\nmutate(): Starts the transformation process by defining new or modified columns.\nincome_category = case_when(...):\n\nThis creates a new column named income_category.\nThe case_when() function defines conditions for assigning values to this new column.\n\ncase_when():\n\ncase_when() is used here to assign categorical labels based on conditions.\nincome_gross &lt;= LOW_THRESHOLD ~ \"Low\": If income_gross is less than or equal to LOW_THRESHOLD, income_category will be labeled “Low.”\nincome_gross &gt;= HIGH_THRESHOLD ~ \"High\": If income_gross is greater than or equal to HIGH_THRESHOLD, income_category will be labeled “High.”\nTRUE ~ \"Middle\": Any values not meeting the previous conditions are labeled “Middle.”\n\n\n\nTask: Add some Metadata.\n\nDefine metadata for the new variable by labeling income categories.\n\n# Add metadata by converting to a factor and defining labels\n\nfrs_independent$income_category &lt;- factor(frs_independent$income_category,\n                        levels = c(\"Low\", \"Middle\", \"High\"), labels = c(\"&lt;= £10,000\", \"£10,001 - £49,999\", \"&gt;= £50,000\"))\n\n\nTask: Check your work.\n\nExamine the frequency distribution of the variable you have just created. Both variables should have the same number of missing cases, unless:\n\nMissing cases in the old variable have been intentionally converted into valid cases in the new variable.\nYou forgot to allocate a new value to one of the old variable categories, in which case the new variable will have more missing cases than the old variable.\n\n\n# Frequency distribution of income categories\ntable(frs_independent$income_category)\n\n\n       &lt;= £10,000 £10,001 - £49,999        &gt;= £50,000 \n             8584             22981              2271 \n\n\nAfter preparing the data, use cross-tabulations to compare income levels across demographic groups.\n\n# Cross-tabulate income category by age group, nationality, etc.\ntable(frs_independent$income_category, frs_independent$age_group)\n\n                   \n                    16-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64\n  &lt;= £10,000          373   680   492   558   474   511   554   652   781   826\n  £10,001 - £49,999   263  1241  1802  2056  2052  1948  1995  1967  1749  1772\n  &gt;= £50,000            1     8    59   186   314   331   334   356   237   177\n                   \n                    65-69 70-74  75+\n  &lt;= £10,000          773   744 1166\n  £10,001 - £49,999  2073  1554 2509\n  &gt;= £50,000          144    56   68\n\n\nExplore income distribution across different regions.\n\n# Cross-tabulate income category by region\ntable(frs_independent$income_category, frs_independent$region) \n\n                   \n                    East Midlands East of England London North East North West\n  &lt;= £10,000                  562             665    740        357        878\n  £10,001 - £49,999          1550            1855   1850        979       2347\n  &gt;= £50,000                  135             245    367         48        174\n                   \n                    Northern Ireland Scotland South East South West Wales\n  &lt;= £10,000                     874     1212        895        588   399\n  £10,001 - £49,999             2305     3234       2563       1707   971\n  &gt;= £50,000                     123      322        367        149    63\n                   \n                    West Midlands Yorks and the Humber\n  &lt;= £10,000                  744                  670\n  £10,001 - £49,999          1892                 1728\n  &gt;= £50,000                  164                  114\n\n\nTips for Cross-Tabulation\n\nPlace the income variable in the columns.\nAdd multiple variables in the rows to create simultaneous cross-tabulations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Lab: Exploring a Dataset</span>"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#setup",
    "href": "general/reportTemplate.html#setup",
    "title": "Assessment Template",
    "section": "",
    "text": "Once all the chunks execute correctly (no errors when running all the code, Ctrl +Alt + R ) you can render the qmd file. Follow the instructions here to render the document to a PDF. Remove this message and any irrelevant text from the final submission.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "general/reportTemplate.html#introduction",
    "href": "general/reportTemplate.html#introduction",
    "title": "Assessment Template",
    "section": "Introduction",
    "text": "Introduction\nAim. State clearly what you want to investigate (e.g., association between X and Y).\nRelevance. Explain why the topic matters (theory, policy, practice).\nGap & Research Question (RQ). Identify what is missing in the current knowledge and pose 1 RQ (avoid yes/no questions; ask how, to what extent, which factors).\nStructure. Briefly preview how the rest of the article is organized.",
    "crumbs": [
      "ENVS225 Assignment 1"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html",
    "href": "labs/02.MultipleLinear.html",
    "title": "3  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "3.1 Part I. Correlation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-i.-correlation",
    "href": "labs/02.MultipleLinear.html#part-i.-correlation",
    "title": "3  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "",
    "text": "3.1.1 Data Overview: Descriptive Statistics:\nLet’s start by picking one dataset derived from the English-Wales 2021 Census data. You can choose one dataset that aggregates data either at a) county, b) district, or c) ward-level. Lower Tier Local Authority-, Region-, and Country-level data is also available in the data folder.\nsee also: https://canvas.liverpool.ac.uk/courses/77895/pages/census-data-2021\n\n# Load necessary libraries \nlibrary(ggplot2) \nlibrary(dplyr) \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\noptions(scipen = 999, digits = 4)  # Avoid scientific notation and round to 4 decimals globally\n\n# load data\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\") # District level\n\nWe’re using a (district/ward/etc.)-level census dataset that includes:\n\n% of population with poor health (variable name: pct_Very_bad_health).\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nFirst, let’s get some descriptive statistics that help identify general trends and distributions in the data.\n\n# Summary statistics\nsummary_data &lt;- census %&gt;%\n  select(pct_Very_bad_health, pct_No_qualifications, pct_Males, pct_Higher_manager_prof) %&gt;%\n  summarise_all(list(mean = mean, sd = sd))\nsummary_data\n\n  pct_Very_bad_health_mean pct_No_qualifications_mean pct_Males_mean\n1                    1.173                       17.9          48.97\n  pct_Higher_manager_prof_mean pct_Very_bad_health_sd pct_No_qualifications_sd\n1                        13.22                 0.3401                    3.959\n  pct_Males_sd pct_Higher_manager_prof_sd\n1       0.6605                       4.73\n\n\nQ1. Complete the table below by specifying each variable type (continuous or categorical) and reporting its mean and standard deviation.\n\n\n\n\n\n\n\n\n\nVariable Name\nType (Continuous or Categorical)\nMean\nStandard Deviation\n\n\n\n\npct_Very_bad_health\n\n\n\n\n\npct_No_qualifications\n\n\n\n\n\npct_Males\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\n\n3.1.2 Simple visualisation for continuous data\nYou can visualise the relationship between two continuous variables using a scatter plot. Using the chosen census datasets, visualise the association between the % of population with bad health (pct_Very_bad_health) and each of the following:\n\nthe % of population with no qualifications (pct_No_qualifications);\nthe % of population aged 65 to 84 (pct_Age_65_to_84);\nthe % of population in a married couple (pct_Married_couple);\nthe % of population in a Higher Managerial or Professional occupation (pct_Higher_manager_prof).\n\n\n# Scatterplot for each variable variables \nvariables &lt;- c(\"pct_No_qualifications\", \"pct_Age_65_to_84\", \"pct_Married_couple\", \"pct_Higher_manager_prof\")\n\n# Loop to create scatterplots and calculate correlations \n# x and y variables for each scatter plot,\nfor (var in variables) { \n  # Scatterplot \n  ggplot(census, aes_string(x = var, y = \"pct_Very_bad_health\")) +\n    geom_point() + \n    labs(title = paste(\"Scatterplot of pct_Very_bad_health vs\", var), \n         x = var, y = \"pct_Very_bad_health\") +\n    theme_minimal()\n}\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\nQ2. Which of the associations do you think is strongest, which one is the weakest?\nAs noted, before, an observed association between two variables is no guarantee of causation. It could be that the observed association is:\n\nsimply a chance one due to sampling uncertainty;\ncaused by some third underlying variable which explains the spatial variation of both of the variables in the scatterplot;\ndue to the inherent arbitrariness of the boundaries used to define the areas being analysed (the ‘Modifiable Area Unit Problem’).\n\nQ3. Setting these caveats to one side, are the associations observed in the scatter-plots suggestive of any causative mechanisms of bad health?\nRather than relying upon an impressionistic view of the strength of the association between two variables, we can measure that association by calculating the relevant correlation coefficient. The Table below identifies the statistically appropriate measure of correlation to use between two continuous variables.\n\n\n\n\n\n\n\n\nVariable Data Type\nMeasure of Correlation\nRange\n\n\n\n\nBoth symmetrically distributed\nPearson’s\n-1 to +1\n\n\nOne or both with a skewed distribution\nSpearman’s Rank\n-1 to +1\n\n\n\nDifferent Calculation Methods: Pearson’s correlation assumes linear relationships and is suitable for symmetrically distributed (normally distributed) variables, measuring the strength of the linear relationship. Spearman’s rank correlation, however, works on ranked data, so it’s more suitable for skewed data or variables with non-linear relationships, measuring the strength and direction of a monotonic relationship.\nWhen calculating correlation for a single pair of variables, select the method that best fits their data distribution:\n-   Use **Pearson’s** if both variables are symmetrically distributed.\n-   Use **Spearman’s** if one or both variables are skewed.\n\n\n\n\n\n\n\n\n\nYou can check the distribution of a variable (e.g. pct_No_qualifications like this):\n\n# Plot histogram with density overlay for a chosen variable (e.g., 'pct_No_qualifications')\nggplot(census, aes(x = pct_No_qualifications)) + \n    geom_histogram(aes(y = after_stat(density)), bins = 30, color = \"black\", fill = \"skyblue\", alpha = 0.7) +\n    geom_density(color = \"darkblue\", linewidth = 1) +\n    labs(title = \"Distribution of pct_No_qualifications\", x = \"Value\", y =  \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhen analyzing multiple pairs of variables, using different measures (Pearson for some pairs, Spearman for others) creates inconsistencies since Pearson and Spearman values aren’t directly comparable in size due to their different calculation methods. To maintain consistency across comparisons, calculate both Pearson’s and Spearman’s correlations for each pair, e.g. do the trends align (both showing strong, weak, or moderate correlation in the same direction)? This consistency check can give confidence that the relationships observed are not dependent on the correlation method chosen. While in a report you’d typically include only one set of correlations (usually Pearson’s if the relationships appear linear), calculating both can validate that your observations aren’t an artifact of the correlation method.\n\nResearch Question 1: Which of our selected variables are most strongly correlated with % of population with bad health?\n\nTo answer this question, complete the Table below by editing/running this code:.\nPearson correlations\n\npearson_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications,use = \"complete.obs\", method = \"pearson\")\n    \n# Display the results\ncat(\"Pearson Correlation:\", pearson_correlation, \"\\n\")\n\nPearson Correlation: 0.7619 \n\n\nSpearman correlations:\n\nspearman_correlation &lt;- cor(census$pct_Very_bad_health,\n    census$pct_No_qualifications, use = \"complete.obs\", method = \"spearman\")\n\ncat(\"Spearman Correlation:\", spearman_correlation, \"\\n\")\n\nSpearman Correlation: 0.7781 \n\n\n\n\n\nCovariates\nPearson\nSpearman\n\n\n\n\npct_Very_bad_health - pct_No_qualifications\n\n\n\n\npct_Very_bad_health - pct_Age_65_to_84\n\n\n\n\npct_Very_bad_health - pct_Married_couple\n\n\n\n\npct_Very_bad_health - pct_Higher_manager_prof\n\n\n\n\n\nWhat can you make of this numbers?\nIf you think you have found a correlation between two variables in our dataset, this doesn’t mean that an association exists between these two variables in the population at large. The uncertainty arises because, by chance, the random sample included in our dataset might not be fully representative of the wider population.\nFor this reason, we need to verify whether the correlation is statistically significant,\n\n# significance test for pearson, for example\npearson_test &lt;- cor.test(census$pct_Very_bad_health,\n    census$pct_No_qualifications, method = \"pearson\", use = \"complete.obs\")\npearson_test\n\n\n    Pearson's product-moment correlation\n\ndata:  census$pct_Very_bad_health and census$pct_No_qualifications\nt = 21, df = 329, p-value &lt;0.0000000000000002\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7127 0.8037\nsample estimates:\n   cor \n0.7619 \n\n\nLook at https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor.test for details about the function. But in general, when calculating the correlation between two variables, a p-value accompanies the correlation coefficient to indicate the statistical significance of the observed association. This p-value tests the null hypothesis that there is no association between the two variables (i.e., that the correlation is zero).\nWhen interpreting p-values, certain thresholds denote different levels of confidence. A p-value less than 0.05 is generally considered statistically significant at the 95% confidence level, suggesting that we can be 95% confident there is an association between the variables in the broader population. When the p-value is below 0.01, the result is significant at the 99% confidence level, meaning we have even greater confidence (99%) that an association exists. Sometimes, on research papers or tables significance levels are denoted with asterisks: one asterisk (*) typically indicates significance at the 95% level (p &lt; 0.05), two asterisks (**) significance at the 99% level (p &lt; 0.01), three asterisks (***) significance at the 99.99% level (p &lt; 0.01).\nTypically, p-values are reported under labels such as “Sig (2-tailed),” where “2-tailed” refers to the fact that the test considers both directions (positive and negative correlations). Reporting the exact p-value (e.g., p = 0.002) is more informative than using thresholds alone, as it gives a clearer picture of how strongly the data contradicts the null hypothesis of no association.\nIn a nutshell, lower p-values suggest a stronger statistical basis for believing that an observed correlation is not due to random chance. A statistically significant p-value reinforces confidence that an association is likely to exist in the wider population, though it does not imply causation.\n\n\n3.1.3 Part. 2: Implementing a Linear Regression Model\nA key goal of data analysis is to explore the potential factors of health at the local district level. So far, we have used cross-tabulations and various bivariate correlation analysis methods to explore the relationships between variables. One key limitation of standard correlation analysis is that it remains hard to look at the associations of an outcome/dependent variable to multiple independent/explanatory variables at the same time. Regression analysis provides a very useful and flexible methodological framework for such a purpose. Therefore, we will investigate how various local factors impact residents’ health by building a multiple linear regression model in R.\nWe use pct_Very_bad_health as a proxy for residents’ health.\n\nResearch Question 2: How do local factors affect residents’ health?\n\nDependent (or Response) Variable:\n\n% of population with bad health (pct_Very_bad_health).\n\nIndependent (or Explanatory) Variables:\n\n% of population with no qualifications (pct_No_qualifications).\n% of male population (pct_Males).\n% of population in a higher managerial/professional occupation (pct_Higher_manager_prof).\n\nLoad some other Libraries\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\n\nand the data (if not loaded):\n\n# Load dataset\ncensus &lt;- read.csv(\"../data/Census2021/EW_DistrictPercentages.csv\")\n\nRegression models are the standard method for constructing predictive and explanatory models. They tell us how changes in one variable (the target variable or independent variable, \\(Y\\)) are associated with changes in explanatory variables, or dependent variables, \\(X_1, X_2, X_3\\) (\\(X_n\\)), etc. Classic linear regression is referred to Ordinary least squares (OLS) regression because they estimate the relationship between one or more independent variables and a dependent variable \\(Y\\) using a hyperplane (i.e. a multi-dimensional line) that minimises the sum of the squared difference between the observed values of \\(Y\\) and the values predicted by the model (denoted as \\(\\hat{Y}\\), \\(Y\\)-hat).\nHaving seen Single Linear Regression in class - where the relationship between one independent variable and a dependent variable is modeled - we can extend this concept to situations where more than one explanatory variable might influence the outcome. While single linear regression helps us understand the effect of ONE variable in isolation, real-world phenomena are often influenced by multiple factors simultaneously. Multiple linear regression addresses this complexity by allowing us to model the relationship between a dependent variable and multiple independent variables, providing a more comprehensive view of how various explanatory variables contribute to changes in the outcome.\nHere, regression allows us to examine the relationship between people’s health rates and multiple dependent variables.\nBefore starting, we define two hypotheses:\n\nNull hypothesis (\\(H_0\\)): For each variable \\(X_n\\), there is no effect of \\(X_n\\) on \\(Y\\).\nAlternative hypothesis (\\(H_1\\)): There is an effect of\\(X_n\\) on \\(Y\\).\n\nWe will test if we can reject the null hypothesis.\n\n\n3.1.4 Model fit\n\n# Linear regression model\nmodel &lt;- lm(pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof, data = census)\nsummary(model)\n\n\nCall:\nlm(formula = pct_Very_bad_health ~ pct_No_qualifications + pct_Males + \n    pct_Higher_manager_prof, data = census)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.4911 -0.1357 -0.0368  0.0985  0.7669 \n\nCoefficients:\n                        Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)              4.00293    0.87981    4.55            0.0000076 ***\npct_No_qualifications    0.05283    0.00591    8.94 &lt; 0.0000000000000002 ***\npct_Males               -0.07353    0.01785   -4.12            0.0000479 ***\npct_Higher_manager_prof -0.01318    0.00494   -2.67                0.008 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.213 on 327 degrees of freedom\nMultiple R-squared:  0.61,  Adjusted R-squared:  0.607 \nF-statistic:  171 on 3 and 327 DF,  p-value: &lt;0.0000000000000002\n\n\nCode explanation\nlm() Function:\n\nlm() stands for “linear model” and is used to fit a linear regression model in R.\nThe formula syntax pct_Very_bad_health ~ pct_No_qualifications + pct_Males + pct_Higher_manager_prof specifies a relationship between:\n\nDependent Variable: pct_Very_bad_health.\nIndependent Variables: pct_No_qualifications, pct_Males, and pct_Higher_manager_prof. The model is trained on the data dataset.\n\n\nStoring the Model: The model &lt;- syntax stores the fitted model in an object called model.\nsummary(model) provides a detailed output of the model’s results, including:\n\nCoefficients: Estimates of the regression slopes (i.e., how each independent variableaffects pct_Very_bad_health).\nStandard Errors: The variability of each coefficient estimate.\nt-values and p-values: Indicate the statistical significance of the effect of each independent (explanatory) variable.\n\nR-squared and Adjusted R-squared: Show how well the independent variables explain the variance in the dependent variable.\nF-statistic: Tests the overall significance of the model.\n\nWe can focus only on certain output metrics:\n\n# Regression coefficients\ncoefficients &lt;- tidy(model)\ncoefficients\n\n# A tibble: 4 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.00     0.880        4.55 7.58e- 6\n2 pct_No_qualifications     0.0528   0.00591      8.94 2.99e-17\n3 pct_Males                -0.0735   0.0178      -4.12 4.79e- 5\n4 pct_Higher_manager_prof  -0.0132   0.00494     -2.67 7.97e- 3\n\n\nThese are:\n\nRegression Coefficient Estimates.\nP-values.\nAdjusted R-squared.\n\n\n\n3.1.5 How to interpret the output metrics\n\n3.1.5.1 Regression Coefficient Estimates\nThe Estimate column in the output table tells us the rate of change between each dependent variable \\(X_n\\) and \\(Y\\).\nIntercept: In the regression equation, this is \\(β_0\\) and it indicates the value of \\(Y\\) when \\(X_n\\) are equal to zero.\nSlopes: These are the other regression coefficients of an independent variable, e.g. \\(β_1\\), i.e. estimated average changes in \\(Y\\) for a one unit change in an independent variable, e.g. \\(X_1\\), when all other dependent or explanatory variables are held constant.\nThere are two key points worth mentioning:\n\nThe unit of \\(X\\) and \\(Y\\): you need to know what the units are of the independent and dependent variables. For instance, one unit could be one year if you have an age variable, or a one percentage point if the variable is measured in percentages (all the variables in this week’s practical).\nAll the other explanatory variables are held constant. It means that the coefficient of an explanatory variable \\(X_1\\) (e.g. \\(β_1\\)) should be interpreted as: a one unit change in \\(X_1\\) is associated with \\(β_1\\) units change in \\(Y\\), keeping other values of explanatory variables (e.g. \\(X_2\\), \\(X_3\\)) constant – for instance, \\(X_2\\)= 0.1 or \\(X_3\\)= 0.4.\n\nFor the independent variable \\(X\\), we can derive how changes of 1 unit for the independent are associated with the changes in pct_Very_bad_health, for example:\n\nThe association of pct_No_qualifications is positive and strong: each increase in 1% of pct_No_qualifications is associated with an increase of 0.05% of very bad health rate.\nThe association of pct_Males is negative and strong: each decrease in 1% of pct_Males is associated with an increase of 0.07% of pct_Very_bad_health in the population in England and Wales.\nThe association of pct_Higher_manager_prof is negative but weak: each decrease in 1% of pct_Higher_manager_prof is associated with an increase of 0.013% of pct_Very_bad_health.\n\n\n\n3.1.5.2 P-values and Significance\nThe t tests of regression coefficients are used to judge the statistical inferences on regression coefficients, i.e. associations between independent variables and the outcome variable. For a t-statistic of a dependent variable, there is a corresponding p-value that indicates different levels of significance in the column Pr(&gt;|t|) and the asterisks ∗.\n\n*** indicates “changes in \\(X_n\\) are significantly associated with changes in \\(Y\\) at the &lt;0.001 level”.\n** suggests that “changes in \\(X_n\\) are significantly associated with changes in \\(Y\\) between the 0.001 and (&lt;) 0.01 levels”.\nNow you should know what * means: The significance is between the 0.01 and 0.05 levels, which means that we observe a less significant (but still significant) relationship between the variables.\n\nP-value provide a measure of how significant the relationship is; it is an indication of whether the relationship between \\(X_n\\) and \\(Y\\) found in this data could have been found by chance. Very small p-values suggest that the level of association found here might not have come from a random sample of data.\nIn this case, we can say:\n\nGiven that the p-value is indicated by ***, changes in pct_No_qualifications and pct_Males are significantly associated with changes in pct_Very_bad_health at the &lt;0.001 level; the association is highly statistically significant; we can be confident that the observed relationship between these variables and pct_Very_bad_health is not due to chance.\nGiven that the p-value is indicated by **, changes in pct_Higher_manager_prof are significantly associated with changes in pct_Very_bad_health at the 0.001 level. This means that the association between the independent and dependent variable is not one that would be found by chance in a series of random sample 99.999% of the time.\n\nIn both cases we can then confidently reject the Null hypothesis (\\(H_0\\): no association between dependent and independent variables exist).\nRemember, If the p-value of a coefficient is smaller than 0.05, that coefficient is statistically significant. In this case, you can say that the relationship between this independent variable and the outcome variable is statistically significant. Contrarily, if the p-value of a coefficient is larger than 0.05 you can conclude that there is no evidence of an association or relationship between the independent variable and the outcome variable.\n\n\n3.1.5.3 R-squared and Adjusted R-squared\nThese provide a measure of model fit. They are calculated as the difference between the actual value of \\(Y\\) and the value predicted by the model. The R-squared and Adjusted R-squared values are statistical measures that indicate how well the independent variables in your model explain the variability of the dependent variable. Both R-squared and Adjusted R-squared help us understand how closely the model’s predictions align with the actual data. An R-squared of 0.6, for example, indicates that 60% of the variability in \\(Y\\) is explained by the independent variables in the model. The remaining 40% is due to other factors not captured by the model.\nAdjusted R-squared also measures the goodness of fit, but it adjusts for the number of independent variables in the model, accounting for the fact that adding more variables can artificially inflate R-squared without genuinely improving the model. This is especially useful when comparing models with different numbers of independent variables. If Adjusted R-squared is close to or above 0.6, as in your example, it implies that the model has a strong explanatory power while not being overfit with unnecessary explanatory variables.\nA high R-squared and Adjusted R-squared indicate that the model captures much of the variation in the data, making it more reliable for predictions or for understanding the relationship between \\(Y\\) and the explanatory variables. However Low R-squared values suggest (e.g. 0.15) that the model might be missing important explanatory variables or that the relationship between \\(Y\\) and the selected explanatory variables is not well-captured by a linear approach.\nAn R-squared and Adjusted R-squared over 0.6 are generally seen as signs of a well-fitting model in many fields, though the ideal values can depend on the context and the complexity of the data.\n\n\n\n3.1.6 Interpreting the Results\n\ncoefficients\n\n# A tibble: 4 × 5\n  term                    estimate std.error statistic  p.value\n  &lt;chr&gt;                      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)               4.00     0.880        4.55 7.58e- 6\n2 pct_No_qualifications     0.0528   0.00591      8.94 2.99e-17\n3 pct_Males                -0.0735   0.0178      -4.12 4.79e- 5\n4 pct_Higher_manager_prof  -0.0132   0.00494     -2.67 7.97e- 3\n\n\nQ4. Complete the table above by filling in the coefficients, t-values, p-values, and indicating if each variable is statistically significant.\n\n\n\n\n\n\n\n\n\n\nVariable Name\nCoefficients\nt-values\np-values\nSignificant?\n\n\n\n\npct_No_qualifications\n\n\n\n\n\n\npct_Males\n\n\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\n\n\n\nFrom the lecture notes, you know that the Intercept or Constant represents the estimated average value of the outcome variable when the values of all independent variables are equal to zero.\nQ5. When values of pct_Males, pct_No_qualifications and pct_Higher_manager_prof are all \\(zero\\), what is the % of population with very bad health? Is the intercept term meaningful? Are there any districts (or zones, depending on the dataset you chose) with zero percentages of persons with no qualification in your data set?\nQ6. Interpret the regression coefficients of pct_Males, pct_No_qualifications and pct_Higher_manager_prof. Do they make sense?\n\n\n3.1.7 Identify factors of % bad health\nNow combine the above two sections and identify factors affecting the percentage of population with very bad health. Fill in each row for the direction (positive or negative) and significance level of each variable.\n\n\n\n\n\n\n\n\nVariable Name\nPositive or Negative\nStatistical Significance\n\n\n\n\npct_No_qualifications\n\n\n\n\npct_Higher_manager_prof\n\n\n\n\npct_Males\n\n\n\n\n\nQ7. Think about the potential conclusions that can be drawn from the above analyses. Try to answer the research question of this practical: How do local factors affect residents’ health? Think about causation vs association and consider potential confounders when interpreting the results. How could these findings influence local health policies?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "href": "labs/02.MultipleLinear.html#part-c-practice-and-extension",
    "title": "3  Lab: Correlation, Single, and Multiple Linear Regression",
    "section": "3.2 Part C: Practice and Extension",
    "text": "3.2 Part C: Practice and Extension\nIf you haven’t understood something, if you have doubts, even if they seem silly, ask.\n\nFinish working through the practical.\nRevise the material.\nExtension activities (optional): Think about other potential factors of very bad health and test your ideas with new linear regression models.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Lab: Correlation, Single, and Multiple Linear Regression</span>"
    ]
  }
]