% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{luacolor}
  \usepackage[soul]{lua-ul}
\else
  \usepackage{soul}
  
\fi
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Exploring the Social World - Quantitative Block: Statistics},
  pdfauthor={Gabriele Filomena and Zi Ye},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Exploring the Social World - Quantitative Block: Statistics}
\author{Gabriele Filomena and Zi Ye}
\date{2024-11-21}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

This is the website for ``Exploring the Social World - Quantitative
Block: Statistics'' (module \textbf{ENVS225}) at the University of
Liverpool. This block of the module is designed and delivered by
Dr.~Gabriele Filomena and Dr.~Zi Ye from the Geographic Data Science Lab
at the University of Liverpool. The module seeks to provide hands-on
experience and training in introductory statistics for human
geographers.

The website is \textbf{free to use} and is licensed under the
\href{https://creativecommons.org/licenses/by-nc-nd/4.0/}{Attribution-NonCommercial-NoDerivatives
4.0 International}. A compilation of this web course is hosted as a
GitHub repository that you can access:

\begin{itemize}
\tightlist
\item
  As an \href{https://gdsl-ul.github.io/stats}{html website}.
\item
  As a \href{https://github.com/GDSL-UL/stats}{GitHub repository}.
\end{itemize}

\section*{Contact}\label{contact}
\addcontentsline{toc}{section}{Contact}

\markright{Contact}

\begin{quote}
Gabriele Filomena - gfilo {[}at{]} liverpool.ac.uk Lecturer in
Geographic Data Science Office 1xx, Roxby Building, University of
Liverpool - 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.
\end{quote}

\begin{quote}
Zi Ye - zi.ye {[}at{]} liverpool.ac.uk Lecturer in Geographic
Information Science Office 107, Roxby Building, University of Liverpool
- 74 Bedford St S, Liverpool, L69 7ZT, United Kingdom.
\end{quote}

\bookmarksetup{startatroot}

\chapter*{Overview}\label{overview}
\addcontentsline{toc}{chapter}{Overview}

\markboth{Overview}{Overview}

\section*{Aim and Learning
Objectives}\label{aim-and-learning-objectives}
\addcontentsline{toc}{section}{Aim and Learning Objectives}

\markright{Aim and Learning Objectives}

This sub-module aims to provide training and skills on a set of basic
quantitative research methods for data collection, analysis, and
interpretation. You will learn how to define coherent, relevant research
questions, utilise various research quantitative methods, and identify
appropriate methodologies to tackle your research questions.
\textbf{This block serves as the foundation for the dissertation and
fieldwork modules.}

\textbf{Background}

Data and research are key pillars of the global economy and society
today. We need rigorous approaches to collecting and analysing both the
statistics that can tell us `how much' and if there are observable
relationships between phenomena; and the information gives us a nuanced
understanding of cultural contexts and human dynamics. Quantitative
skills enable us to explore and measure socio-economic activities and
processes at large scales, while qualitative skills enable understanding
of social, cultural, and political contexts and diverse lived
experiences. Rather than being in opposition, qualitative and
quantitative research can complement one another in the investigation of
today's pressing research questions.

To these ends, this block will help you develop your quantitative
(statistical) skills, as critical tools. This course will help you
understand what quantitative statistical researchers use and develop a
set of research techniques that can be used in your field classes and
dissertations.

\textbf{Learning objectives:}

\begin{itemize}
\tightlist
\item
  Understand how to explore a dataset, containing a number of
  observations described by a set of variables.
\item
  Demonstrate an understanding in the application and interpretation of
  commonly used quantitative research methods.
\item
  Demonstrate an understanding of how to work with quantitative data to
  address real-world research questions.
\end{itemize}

\section*{Module Structure}\label{module-structure}
\addcontentsline{toc}{section}{Module Structure}

\markright{Module Structure}

\textbf{Staff:} Dr Zi Ye and Dr Gabriele Filomena

\textbf{Where and When}

Quantitative Block (Weeks 7-12):

\begin{itemize}
\tightlist
\item
  \textbf{Lecture}: 10 am -- 10.45 am Fridays
\item
  \textbf{PC Practical sessions}: 11am -- 1 pm, following the Lecture
\end{itemize}

\textbf{Week 7}: Central Teaching Hub: PC Teaching Centre
BLUE+GREEN+ORANGE ZONES

\textbf{Week 8 -12}: Central Teaching Hub, PCTC

Lectures will introduce and explain the fundamentals of quantitative
methods, with the opportunity to apply the method introduced in the labs
later in the week.

The computer practical sessions, will give you the chance to use and
apply quantitative methods to real-world data. These are primarily
self-directed sessions, but with support on hand if you get stuck.
Support and training in R will be provided through these sessions.
Weekly sessions will be driven by empirical research questions.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0659}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.4945}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3626}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.0769}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Week
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Topic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Format
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Staff
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
7 & Introduction \& Review & Lecture and Computer Lab Practical & GF \\
8 & Single \& Multiple Linear Regression & Lecture and Computer Lab
Practical & GF \\
9 & Multiple Linear Regression with Categorical Variables & Lecture and
Computer Lab Practical & ZY \\
10 & Logistic Regression & Lecture and Computer Lab Practical & ZY \\
11 & Data Visualisation & Lecture and Computer Lab Practical & GF \\
12 & Summary and Assessment Support & Lecture and Computer Lab Practical
& ZY \\
\end{longtable}

\section*{Software and Data}\label{software-and-data}
\addcontentsline{toc}{section}{Software and Data}

\markright{Software and Data}

For quantitative training sessions, ensure you have installed and/or
have access to \textbf{RStudio}. To run the analysis and reproduce the
code in R, you need the following software installed on your machine:

\begin{itemize}
\tightlist
\item
  R-4.2.2
\item
  RStudio 2022.12.0-353
\end{itemize}

To install and update:

\begin{itemize}
\tightlist
\item
  R, download the appropriate version from
  \href{https://cran.r-project.org/}{The Comprehensive R Archive Network
  (CRAN)}.
\item
  RStudio, download the appropriate version from
  \href{https://posit.co/download/rstudio-desktop/}{here}.
\end{itemize}

\textbf{This software is already installed on University Machines. But
you will need it to run the analysis on your personal devices.}

\textbf{Data}

Example datasets could be accessed through Canvas or the
\href{https://github.com/GDSL-UL/stats}{GitHub} Repository of the
module. These include:

\begin{itemize}
\tightlist
\item
  2021 UK Census Data.
\item
  2021 Annulation Population Survey.
\item
  2016 Family Resource Survey.
\end{itemize}

\emph{Note: The Annual Population Survey requires the completion of a
form prior to its usage, as it is licensed.}

\bookmarksetup{startatroot}

\chapter*{Assessment}\label{assessment}
\addcontentsline{toc}{chapter}{Assessment}

\markboth{Assessment}{Assessment}

\textbf{Deadline}: Tuesday 7th January 2025.

The assignment \textbf{Data Exploration and Analysis} consists of
writing a research report using one of the regression techniques learned
during the module. The basic idea is to put in practice the methods
learned during the quantitative block of the module. You are required to
apply a linear or logistic regression model to the data provided for the
module. The report needs to include the following sections (in brackets,
\% of the whole lenght):

\begin{itemize}
\tightlist
\item
  Introduction (5\%).
\item
  Literature Review (20\%).
\item
  Methods and data (30\%).
\item
  Results and discussion (40\%).
\item
  Conclusion (5\%).
\item
  Reference List.
\end{itemize}

\section*{Required Report Structure}\label{required-report-structure}
\addcontentsline{toc}{section}{Required Report Structure}

\markright{Required Report Structure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Introduction}

  \begin{itemize}
  \tightlist
  \item
    Context: Why is the topic relevant or worth being investigated?
  \item
    Brief discussion of existing literature.
  \item
    Knowledge gap and Aim.
  \item
    Research questions.
  \end{itemize}
\item
  \textbf{Literature review}

  \begin{itemize}
  \tightlist
  \item
    More detailed Literature review, i.e.~what do we already know about
    this subject
  \item
    Rationale for including certain predictor variables in the model.
  \item
    What knowledge gap remains that this article will address? (includes
    ``not studied before in this area''). \emph{Note: there is no
    expectation on totally original research. The focus is on a clean,
    sensible, data analysis situated in existing ideas.}
  \end{itemize}
\item
  \textbf{Methodology:}

  \begin{itemize}
  \tightlist
  \item
    A brief introduction to the dataset being analysed (who collected
    it? When? How many responses? etc.)
  \item
    A description of the variables chosen to be analysed.
  \item
    A description of any transformation made to the original data,
    i.e.~turning a continuous variable of income into intervals, or
    reducing the number of age groups from 11 to 3.
  \item
    A description and justification of the statistical techniques used
    in the subsequent analysis (i.e.~the Multivariate regression model:
    Multiple or Logistic Linear Regression).
  \end{itemize}
\item
  \textbf{Results and Discussion}

  \begin{itemize}
  \tightlist
  \item
    Descriptive statistics and summary of the variables employed.
  \item
    Correct interpretation of correlation coefficients.
  \item
    Usage and results of an appropriate multivariate regression model.
  \item
    Interpretation of the results, including links and contrasts to
    existing literature.
  \item
    Selective illustrations (graphs and tables) to make your findings as
    clear as possible.
  \end{itemize}
\item
  \textbf{Conclusion}

  \begin{itemize}
  \tightlist
  \item
    Summary of main findings.
  \item
    Limitations of study (self-critique).
  \end{itemize}
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Highlight any implications derived from the study.
\end{itemize}

Follow this structure and include \textbf{ALL} these points, do not make
your life harder.

\section*{How to get there?}\label{how-to-get-there}
\addcontentsline{toc}{section}{How to get there?}

\markright{How to get there?}

The first stage is to identify \textbf{ONE} a relevant research question
to be addressed. Based on the chosen question, you will need to identify
a dependent (or outcome) variable which you want to explain, and at
least two relevant independent variables that you can use to explain the
chosen dependent variable. The selection of variables should be informed
by the literature and empirical evidence.

\textbf{To detail in the Methods Section:} Once the variables have been
chosen, you will need to describe the data and \textbf{appropriate} type
of regression to be used for the analysis. You need to explain any
transformation done to the original data source, such as reclassifying
variables, or changing variables from continuous to nominal scales. You
also need to briefly describe the data use: source of data, year of data
collection, indicate the number of records used, state if you are using
individual records or geographical units, explain if you are selecting a
sample, and any relevant details. You also need to identify type of
regression to be used and why.

\textbf{To detail in the Results and Discussion Section:} Firstly, you
need to provide two types of analyses. First, you need to provide a
descriptive analysis of the data. Here you could use tables and/or plots
reporting relevant descriptive statistics, such as the mean, median and
standard deviation; variable distributions using histograms; and
relationships between variables using correlation matrices or scatter
plots. Secondly, you need to present an estimated regression model or
models and the interpretation of the estimated coefficients. You need a
careful and critical analysis of the regression estimates. You should
think that you intend to use your regression models to advice your boss
who is expecting to make some decisions based on the information you
will provide. As part of this process, you need to discuss the model
assessment results for the overall model and regression coefficients.
Remember to substantiate your arguments using relevant literature and
evidence, and present results clearly in tables and graphs.

\section*{How to submit}\label{how-to-submit}
\addcontentsline{toc}{section}{How to submit}

\markright{How to submit}

You should submit a \texttt{.pdf} file, that is a rendered version of a
Quarto Markdown file (\texttt{qmd} file). This will allow you to write a
research paper that also includes your working code, without the need of
including the data (rendered \texttt{.qmd} files are executed before
being converted to R).

How to get a PDF?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Install Quarto}: Make sure you have Quarto installed. You can
  download it from quarto.org.
\item
  \textbf{LaTeX Installation}: For PDF output, you'll need a LaTeX
  distribution like \textbf{TinyTeX} from R, by executing this in the R
  console:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tinytex"}\NormalTok{)}
\NormalTok{tinytex}\SpecialCharTok{::}\FunctionTok{install\_tinytex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Open the Quarto File}: Open your \texttt{.qmd} file in
  RStudio.
\item
  \textbf{Set Output Format}: In the YAML header at the top of your
  Quarto file, specify \texttt{pdf} under \texttt{format}:
\end{enumerate}

\includegraphics[width=3.01in,height=\textheight]{general/../img/quartoHeader.png}

\begin{verbatim}
    title: "Your Document Title"
    author: "Anonymous" # do not change
    format: pdf
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Click the \textbf{Render} button in the RStudio toolbar (next to the
  Knit button).
\end{enumerate}

\section*{How is it graded?}\label{how-is-it-graded}
\addcontentsline{toc}{section}{How is it graded?}

\markright{How is it graded?}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1806}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2778}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\textbf{Grade}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Score Range}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{UG}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Descriptor}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Assignment Expectations}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Fail} & 0-34\% & Fail & \textbf{Inadequate} & \textbf{Literature
Review}: Lacks relevance and fails to justify variable choice. Evidence
is irrelevant or missing, providing no support to the research question.
\textbf{Methods}: Data is not described, and the regression model is
entirely missing. No appropriate statistical method is applied.
\textbf{Results and Discussion}: No descriptive statistics, graphs, or
tables are provided. Model results and interpretation are absent.
\textbf{Structure and References}: Report is disorganized with
significant referencing and citation errors throughout. \\
\textbf{Narrow Fail} & 35-39\% & Fail & \textbf{Highly Deficient} &
\textbf{Literature Review}: Review is present but lacks coherence and
fails to justify variable choice. Evidence is poorly aligned with the
research question and mostly irrelevant. \textbf{Methods}: Minimal data
description; the regression model is missing but some statistical
methods are mentioned. \textbf{Results and Discussion}: Few or no
descriptive statistics or visuals are present. Statistical methods are
unclear or incorrectly applied. Results are vague and lack meaningful
interpretation. \textbf{Structure and References}: Report structure is
poor, with referencing errors in multiple sections. \\
\textbf{Third / Fail} & 40-49\% & Third (UG) & \textbf{Deficient} &
\textbf{Literature Review}: Relevant literature is partially addressed
but lacks depth, with limited justification for variable choice.
Evidence is minimally aligned with the research question.
\textbf{Methods}: A very basic data description is provided, but the
selected regression model is deeply inadequate or incorrect (e.g.,
multiple linear regression for a categorical outcome; logistic
regression for a continuous outcome). \textbf{Results and Discussion}:
Descriptive statistics or visuals may be present but insufficient. Model
results are presented with little to no interpretation.
\textbf{Structure and References}: Report structure is present but lacks
clarity, with inconsistencies in citations and citation style. \\
\textbf{2.2 / Pass} & 50-59\% & 2.2 (UG) & \textbf{Adequate} &
\textbf{Literature Review}: Addresses relevant literature but with
limited justification of variable choices. Evidence generally supports
the research question but lacks detail. \textbf{Methods}: Data
description is present but brief; a regression model is included but
applied illogically or incorrectly (e.g., multiple linear regression for
a categorical outcome; logistic regression for a continuous outcome) and
with little explanation. \textbf{Results and Discussion}: Basic
descriptive statistics, graphs, or tables are presented; the regression
model is applied with some inaccuracies and/or interpretation is
minimal. \textbf{Structure and References}: Report is mostly organized,
though with referencing inconsistencies. \\
\textbf{2.1 / Merit} & 60-69\% & 2.1 (UG) & \textbf{Good} &
\textbf{Literature Review}: Relevant literature is discussed, with some
justification for variable choice. Evidence supports the research
question well. \textbf{Methods}: Data is described with some detail,
though potential data transformations are under-explored. The regression
model is appropriate for the selected variable types. \textbf{Results
and Discussion}: Descriptive statistics and visuals are provided. Model
results are discussed, though interpretation lacks depth. Findings are
compared to existing literature. \textbf{Structure and References}:
Report is logically structured and clear, with mostly correct
citations. \\
\textbf{First / Distinction} & 70-79\% & First (UG) & \textbf{Very Good}
& \textbf{Literature Review}: Strong grasp of relevant literature, with
well-justified variable selection. Evidence aligns well with the
research question. \textbf{Methods}: Data is comprehensively described
with consideration of relevant transformations. The regression model is
appropriate and well-justified. \textbf{Results and Discussion}:
Descriptive statistics and clear visuals support findings. Model results
are accurately interpreted with strong connections to existing
literature. \textbf{Structure and References}: Report has a coherent,
professional structure with only minor referencing errors. \\
\textbf{High First / High Distinction} & 80-100\% & High First (UG) &
\textbf{Excellent to Outstanding} & \textbf{Literature Review}: Critical
and thorough literature review with strong, well-justified variable
selection. Evidence fully supports the research question with insightful
connections. \textbf{Methods}: Detailed data description and
transformation steps are clearly articulated. Regression model is
expertly applied and justified. \textbf{Results and Discussion}:
Comprehensive descriptive statistics, graphs, and tables are provided.
Model results are innovatively interpreted with strong links to existing
research. \textbf{Structure and References}: Report is professionally
structured, with flawless citations and a high standard of
organization. \\
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

In summary:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Introduction}: Should establish the topic's relevance, present
  a concise literature overview, identify a knowledge gap, and outline
  research questions.
\item
  \textbf{Literature Review}: Requires an in-depth review of relevant
  studies, justification for chosen independent variables, and
  identification of a potential knowledge gap or unexplored area aligned
  with the chosen research question.
\item
  \textbf{Methods and Data}: Should describe the dataset, variable
  transformations, and justify the regression technique. Key
  transformations, such as reclassifying variables, should be explained
  with clarity and relevance.
\item
  \textbf{Results and Discussion}: Involves presenting descriptive
  statistics, followed by a clear regression analysis. Discussion should
  interpret results, compare findings with existing literature, and
  include meaningful tables and graphs.
\item
  \textbf{Conclusion}: Summarize findings, discuss limitations, and
  suggest future directions.
\item
  \textbf{Referencing}: Requires correct and consistent citations and a
  well-structured reference list.
\end{enumerate}

Employing a novel dataset, i.e.~not employed during the practical
sessions, for the assignment will be awarded with a higher grade. For
example, the quantitative dataset from
\href{https://canvas.liverpool.ac.uk/courses/79653/pages/secondary-datasets-suitable-for-human-geography-slash-planning-students?module_item_id=2174709}{Secondary
datasets for Human Geography and Planning Students: 202425-ENVS203}.

\bookmarksetup{startatroot}

\chapter{Lab: Introduction to R for
Statistics}\label{lab-introduction-to-r-for-statistics}

\emph{The following material has been readapted from}:

\begin{itemize}
\tightlist
\item
  https://dereksonderegger.github.io/570L/1-introduction.html by Derek
  L. Sonderegger;
\item
  https://pietrostefani.github.io/gds/environR.html by Elisabetta
  Pietrostefani and Carmen Cabrera-Arnau
\end{itemize}

The lecture's slides can be found
\href{https://github.com/GDSL-UL/stats/blob/main/lectures/lecture07.pdf}{here}.

For the lab sessions and your assignment, you will need the following
software:

\begin{itemize}
\tightlist
\item
  R-4.2.2
\item
  RStudio 2022.12.0-353
\item
  The list of libraries in the next section
\end{itemize}

To install and update:

\begin{itemize}
\tightlist
\item
  R, download the appropriate version from
  \href{https://cran.r-project.org/}{The Comprehensive R Archive Network
  (CRAN)}
\item
  RStudio, download the appropriate version from
  \href{https://posit.co/download/rstudio-desktop/}{Posit}
\end{itemize}

\section{R?}\label{r}

R is an open-source program that is commonly used in Statistics. It runs
on almost every platform and is completely free and is available at
\url{www.r-project.org}. Most of the cutting-edge statistical research
is first available on R.

R is a script based language, so there is no point and click interface.
While the initial learning curve will be steeper, understanding how to
write scripts will be valuable because it leaves a clear description of
what steps you performed in your data analysis. Typically you will want
to write a script in a separate file and then run individual lines. This
saves you from having to retype a bunch of commands and speeds up the
debugging process.

\section{R(Studio) Basics}\label{rstudio-basics}

We will be running R through the program RStudio which is located at
\href{http://www.rstudio.org}{rstudio.com}. When you first open up
RStudio the console window gives you some information about the version
of R you are running and then it gives the prompt
\texttt{\textgreater{}}. This prompt is waiting for you to input a
command. The prompt + tells you that the current command is spanning
multiple lines. In a script file you might have typed something like
this:

\begin{verbatim}
for( i in 1:5 ){
    print(i)
}
\end{verbatim}

Finding help about a certain function is very easy. At the prompt, just
type \texttt{help(function.name)} or \texttt{?function.name}. If you
don't know the name of the function, your best bet is to go the the web
page www.rseek.org which will search various R resources for your
keyword(s). Another great resource is the coding question and answer
site \href{http://stackoverflow.com}{stackoverflow}.

\subsection{Starting a session in
RStudio}\label{starting-a-session-in-rstudio}

Upon startup, RStudio will look something like this.

\includegraphics[width=9.71in,height=\textheight]{labs/../img/environ_R_1.png}

\emph{Note}: the \textbf{Pane Layout} and \textbf{Appearance} settings
can be altered:

\begin{itemize}
\tightlist
\item
  on Windows by clicking RStudio\textgreater Tools\textgreater Global
  Options\textgreater Appearance or Pane Layout
\item
  on Mac OS by clicking
  RStudio\textgreater Preferences\textgreater Appearance or Pane Layout.
\end{itemize}

You will also have a standard white background; but you can choose
specific
\href{https://quarto.org/docs/output-formats/html-themes.html}{themes}.

\textbf{Source Panel (Top-Left)}

This is where you write, edit, and view scripts, R Markdown/Quarto
documents, or R scripts. It allows:

\begin{itemize}
\tightlist
\item
  Editing Scripts: Write and edit R scripts or documents (\texttt{.R},
  \texttt{.Rmd}, \texttt{.qmd}).
\item
  Executing the Code: Run lines, blocks, or the entire script directly
  from the editor.
\end{itemize}

\textbf{Console Panel (Bottom-Left)}

\textbf{The Console is the main place to run R commands interactively.}
It allows:

\begin{itemize}
\tightlist
\item
  Executing the Code: Type and run R commands directly.
\item
  Viewing outputs, warnings, and errors for immediate feedback.
\item
  Browsing and reusing past commands (History Tab).
\item
  Toggling between the R Console, and the Terminal (yuo don't really
  need the latter).
\end{itemize}

\textbf{Environment Panel (Top-Right)}

This panel helps track variables, functions, and the history of commands
used. It contains:

\begin{itemize}
\tightlist
\item
  Environment Tab: Shows all current variables, datasets, and objects in
  your session, including their structure and values.
\item
  History Tab: Provides a record of past commands. You can re-run or
  move commands to the console or script.
\end{itemize}

\textbf{Files / Plots / Packages / Help Panel (Bottom-Right)}

This multifunctional panel is for file navigation, plotting, managing
packages, viewing help, and managing jobs. It contains:

\begin{itemize}
\tightlist
\item
  Files Tab: Navigate, open, and manage files and directories within
  your project.
\item
  Plots Tab: Displays plots generated in your session. You can export or
  navigate through multiple plots here.
\item
  Packages Tab: Lists installed packages and allows you to install,
  load, and update packages.
\item
  Help Tab: Displays help documentation for R functions, packages, and
  other resources. You can search for documentation by typing a function
  or package name.
\end{itemize}

\textbf{Important: Unless you are working with a script, you will be
likely writing code on the console.}

At the start of a session, it's good practice clearing your R
environment (console):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rm}\NormalTok{(}\AttributeTok{list =} \FunctionTok{ls}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

In R, we are going to be working with \textbf{relative paths}. With the
command \texttt{getwd()}, you can see where your working directory is
currently set.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

For ENVS225, download the
\href{https://github.com/GDSL-UL/stats/archive/refs/heads/main.zip}{material}
of the module an unzip it whever you like.

The folder structure should look like:

\begin{verbatim}
stats/
├── data/
├── labs_img/
└── labs/
\end{verbatim}

You can delete other sub-folders (e.g.~docs).

{This should be on your personal computer or if on a local machine, I
suggest using the directory M: to store the folder, it can be accessed
from every computer.}

Then, in R Studio - on Windows by clicking
RStudio\textgreater Tools\textgreater Global
Options\textgreater General.. - on Mac OS by clicking
RStudio\textgreater Preferences\textgreater Appearance or Pane
Layout\ldots{}

browse and set the folder you just creted as your working directory.

Check if that has been applied.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

File paths in R work like this:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
File Path
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{MyFile.csv} & Look in the working directory for
\texttt{MyFile.csv}. \\
\texttt{MyFolder/MyFile.csv} & In the working directory, there is a
subdirectory called \texttt{MyFolder} and inside that folder is
\texttt{MyFile.csv}. \\
\end{longtable}

You do not need to set your working directory if you are using an
R-markdown or Quarto document and you have it saved in the right
location. The pathway will start from where your document is saved.

\subsection{Using the console}\label{using-the-console}

Try to use the console to perform a few operations. For example type in:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1}\SpecialCharTok{+}\DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

Slightly more complicated:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{print}\NormalTok{(}\StringTok{"hello world"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "hello world"
\end{verbatim}

If you are unsure about what a command does, use the ``Help'' panel in
your Files pane or type \texttt{?function} in the console. For example,
to see how the \texttt{dplyr::rename()} function works, type in
\texttt{?dplyr::rename}. When you see the double colon syntax like in
the previous command, it's a call to a package without loading its
library.

\subsection{R as a simple calculator}\label{r-as-a-simple-calculator}

You can use R as a simple calculator. At the prompt, type 2+3 and hit
enter. What you should see is the following

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Some simple addition}
\DecValTok{2}\SpecialCharTok{+}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5
\end{verbatim}

In this fashion you can use R as a very capable calculator.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{6}\SpecialCharTok{*}\DecValTok{8}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 48
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{4}\SpecialCharTok{\^{}}\DecValTok{3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 64
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\DecValTok{1}\NormalTok{)   }\CommentTok{\# exp() is the exponential function}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.718282
\end{verbatim}

R has most constants and common mathematical functions you could ever
want. For example, the absolute value of a number is given by
\texttt{abs()}, and \texttt{round()} will round a value to the nearest
integer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pi     }\CommentTok{\# the constant 3.14159265...}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.141593
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{abs}\NormalTok{(}\FloatTok{1.77}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.77
\end{verbatim}

Whenever you call a function, there will be some arguments that are
mandatory, and some that are optional and the arguments are separated by
a comma. In the above statements the function \texttt{abs()} requires at
least one argument, and that is the number you want the absolute value
of.

When functions require more than one argument, arguments can be
specified via the order in which they are passed or by naming the
arguments. So for the \texttt{log()} function, for example, which
calculates the logarithm of a number, one can specify the arguments
using the named values; the order woudn't matter:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Demonstrating order does not matter if you specify}
\CommentTok{\# which argument is which}
\FunctionTok{log}\NormalTok{(}\AttributeTok{x=}\DecValTok{5}\NormalTok{, }\AttributeTok{base=}\DecValTok{10}\NormalTok{)   }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.69897
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log}\NormalTok{(}\AttributeTok{base=}\DecValTok{10}\NormalTok{, }\AttributeTok{x=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.69897
\end{verbatim}

When we don't specify which argument is which, R will decide that
\texttt{x} is the first argument, and \texttt{base} is the second.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# If not specified, R will assume the second value is the base...}
\FunctionTok{log}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.69897
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{log}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.430677
\end{verbatim}

When we want to specify the arguments, we can do so using the
\texttt{name=value} notation.

\subsection{Variables Assignment}\label{variables-assignment}

We need to be able to assign a value to a variable to be able to use it
later. R does this by using an arrow \texttt{\textless{}-} or an equal
sign \texttt{=}. While R supports either, for readability, I suggest
people pick one assignment operator and stick with it.

\textbf{Variable names cannot start with a number, may not include
spaces, and are case sensitive.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\FloatTok{7.5}       \CommentTok{\# create two variables}
\NormalTok{another\_var }\OtherTok{=} \DecValTok{5}   \CommentTok{\# notice they show up in \textquotesingle{}Environment\textquotesingle{} tab in RStudio!}
\NormalTok{var }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var }\SpecialCharTok{*}\NormalTok{ another\_var }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 75
\end{verbatim}

As your analysis gets more complicated, you'll want to save the results
to a variable so that you can access the results later. \emph{If you
don't assign the result to a variable, you have no way of accessing the
result.}

\subsection{Working with Scripts}\label{working-with-scripts}

\textbf{R Scripts (.R files)}

Traditional script files look like this:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Problem 1 }
\CommentTok{\# Calculate the log of a couple of values and make a plot}
\CommentTok{\# of the log function from 0 to 3}
\FunctionTok{log}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\FunctionTok{log}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\FunctionTok{log}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(.}\DecValTok{1}\NormalTok{,}\DecValTok{3}\NormalTok{, }\AttributeTok{length=}\DecValTok{1000}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x, }\FunctionTok{log}\NormalTok{(x))}

\CommentTok{\# Problem 2}
\CommentTok{\# Calculate the exponential function of a couple of values}
\CommentTok{\# and make a plot of the function from {-}2 to 2}
\FunctionTok{exp}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)}
\FunctionTok{exp}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{, }\AttributeTok{length=}\DecValTok{1000}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(x, }\FunctionTok{exp}\NormalTok{(x))}
\end{Highlighting}
\end{Shaded}

In RStudio you can create a new script by going to
\texttt{File\ -\textgreater{}\ New\ File\ -\textgreater{}\ R\ Script}.
This opens a new window in RStudio where you can type commands and
functions as a common text editor.

This looks perfectly acceptable as a way of documenting what one does,
but this script file doesn't contain the actual results of commands you
ran, nor does it show you the plots. Also anytime you want to comment on
some output, it needs to be offset with the commenting character
\texttt{\#}. It would be nice to have both the commands and the results
merged into one document. This is what the R Markdown file does for us.

\textbf{R Markdown (\texttt{.Rmd} and \texttt{.qmd} files)}

The R Markdown is an implementation of the Markdown syntax that makes it
extremely easy to write webpages or scientific documents that include
conde. This syntax was extended to allow users to embed R code directly
into more complex documents. Perhaps the easiest way to understand the
syntax is to look at an at the
\href{http://rmarkdown.rstudio.com}{RMarkdown website}.

The R code in a R Markdown document (\texttt{.rmd} file extension) can
be nicely separated from regular text using the three backticks (3 times
`, see below) and an instruction that it is R code that needs to be
evaluated. A code chunk will look like:

\begin{Shaded}
\begin{Highlighting}[]
    \ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}\FunctionTok{print}\NormalTok{(i)\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
\end{verbatim}

\textbf{In ENVS225: In this module we will be using \texttt{.qmd} a more
flexible development of \texttt{.rmd} files.}

Markdown files present several advantages compared to writing your code
in the console or just using scripts. You'll save yourself a huge amount
of work by embracing Markdown files from the beginning; you will keep
track of your code and your steps, be able to document and present how
you did your analysis (helpful when writing the methods section of a
paper), and it will make it easier to re-run an analysis after a change
in the data (such as additional data values, transformed data, or
removal of outliers) or once you spot an error. Finally, it makes the
script more readable.

\subsection{R Packages}\label{r-packages}

One of the greatest strengths about R is that so many people have
developed add-on packages to do some additional function. To download
and install the package from the Comprehensive R Archive Network (CRAN),
you just need to ask RStudio it to install it via the menu
\texttt{Tools} -\textgreater{} \texttt{Install\ Packages...}. Once
there, you just need to give the name of the package and RStudio will
download and install the package on your computer.

Once a package is downloaded and installed on your computer, it is
available, but it is not loaded into your current R session by default.
To improve overall performance only a few packages are loaded by default
and the you must explicitly load packages whenever you want to use them.
You only need to load them once per session/script.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)   }\CommentTok{\# load the dplyr library, will be useful later}
\end{Highlighting}
\end{Shaded}

\section{Practice: Dataset and
Dataframes}\label{practice-dataset-and-dataframes}

\begin{quote}
\textbf{First of all}, create a new Markdown document. We use the
\texttt{File\ -\textgreater{}\ New\ File\ -\textgreater{}\ Quarto\ Document..}
dropdown option, and a menu will appear asking you for the document
title, author, and preferred output type. You can select HTML, but you
will need your assignment to be submitted in PDF; more on that later.

Follow the practical below. You can describe what you are doing in
normal text. See
\href{https://quarto.org/docs/authoring/markdown-basics.html}{here} for
how to format normal text in Markdown documents

Remember, when you want to write code in a markdown document you have to
enclose it like this:
\end{quote}

\includegraphics[width=0.39in,height=\textheight]{labs/../img/codeChunk.png}

\begin{quote}
or you can insert it manually:
\end{quote}

\includegraphics[width=3.45in,height=\textheight]{labs/../img/codeChunk_visual.png}

Within this module we will be working with data stored in so-called
datasets. A dataset is a structured collection of data points that
represent various measurements or observations, often organized in a
tabular format with rows and columns. A dataset might contain
information about different locations, such as neighborhoods or cities,
with each row representing a place and each column detailing
characteristics like population density, average income, or number of
green parks. For example, a dataset could be compiled to study patterns
in urban mobility, where the data includes the number of daily
commuters, the distance they travel, and the mode of transport they use.
Datasets provide the essential building blocks for statistical analysis;
they enable exploring relationships, identifying patterns, and drawing
conclusions about certatin phenomena.

Examples of everyday datasets:

\begin{itemize}
\tightlist
\item
  \textbf{Premier League Standings}: Each row represents a team, with
  columns for points, games played, wins, draws, and losses.
\item
  \textbf{Movie Dataset}: Each row represents a movie, with columns
  showing its title, genre, release year, director, and rating.
\item
  \textbf{Weather Dataset}: Each row shows a day's weather in a city,
  with columns for temperature, humidity, wind speed, and precipitation.
\end{itemize}

Usually, data is organized in

\begin{itemize}
\tightlist
\item
  \textbf{Columns} of data representing some trait or variable that we
  might be interested in. In general, we might wish to investigate the
  relationship between variables.
\item
  \textbf{Rows} represent a single object on which the column traits are
  measured.
\end{itemize}

For example, in a grade book for recording students scores throughout
the semester, their is one row for every student and columns for each
assignment. A greenhouse experiment dataset will have a row for every
plant and columns for treatment type and biomass.

\subsection{Datasets in R}\label{datasets-in-r}

In R, we want a way of storing data where it feels just as if we had an
Excel Spreadsheet where each row represents an observation and each
column represents some information about that observation. We will call
this object a \texttt{data.frame}, an R represention of a data set. The
easiest way to understand data frames is to create one.

\textbf{Task}: Copy the code below in your markdown. Create a
\texttt{data.frame} that represents an instructor's grade book, where
each row is a student, and each column represents some sort of
assessment.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Grades }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Name  =} \FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Bob\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Jeff\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Mary\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}Valerie\textquotesingle{}}\NormalTok{),     }
  \AttributeTok{Exam.1 =} \FunctionTok{c}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{75}\NormalTok{, }\DecValTok{92}\NormalTok{, }\DecValTok{85}\NormalTok{),}
  \AttributeTok{Exam.2 =} \FunctionTok{c}\NormalTok{(}\DecValTok{87}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{81}\NormalTok{)}
\NormalTok{)}
\CommentTok{\# Show the data.frame }
\CommentTok{\# View(Grades)  \# show the data in an Excel{-}like tab.  Doesn\textquotesingle{}t work when knitting }
\NormalTok{Grades          }\CommentTok{\# show the output in the console. This works when knitting}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Name Exam.1 Exam.2
1     Bob     90     87
2    Jeff     75     71
3    Mary     92     95
4 Valerie     85     81
\end{verbatim}

\textbf{To execute just one chunk of code press the green arrow
top-right of the chunk:}

\includegraphics[width=2.68in,height=\textheight]{labs/../img/runChunk.png}

R allows two differnt was to access elements of the \texttt{data.frame}.
First is a matrix-like notation for accessing particular values.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Format & Result \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{{[}a,b{]}} & Element in row \texttt{a} and column \texttt{b} \\
\texttt{{[}a,{]}} & All of row \texttt{a} \\
\texttt{{[},b{]}} & All of column \texttt{b} \\
\end{longtable}

Because the columns have meaning and we have given them column names, it
is desirable to want to access an element by the name of the column as
opposed to the column number.

\textbf{Task}: Copy and Run:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Grades[, }\DecValTok{2}\NormalTok{]       }\CommentTok{\# print out all of column 2 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 90 75 92 85
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Grades}\SpecialCharTok{$}\NormalTok{Name       }\CommentTok{\# The ${-}sign means to reference a column by its label}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Bob"     "Jeff"    "Mary"    "Valerie"
\end{verbatim}

\subsection{Importing Data in R}\label{importing-data-in-r}

From:
https://raw.githubusercontent.com/dereksonderegger/570L/master/07\_DataImport.Rmd

Usually we won't type the data in by hand, but rather load the data from
some package. Reading data from external sources is a necessary skill.

\textbf{\emph{Comma Separated Values}} \textbf{Data}

To consider how data might be stored, we first consider the simplest
file format: the comma separated values file (\texttt{.csv}). In this
file time, each of the ``cells'' of data are separated by a comma. For
example, the data file storing scores for three students might be as
follows:

\begin{verbatim}
Able, Dave, 98, 92, 94
Bowles, Jason, 85, 89, 91
Carr, Jasmine, 81, 96, 97
\end{verbatim}

Typically when you open up such a file on a computer with MS Excel
installed, Excel will open up the file assuming it is a spreadsheet and
put each element in its own cell. However, you can also open the file
using a more primitive program (say Notepad in Windows, TextEdit on a
Mac) you'll see the raw form of the data.

Having just the raw data without any sort of column header is
problematic (which of the three exams was the final??). Ideally we would
have column headers that store the name of the column.

\begin{verbatim}
LastName, FirstName, Exam1, Exam2, FinalExam
Able, Dave, 98, 92, 94
Bowles, Jason, 85, 89, 91
Carr, Jasmine, 81, 96, 97
\end{verbatim}

\textbf{Reading (\texttt{.csv}) files}

To make R read in the data arranged in this format, we need to tell R
three things:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Where does the data live? Often this will be the name of a file on
  your computer, but the file could just as easily live on the internet
  (provided your computer has internet access).
\item
  Is the first row data or is it the column names?
\item
  What character separates the data? Some programs store data using tabs
  to distinguish between elements, some others use white space. R's
  mechanism for reading in data is flexible enough to allow you to
  specify what the separator is.
\end{enumerate}

The primary function that we'll use to read data from a file and into R
is the function \texttt{read.csv()}. This function has many optional
arguments but the most commonly used ones are outlined in the table
below.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Argument
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Default
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\texttt{file} & Required & A character string denoting the file
location. \\
\texttt{header} & \texttt{TRUE} & Specifies whether the first line
contains column headers. \\
\texttt{sep} & \texttt{","} & Specifies the character that separates
columns. For \texttt{read.csv()}, this is usually a comma. \\
\texttt{skip} & \texttt{0} & The number of lines to skip before reading
data; useful for files with descriptive text before the actual data. \\
\texttt{na.strings} & \texttt{"NA"} & Values that represent missing
data; multiple values can be specified, e.g.,
\texttt{c("NA",\ "-9999")}. \\
\texttt{quote} & \texttt{"} & Specifies the character used to quote
character strings, typically \texttt{"} or
\texttt{\textquotesingle{}}. \\
\texttt{stringsAsFactors} & \texttt{FALSE} & Controls whether character
strings are converted to factors; \texttt{FALSE} means they remain as
character data. \\
\texttt{row.names} & \texttt{NULL} & Allows specifying a column as row
names, or assigning \texttt{NULL} to use default indexing for rows. \\
\texttt{colClasses} & \texttt{NULL} & Specifies the data type for each
column to speed up reading for large files, e.g.,
\texttt{c("character",\ "numeric")}. \\
\texttt{encoding} & \texttt{"unknown"} & Sets the text encoding of the
file, which can be useful for files with special or international
characters. \\
\end{longtable}

Most of the time you just need to specify the file. \textbar{}

Task: Let's read in a dataset of terrorist attacks that have taken place
in the UK:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attacks }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\AttributeTok{file   =} \StringTok{\textquotesingle{}../data/attacksUK.csv\textquotesingle{}}\NormalTok{)  }\CommentTok{\# where the data lives                                 }
\FunctionTok{View}\NormalTok{(attacks)}
\end{Highlighting}
\end{Shaded}

\section{Practice: Descriptive
Statistics}\label{practice-descriptive-statistics}

\subsection{Summarizing Data}\label{summarizing-data}

It is very important to be able to take a data set and produce summary
statistics such as the mean and standard deviation of a column. For this
sort of manipulation, we use the package \texttt{dplyr}. This package
allows chaining together many common actions to form a particular task.

The foundational operations to perform on a data set are:

\begin{itemize}
\item
  Subsetting - Returns a with only particular columns or rows

  -- \texttt{select} - Selecting a subset of columns by name or column
  number.

  -- \texttt{filter} - Selecting a subset of rows from a data frame
  based on logical expressions.

  -- \texttt{slice} - Selecting a subset of rows by row number.
\item
  \texttt{arrange} - Re-ordering the rows of a data frame.
\item
  \texttt{mutate} - Add a new column that is some function of other
  columns.
\item
  \texttt{summarise} - calculate some summary statistic of a column of
  data. This collapses a set of rows into a single row.
\end{itemize}

Each of these operations is a function in the package \texttt{dplyr}.
These functions all have a similar calling syntax,: - The first argument
is a data set;. - Subsequent arguments describe what to do with the
input data frame and you can refer to the columns without using the
\texttt{df\$column} notation.

All of these functions will return a data set.

The \texttt{dplyr} package also includes a function that ``pipes''
commands together. The idea is that the \texttt{\%\textgreater{}\%}
operator works by translating the command
\texttt{a\ \%\textgreater{}\%\ f(b)} to the expression \texttt{f(a,b)}.
This operator works on any function \texttt{f}. The beauty of this comes
when you have a suite of functions that takes input arguments of the
same type as their output. For example if we wanted to start with
\texttt{x}, and first apply function \texttt{f()}, then \texttt{g()},
and then \texttt{h()}, the usual R command would be \texttt{h(g(f(x)))}
which is hard to read because you have to start reading at the innermost
set of parentheses. Using the pipe command \texttt{\%\textgreater{}\%},
this sequence of operations becomes
\texttt{x\ \%\textgreater{}\%\ f()\ \%\textgreater{}\%\ g()\ \%\textgreater{}\%\ h()}.
For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Grades }\CommentTok{\# Recall the Grades data }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Name Exam.1 Exam.2
1     Bob     90     87
2    Jeff     75     71
3    Mary     92     95
4 Valerie     85     81
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The following code takes the Grades data.frame and calculates }
\CommentTok{\# a column for the average exam score, and then sorts the data }
\CommentTok{\# according to the that average score}
\NormalTok{Grades }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{( }\AttributeTok{Avg.Score =}\NormalTok{ (Exam}\FloatTok{.1} \SpecialCharTok{+}\NormalTok{ Exam}\FloatTok{.2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{ ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{( Avg.Score )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Name Exam.1 Exam.2 Avg.Score
1    Jeff     75     71      73.0
2 Valerie     85     81      83.0
3     Bob     90     87      88.5
4    Mary     92     95      93.5
\end{verbatim}

Keep it in mind, it is not necessary to memorise this.

Let's consider the \texttt{summarize} function to calculate the mean
score for \texttt{Exam.1}. Notice that this takes a data frame of four
rows, and summarizes it down to just one row that represents the
summarized data for all four students.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr) }\CommentTok{\# load the library}
\NormalTok{Grades }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\AttributeTok{Exam.1.mean =} \FunctionTok{mean}\NormalTok{( Exam}\FloatTok{.1}\NormalTok{ ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Exam.1.mean
1        85.5
\end{verbatim}

Similarly you could calculate the \textbf{standard deviation} for the
exam as well.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Grades }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{( }\AttributeTok{Exam.1.mean =} \FunctionTok{mean}\NormalTok{( Exam}\FloatTok{.1}\NormalTok{ ),}
             \AttributeTok{Exam.1.sd   =} \FunctionTok{sd}\NormalTok{(   Exam}\FloatTok{.1}\NormalTok{   ) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Exam.1.mean Exam.1.sd
1        85.5  7.593857
\end{verbatim}

\textbf{Task:} Write the code above in your markdown file and run it. Do
not to copy it this time.

Let's go back to the terrorist attacks. There are attacks perpetrated by
several different groups. Each record is a single attack and contains
information about who perpetrated the attack, what year, how many were
killed and how many were wounded. You can get a glimpse of the dataframe
with the function head

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(attacks, }\AttributeTok{n =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   nrKilled nrWound year        country                        group
1         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades
2         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades
3         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades
4         0       0 2005 United Kingdom   Abu Hafs al-Masri Brigades
5         0       1 1982 United Kingdom Abu Nidal Organization (ANO)
6         0       0 2014 United Kingdom                   Anarchists
7         0       0 2014 United Kingdom                   Anarchists
8         0       0 2014 United Kingdom                   Anarchists
9         0       0 2014 United Kingdom                   Anarchists
10        0       0 2014 United Kingdom                   Anarchists
                           attack                      target
1               Bombing/Explosion              Transportation
2               Bombing/Explosion              Transportation
3               Bombing/Explosion              Transportation
4               Bombing/Explosion              Transportation
5                   Assassination     Government (Diplomatic)
6  Facility/Infrastructure Attack                    Business
7  Facility/Infrastructure Attack                    Business
8  Facility/Infrastructure Attack                    Business
9  Facility/Infrastructure Attack Private Citizens & Property
10 Facility/Infrastructure Attack                      Police
                      weapon
1  Explosives/Bombs/Dynamite
2  Explosives/Bombs/Dynamite
3  Explosives/Bombs/Dynamite
4  Explosives/Bombs/Dynamite
5                   Firearms
6                 Incendiary
7                 Incendiary
8                 Incendiary
9                 Incendiary
10                Incendiary
\end{verbatim}

We might want to compare different actors and see the mean and standard
deviation of the number of people wound, by each group's attack, across
time. To do this, we are still going to use the \texttt{summarize}, but
we will precede that with \texttt{group\_by(group)} to tell the
subsequent \texttt{dplyr} functions to perform the actions separately
for each breed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{attacks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{( group) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{( }\AttributeTok{Mean =} \FunctionTok{mean}\NormalTok{(attacks}\SpecialCharTok{$}\NormalTok{nrWound), }
             \AttributeTok{Std.Dev =} \FunctionTok{sd}\NormalTok{(attacks}\SpecialCharTok{$}\NormalTok{nrWound))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 38 x 3
   group                                               Mean Std.Dev
   <chr>                                              <dbl>   <dbl>
 1 Abu Hafs al-Masri Brigades                         0.963    7.22
 2 Abu Nidal Organization (ANO)                       0.963    7.22
 3 Anarchists                                         0.963    7.22
 4 Animal Liberation Front (ALF)                      0.963    7.22
 5 Animal Rights Activists                            0.963    7.22
 6 Armenian Secret Army for the Liberation of Armenia 0.963    7.22
 7 Black September                                    0.963    7.22
 8 Continuity Irish Republican Army (CIRA)            0.963    7.22
 9 Dissident Republicans                              0.963    7.22
10 Informal Anarchist Federation                      0.963    7.22
# i 28 more rows
\end{verbatim}

\textbf{Task:} Write the code above in your markdown file and run it.
Try out another categorical variable instead of \texttt{group}
(e.g.~\texttt{year}) and \texttt{nrKilled} instead of \texttt{nrWound}.

Let's now move to another dataset to address a research question. For
illustration purposes, we will use the \textbf{Family Resources Survey
(FRS)}. The FRS is an annual survey conducted by the UK government that
collects detailed information about the income, living conditions, and
resources of private households across the United Kingdom. Managed by
the Department for Work and Pensions (DWP), the FRS provides data that
is essential for understanding the economic and social conditions of
households and informing public policy.

Consider questions such as:

\begin{itemize}
\tightlist
\item
  How many respondents (persons) are there in the 2016-17 FRS?
\item
  How many variables (population attributes) are there?
\item
  What types of variables are present in the FRS?
\item
  What is the most detailed geography available in the FRS?
\end{itemize}

\textbf{Task}: To answer these questions, load and inspect the dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the FRS dataset should be already loaded, otherwise}
\NormalTok{frs\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/FamilyResourceSurvey/FRS16{-}17\_labels.csv"}\NormalTok{) }

\CommentTok{\# Display basic structure }
\FunctionTok{glimpse}\NormalTok{(frs\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 44,145
Columns: 45
$ household        <int> 6087, 6101, 6103, 6122, 6134, 6136, 6138, 6140, 6143,~
$ family           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~
$ person           <int> 5, 3, 3, 3, 2, 4, 4, 3, 3, 4, 4, 3, 4, 2, 5, 3, 4, 3,~
$ country          <chr> "England", "England", "England", "Northern Ireland", ~
$ region           <chr> "London", "South East", "Yorks and the Humber", "Nort~
$ age_group        <chr> "05-10", "05-10", "05-10", "05-10", "05-10", "05-10",~
$ sex              <chr> "Female", "Male", "Male", "Female", "Female", "Female~
$ marital_status   <chr> "Single", "Single", "Single", "Single", "Single", "Si~
$ ethnicity        <chr> "Mixed / multiple ethnic groups", "White", "White", "~
$ hrp              <chr> "Not HRP", "Not HRP", "Not HRP", "Not HRP", "Not HRP"~
$ rel_to_hrp       <chr> "Son/daughter (incl. adopted)", "Son/daughter (incl. ~
$ lifestage        <chr> "Child (0-17)", "Child (0-17)", "Child (0-17)", "Chil~
$ dependent        <chr> "Dependent", "Dependent", "Dependent", "Dependent", "~
$ arrival_year     <chr> "UK Born", "UK Born", "UK Born", "UK Born", "UK Born"~
$ birth_country    <chr> "Dependent child", "Dependent child", "Dependent chil~
$ care_hours       <chr> "0 hours per week", "0 hours per week", "0 hours per ~
$ educ_age         <chr> "Dependent child", "Dependent child", "Dependent chil~
$ educ_type        <chr> "School (full-time)", "School (full-time)", "School (~
$ fam_youngest     <chr> "7", "4", "0", "7", "0", "9", "10", "0", "3", "10", "~
$ fam_toddlers     <int> 0, 1, 1, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1,~
$ fam_size         <int> 4, 4, 4, 3, 4, 4, 3, 5, 4, 4, 3, 4, 4, 3, 5, 4, 4, 4,~
$ happy            <chr> "Dependent child", "Dependent child", "Dependent chil~
$ health           <chr> "Not known", "Not known", "Not known", "Not known", "~
$ hh_accom_type    <chr> "Terraced house/bungalow", "Detached house/bungalow",~
$ hh_benefits      <int> 10868, 0, 1768, 8632, 8372, 1768, 1768, 1768, 0, 0, 1~
$ hh_composition   <chr> "Three or more adults, 1+ children", "One adult femal~
$ hh_ctax_band     <chr> "Band D", "Band F", "Band A", "Band B", "Band A", "Ba~
$ hh_housing_costs <chr> "4316", "10296", "5408", "Northern Ireland", "5720", ~
$ hh_income_gross  <int> 54236, 180804, 26936, 19968, 17992, 76596, 31564, 366~
$ hh_income_net    <int> 44668, 120640, 23556, 19968, 17992, 62868, 29744, 287~
$ hh_size          <int> 5, 4, 4, 3, 4, 4, 4, 5, 4, 4, 4, 4, 4, 5, 5, 4, 4, 4,~
$ hh_tenure        <chr> "Mortgaged (including part rent / part own)", "Mortga~
$ highest_qual     <chr> "Dependent child", "Dependent child", "Dependent chil~
$ income_gross     <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
$ income_net       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~
$ jobs             <chr> "Dependent child", "Dependent child", "Dependent chil~
$ life_satisf      <chr> "Dependent child", "Dependent child", "Dependent chil~
$ nssec            <chr> "Dependent child", "Dependent child", "Dependent chil~
$ sic_chapter      <chr> "Dependent child", "Dependent child", "Dependent chil~
$ sic_division     <chr> "Dependent child", "Dependent child", "Dependent chil~
$ soc2010          <chr> "Dependent child", "Dependent child", "Dependent chil~
$ work_hours       <chr> "Dependent child", "Dependent child", "Dependent chil~
$ workstatus       <chr> "Dependent Child", "Dependent Child", "Dependent Chil~
$ years_ft_work    <chr> "Dependent child", "Dependent child", "Dependent chil~
$ survey_weight    <int> 2315, 1317, 2449, 427, 1017, 1753, 1363, 1344, 828, 1~
\end{verbatim}

and summary:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(frs\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   household         family          person       country         
 Min.   :    1   Min.   :1.000   Min.   :1.00   Length:44145      
 1st Qu.: 4816   1st Qu.:1.000   1st Qu.:1.00   Class :character  
 Median : 9673   Median :1.000   Median :2.00   Mode  :character  
 Mean   : 9677   Mean   :1.106   Mean   :1.98                     
 3rd Qu.:14553   3rd Qu.:1.000   3rd Qu.:3.00                     
 Max.   :19380   Max.   :6.000   Max.   :9.00                     
    region           age_group             sex            marital_status    
 Length:44145       Length:44145       Length:44145       Length:44145      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
  ethnicity             hrp             rel_to_hrp         lifestage        
 Length:44145       Length:44145       Length:44145       Length:44145      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
  dependent         arrival_year       birth_country       care_hours       
 Length:44145       Length:44145       Length:44145       Length:44145      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
   educ_age          educ_type         fam_youngest        fam_toddlers   
 Length:44145       Length:44145       Length:44145       Min.   :0.0000  
 Class :character   Class :character   Class :character   1st Qu.:0.0000  
 Mode  :character   Mode  :character   Mode  :character   Median :0.0000  
                                                          Mean   :0.2557  
                                                          3rd Qu.:0.0000  
                                                          Max.   :4.0000  
    fam_size        happy              health          hh_accom_type     
 Min.   :1.000   Length:44145       Length:44145       Length:44145      
 1st Qu.:2.000   Class :character   Class :character   Class :character  
 Median :2.000   Mode  :character   Mode  :character   Mode  :character  
 Mean   :2.599                                                           
 3rd Qu.:4.000                                                           
 Max.   :9.000                                                           
  hh_benefits    hh_composition     hh_ctax_band       hh_housing_costs  
 Min.   :    0   Length:44145       Length:44145       Length:44145      
 1st Qu.:    0   Class :character   Class :character   Class :character  
 Median : 1768   Mode  :character   Mode  :character   Mode  :character  
 Mean   : 5670                                                           
 3rd Qu.:10192                                                           
 Max.   :54080                                                           
 hh_income_gross   hh_income_net        hh_size      hh_tenure        
 Min.   :-326092   Min.   :-334776   Min.   :1.00   Length:44145      
 1st Qu.:  22256   1st Qu.:  20748   1st Qu.:2.00   Class :character  
 Median :  35984   Median :  31512   Median :3.00   Mode  :character  
 Mean   :  46076   Mean   :  37447   Mean   :2.96                     
 3rd Qu.:  57252   3rd Qu.:  47008   3rd Qu.:4.00                     
 Max.   :1165216   Max.   :1116596   Max.   :9.00                     
 highest_qual        income_gross       income_net          jobs          
 Length:44145       Min.   :-354848   Min.   :-358592   Length:44145      
 Class :character   1st Qu.:     52   1st Qu.:      0   Class :character  
 Mode  :character   Median :  12740   Median :  12012   Mode  :character  
                    Mean   :  17305   Mean   :  14204                     
                    3rd Qu.:  23712   3rd Qu.:  20384                     
                    Max.   :1127360   Max.   :1110928                     
 life_satisf           nssec           sic_chapter        sic_division      
 Length:44145       Length:44145       Length:44145       Length:44145      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
   soc2010           work_hours         workstatus        years_ft_work     
 Length:44145       Length:44145       Length:44145       Length:44145      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
                                                                            
                                                                            
                                                                            
 survey_weight  
 Min.   :  221  
 1st Qu.: 1097  
 Median : 1380  
 Mean   : 1459  
 3rd Qu.: 1742  
 Max.   :39675  
\end{verbatim}

\subsection{Understanding the Structure of the FRS
Datafile}\label{understanding-the-structure-of-the-frs-datafile}

In the FRS data structure, each row represents a person, but:

\begin{itemize}
\tightlist
\item
  Each person is nested within a family.
\item
  Each family is nested within a household.
\end{itemize}

Below is an example dataset structure:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
household
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
family
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
person
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age\_group
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
marital\_status
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
rel\_to\_hrp
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 1 & 1 & London & 40-44 & Female & Married/Civil partnership &
Spouse \\
1 & 1 & 2 & London & 40-44 & Male & Married/Civil partnership &
Household Representative \\
1 & 1 & 3 & London & 5-10 & Male & Single & Son/daughter
(incl.~adopted) \\
1 & 1 & 4 & London & 5-10 & Female & Single & Son/daughter
(incl.~adopted) \\
1 & 1 & 5 & London & 16-19 & Male & Single & Step-son/daughter \\
2 & 1 & 1 & Scotland & 35-39 & Male & Single & Household
Representative \\
3 & 1 & 1 & Yorks and the Humber & 35-39 & Female & Married/Civil
partnership & Household Representative \\
3 & 1 & 2 & Yorks and the Humber & 35-39 & Male & Married/Civil
partnership & Spouse \\
3 & 1 & 3 & Yorks and the Humber & 5-10 & Male & Single &
Step-son/daughter \\
4 & 1 & 1 & Wales & 0-4 & Male & Single & Son/daughter
(incl.~adopted) \\
4 & 1 & 2 & Wales & 60-64 & Male & Married/Civil partnership & Household
Representative \\
4 & 1 & 3 & Wales & 55-59 & Female & Married/Civil partnership &
Spouse \\
4 & 2 & 3 & Wales & 30-34 & Female & Single & Son/daughter
(incl.~adopted) \\
\end{longtable}

The first five people in the FRS all belong to the same household
(household 1); they also all belong to the same family. This family
comprises a married middle-aged couple plus their three children, one of
whom is a stepson.

The second household (household 2) comprises only one person -- a single
middle-aged male.The third household comprises another married couple,
this time with two children.

Superficially the fourth household looks similar to households 1 and 2:
a married couple plus their daughter. The difference is that this
particular married couple is nearing retirement age, and their daughter
is middle-aged. Consequently, despite being a child of the married
couple, the middle-aged daughter is treated as a separate `family'
(family 2 in the household). This is because the FRS (and Census) define
a `family' as a couple plus any `dependent' children. A dependent child
is defined as a child who is either` aged 0-15 or aged 16-19, unmarried
and in full-time education. All children aged 16-19 who are married or
no longer in full-time education are regarded as `independent' adults
who form their own family unit, as are all children aged 20+.

The inclusion of all persons in a household allows us more flexibility
in the types of research question we can answer. For example, we could
explore how the likelihood of a woman being in paid employment
\texttt{WorkStatus} is influenced by the age of the youngest child still
living in her family (if any) \texttt{fam\_youngest}.

In the FRS (and Census), a ``family'' is defined as a couple and any
``dependent'' children. Dependent children are defined as those aged
0--15, or aged 16--19 if unmarried and in full-time education.

\subsection{Explore the Distribution of Your Outcome
Variable}\label{explore-the-distribution-of-your-outcome-variable}

Before starting your analysis, it is critical to know the type of scale
used to measure your outcome variable: is it categorical or continuous?
Here we will start off by exploring a continuous variable which can then
turn into a categorical variable (e.g.~top earners: yes or no). We
explore the income distribution in the UK by first looking at the low
and high end of the distribution ie. What sorts of people have high (or
low) incomes?

In the FRS each person's annual income is recorded, both gross (pre-tax)
and net (post-tax). This income includes all income sources, including
earnings, profits, investment returns, state benefits, occupational
pensions etc. As it is possible to make a loss on some of these
activities, it is also possible (although unusual) for someone's gross
or net annual income in a given year to be negative (representing an
overall loss).

\textbf{Task}: Load the FRS dataset into your R environment, if it's not
already loaded, and inspect the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the dataset (replace \textquotesingle{}frs\_data.csv\textquotesingle{} with the actual file path)}
\NormalTok{frs\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/FamilyResourceSurvey/FRS16{-}17\_labels.csv"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Open the dataset in RStudio's \textbf{Data Viewer} to explore its
structure, including the \texttt{income\_gross} and \texttt{income\_net}
variables.

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\# Open the data in the RStudio Viewer}
 \FunctionTok{View}\NormalTok{(frs\_data)}
\end{Highlighting}
\end{Shaded}

in the \textbf{Data Viewer} tab, scroll horizontally to locate the
\texttt{income\_gross} and \texttt{income\_net} columns. If columns are
listed alphabetically, they will appear near other attributes that start
with ``income.''

You should notice two things:

\begin{itemize}
\tightlist
\item
  Incomes are recorded to the nearest £, NOT in income bands.
\item
  Dependent children almost all have a recorded income of £0.
\end{itemize}

This second observation highlights the somewhat loose wording of our
question above (\emph{What sorts of people have high (or low)
incomes?}). To avoid reaching the somewhat banal conclusion that those
with the lowest of all incomes are almost all children, we should
re-frame the question more precisely as \emph{What sorts of people
(excluding dependent children) have low incomes?}

\textbf{Task}: Determine the Scale of the Outcome Variable.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summarize income variables}
\FunctionTok{summary}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{income\_gross)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-354848      52   12740   17305   23712 1127360 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summarize income variables}
\FunctionTok{summary}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{income\_net)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-358592       0   12012   14204   20384 1110928 
\end{verbatim}

\textbf{Task}: Exclude Dependent Children.

You need to select all cases (persons) that are independent, that is
where the variable \texttt{dependent} has values different from
\texttt{!=} ``Dependent'' or equal \texttt{==} ``Independent''.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Filter to include only independent persons}
\NormalTok{frs\_independent }\OtherTok{\textless{}{-}}\NormalTok{ frs\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(dependent }\SpecialCharTok{!=} \StringTok{"Dependent"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\textbf{Task}: Create a basic histogram (a visualisation lecture is
scheduled later on).

The income variables in the FRS are all scale variables so a good
starting point is to examine its distribution looking at a histogram of
\texttt{income\_gross}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
 
 \FunctionTok{ggplot}\NormalTok{(frs\_independent, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ income\_gross)) }\SpecialCharTok{+}
   \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5000}\NormalTok{, }\AttributeTok{fill =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{labs}\NormalTok{(}
     \AttributeTok{title =} \StringTok{"Distribution of Gross Household Income"}\NormalTok{,}
     \AttributeTok{x =} \StringTok{"Gross Income (£)"}\NormalTok{,}
     \AttributeTok{y =} \StringTok{"Frequency"}
\NormalTok{   ) }\SpecialCharTok{+}
   \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{90000}\NormalTok{) }\SpecialCharTok{+}
   \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/01.introR_files/figure-pdf/unnamed-chunk-34-1.pdf}

You should see the histogram below. It reveals that the income
distribution is very skewed with few people earning high salaries and
the majority earning just over or less 35,000 annually.

\textbf{Task}: Adopt a regrouping strategy.

You can also cross-tabulate gross (or net) income with any of the other
variables in the FRS to your heart's content -- or can you?

Again, here is important to recall that the income variables in the FRS
are all `scale' variables; in other words, they are precise measures
rather than broad categories. Consequently, every single person in the
FRS potentially has their own unique income value. That could make for a
table c.~44,000 rows long (one row per person) if each person has their
own unique value. The solution is to create a categorical version of the
original income variable by assigning each person to one of a set of
income categories (income bands). Having done this, cross-tabulation
then becomes possible.

But which strategy to use? Equal intervals, percentiles or `ad hoc'.
Here I would suggest that `ad hoc' is best: all you want to do is to
allocate each independent adult to one of three arbitrarily defined
groups: `low', `middle' and `high' income. \textbf{Define \texttt{Low}
and \texttt{High} Income Thresholds}

Define thresholds for income categories:

\begin{itemize}
\tightlist
\item
  Low-income threshold: £\_\_\_\_\_\_\_\_
\item
  High-income threshold: £\_\_\_\_\_\_\_
\end{itemize}

\textbf{Task}: Create a New Variable Based on Regrouping of Original
Variable.

Recode \texttt{income\_gross} into categories based on the chosen
thresholds.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Define thresholds for income categories }
\NormalTok{LOW\_THRESHOLD }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# Replace with the upper limit for low income }
\NormalTok{HIGH\_THRESHOLD }\OtherTok{\textless{}{-}} \DecValTok{50000} \CommentTok{\# Replace with the lower limit for high income }

\CommentTok{\# Define income categories based on thresholds }
\NormalTok{frs\_independent }\OtherTok{\textless{}{-}}\NormalTok{ frs\_independent }\SpecialCharTok{\%\textgreater{}\%} 
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{income\_category =} \FunctionTok{case\_when}\NormalTok{( }
\NormalTok{        income\_gross }\SpecialCharTok{\textless{}=}\NormalTok{ LOW\_THRESHOLD }\SpecialCharTok{\textasciitilde{}} \StringTok{"Low"}\NormalTok{, }
\NormalTok{        income\_gross }\SpecialCharTok{\textgreater{}=}\NormalTok{ HIGH\_THRESHOLD }\SpecialCharTok{\textasciitilde{}} \StringTok{"High"}\NormalTok{, }
        \ConstantTok{TRUE} \SpecialCharTok{\textasciitilde{}} \StringTok{"Middle"}\NormalTok{ ))}
\end{Highlighting}
\end{Shaded}

The \texttt{mutate()} function in R, from the \textbf{dplyr} package, is
used to add or modify columns in a data frame. It allows you to create
new variables or transform existing ones by applying calculations or
conditional statements directly within the function.

Explanation of the code

\begin{itemize}
\tightlist
\item
  \textbf{\texttt{frs\_independent\ \%\textgreater{}\%}}: The pipe
  operator \texttt{\%\textgreater{}\%} sends \texttt{frs\_independent}
  into \texttt{mutate()}, allowing us to apply transformations without
  reassigning it repeatedly.
\item
  \textbf{\texttt{mutate()}}: Starts the transformation process by
  defining new or modified columns.
\item
  \textbf{\texttt{income\_category\ =\ case\_when(...)}}:

  \begin{itemize}
  \tightlist
  \item
    This creates a new column named \texttt{income\_category}.
  \item
    The \texttt{case\_when()} function defines conditions for assigning
    values to this new column.
  \end{itemize}
\item
  \textbf{\texttt{case\_when()}}:

  \begin{itemize}
  \tightlist
  \item
    \texttt{case\_when()} is used here to assign categorical labels
    based on conditions.
  \item
    \texttt{income\_gross\ \textless{}=\ LOW\_THRESHOLD\ \textasciitilde{}\ "Low"}:
    If \texttt{income\_gross} is less than or equal to
    \texttt{LOW\_THRESHOLD}, \texttt{income\_category} will be labeled
    ``Low.''
  \item
    \texttt{income\_gross\ \textgreater{}=\ HIGH\_THRESHOLD\ \textasciitilde{}\ "High"}:
    If \texttt{income\_gross} is greater than or equal to
    \texttt{HIGH\_THRESHOLD}, \texttt{income\_category} will be labeled
    ``High.''
  \item
    \texttt{TRUE\ \textasciitilde{}\ "Middle"}: Any values not meeting
    the previous conditions are labeled ``Middle.''
  \end{itemize}
\end{itemize}

\textbf{Task}: Add some Metadata.

Define metadata for the new variable by labeling income categories.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Add metadata by converting to a factor and defining labels}

\NormalTok{frs\_independent}\SpecialCharTok{$}\NormalTok{income\_category }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(frs\_independent}\SpecialCharTok{$}\NormalTok{income\_category,}
                        \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Low"}\NormalTok{, }\StringTok{"Middle"}\NormalTok{, }\StringTok{"High"}\NormalTok{), }\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"\textless{}= £10,000"}\NormalTok{, }\StringTok{"£10,001 {-} £49,999"}\NormalTok{, }\StringTok{"\textgreater{}= £50,000"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\textbf{Task}: Check your work.

Examine the frequency distribution of the variable you have just
created. Both variables should have the same number of missing cases,
unless:

\begin{itemize}
\tightlist
\item
  Missing cases in the old variable have been intentionally converted
  into valid cases in the new variable.
\item
  You forgot to allocate a new value to one of the old variable
  categories, in which case the new variable will have more missing
  cases than the old variable.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Frequency distribution of income categories}
\FunctionTok{table}\NormalTok{(frs\_independent}\SpecialCharTok{$}\NormalTok{income\_category)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

       <= £10,000 £10,001 - £49,999        >= £50,000 
             8584             22981              2271 
\end{verbatim}

After preparing the data, use cross-tabulations to compare income levels
across demographic groups.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}tabulate income category by age group, nationality, etc.}
\FunctionTok{table}\NormalTok{(frs\_independent}\SpecialCharTok{$}\NormalTok{income\_category, frs\_independent}\SpecialCharTok{$}\NormalTok{age\_group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   
                    16-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64
  <= £10,000          373   680   492   558   474   511   554   652   781   826
  £10,001 - £49,999   263  1241  1802  2056  2052  1948  1995  1967  1749  1772
  >= £50,000            1     8    59   186   314   331   334   356   237   177
                   
                    65-69 70-74  75+
  <= £10,000          773   744 1166
  £10,001 - £49,999  2073  1554 2509
  >= £50,000          144    56   68
\end{verbatim}

Explore income distribution across different regions.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Cross{-}tabulate income category by region}
\FunctionTok{table}\NormalTok{(frs\_independent}\SpecialCharTok{$}\NormalTok{income\_category, frs\_independent}\SpecialCharTok{$}\NormalTok{region) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   
                    East Midlands East of England London North East North West
  <= £10,000                  562             665    740        357        878
  £10,001 - £49,999          1550            1855   1850        979       2347
  >= £50,000                  135             245    367         48        174
                   
                    Northern Ireland Scotland South East South West Wales
  <= £10,000                     874     1212        895        588   399
  £10,001 - £49,999             2305     3234       2563       1707   971
  >= £50,000                     123      322        367        149    63
                   
                    West Midlands Yorks and the Humber
  <= £10,000                  744                  670
  £10,001 - £49,999          1892                 1728
  >= £50,000                  164                  114
\end{verbatim}

\textbf{Tips for Cross-Tabulation}

\begin{itemize}
\tightlist
\item
  Place the income variable in the columns.
\item
  Add multiple variables in the rows to create simultaneous
  cross-tabulations.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Lab: Correlation, Single, and Multiple Linear
Regression}\label{lab-correlation-single-and-multiple-linear-regression}

In this week's practical, we will review how to calculate and visualise
correlation coefficients between variables. This practical is split into
two parts. The first part focuses on measuring the correlation between
and visualising the relationship between \textbf{continuous variables}.
The second part goes through the implementation of a Linear Regression
Model, again between continuous variables.

Before getting into it, have a look at this
\href{https://mlu-explain.github.io/linear-regression/}{resource}, it
really helps understand how regression models work.

The lecture's slides can be found
\href{https://github.com/GDSL-UL/stats/blob/main/lectures/lecture08.pdf}{here}.

\textbf{Learning Objectives:}

\begin{itemize}
\tightlist
\item
  Visualise the association between two continuous variables using a
  scatterplot.
\item
  Measure the strength of the association between two variables by
  calculating their correlation coefficient.
\item
  Build a formal regression model.
\item
  Understand how to estimate and interpret a multiple linear regression
  model.
\end{itemize}

\textbf{Notes:} \emph{Note for the practical: copy/edit/document the
code in your own \texttt{.qmd} file. The code is supposed to be run as
if the script was placed in the course folder or in the folder
\texttt{labs}.}

\emph{Note on file paths}: When calling the \texttt{read.csv} function,
the path will vary depending on the location of the script it is being
executed or your working directory (WD):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{If the script or your WD is in a sub-folder} (e.g.,
  \texttt{labs}), use
  \texttt{"../data/Census2021/EW\_DistrictPercentages.csv"}. The
  \texttt{..} tells R to go one level up to the main directory
  (\texttt{stats/}) and then access the \texttt{data/} folder.
  \textbf{Example}:
  \texttt{read.csv("../data/Census2021/EW\_DistrictPercentages.csv")}.
\item
  \textbf{If the script is in the main directory} (e.g.~inside
  \texttt{stats/}), you can access the data directly using
  \texttt{"data/Census2021/EW\_DistrictPercentages.csv"}. Here, no
  \texttt{..} is necessary as the \texttt{data/} folder is directly
  accessible from the working directory. \textbf{Example}:
  \texttt{read.csv("data/Census2021/EW\_DistrictPercentages.csv")}
\end{enumerate}

\section{Part I. Correlation}\label{part-i.-correlation}

\subsection{Data Overview: Descriptive
Statistics:}\label{data-overview-descriptive-statistics}

Let's start by picking \textbf{one dataset derived from the
English-Wales 2021 Census data}. You can choose one dataset that
aggregates data either at a) county, b) district, or c) ward-level.
Lower Tier Local Authority-, Region-, and Country-level data is also
available in the data folder.

see also:
https://canvas.liverpool.ac.uk/courses/77895/pages/census-data-2021

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries }
\FunctionTok{library}\NormalTok{(ggplot2) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: package 'ggplot2' was built under R version 4.3.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: package 'dplyr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}

Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    filter, lag
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen =} \DecValTok{999}\NormalTok{, }\AttributeTok{digits =} \DecValTok{4}\NormalTok{)  }\CommentTok{\# Avoid scientific notation and round to 4 decimals globally}

\CommentTok{\# load data}
\NormalTok{census }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/Census2021/EW\_DistrictPercentages.csv"}\NormalTok{) }\CommentTok{\# District level}
\end{Highlighting}
\end{Shaded}

We're using a (district/ward/etc.)-level census dataset that includes:

\begin{itemize}
\tightlist
\item
  \% of population with poor health (variable name:
  \texttt{pct\_Very\_bad\_health}).
\item
  \% of population with no qualifications
  (\texttt{pct\_No\_qualifications}).
\item
  \% of male population (\texttt{pct\_Males}).
\item
  \% of population in a higher managerial/professional occupation
  (\texttt{pct\_Higher\_manager\_prof}).
\end{itemize}

First, let's get some descriptive statistics that help identify general
trends and distributions in the data.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Summary statistics}
\NormalTok{summary\_data }\OtherTok{\textless{}{-}}\NormalTok{ census }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(pct\_Very\_bad\_health, pct\_No\_qualifications, pct\_Males, pct\_Higher\_manager\_prof) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise\_all}\NormalTok{(}\FunctionTok{list}\NormalTok{(}\AttributeTok{mean =}\NormalTok{ mean, }\AttributeTok{sd =}\NormalTok{ sd))}
\NormalTok{summary\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  pct_Very_bad_health_mean pct_No_qualifications_mean pct_Males_mean
1                    1.173                       17.9          48.97
  pct_Higher_manager_prof_mean pct_Very_bad_health_sd pct_No_qualifications_sd
1                        13.22                 0.3402                    3.959
  pct_Males_sd pct_Higher_manager_prof_sd
1       0.6603                       4.73
\end{verbatim}

\textbf{Q1}. Complete the table below by specifying each variable type
(continuous or categorical) and reporting its mean and standard
deviation.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type (Continuous or Categorical)
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Standard Deviation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
pct\_Very\_bad\_health & & & \\
pct\_No\_qualifications & & & \\
pct\_Males & & & \\
pct\_Higher\_manager\_prof & & & \\
\end{longtable}

\subsection{Simple visualisation for continuous
data}\label{simple-visualisation-for-continuous-data}

You can visualise the relationship between two continuous variables
using a scatter plot. Using the chosen census datasets, visualise the
association between the \% of population with bad health
(\texttt{pct\_Very\_bad\_health}) and each of the following:

\begin{itemize}
\tightlist
\item
  the \% of population with no qualifications
  (\texttt{pct\_No\_qualifications});
\item
  the \% of population aged 65 to 84 (\texttt{pct\_Age\_65\_to\_84});
\item
  the \% of population in a married couple
  (\texttt{pct\_Married\_couple});
\item
  the \% of population in a Higher Managerial or Professional occupation
  (\texttt{pct\_Higher\_manager\_prof}).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Scatterplot for each variable variables }
\NormalTok{variables }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"pct\_No\_qualifications"}\NormalTok{, }\StringTok{"pct\_Age\_65\_to\_84"}\NormalTok{, }\StringTok{"pct\_Married\_couple"}\NormalTok{, }\StringTok{"pct\_Higher\_manager\_prof"}\NormalTok{)}

\CommentTok{\# Loop to create scatterplots and calculate correlations }
\CommentTok{\# x and y variables for each scatter plot,}
\ControlFlowTok{for}\NormalTok{ (var }\ControlFlowTok{in}\NormalTok{ variables) \{ }
  \CommentTok{\# Scatterplot }
  \FunctionTok{ggplot}\NormalTok{(census, }\FunctionTok{aes\_string}\NormalTok{(}\AttributeTok{x =}\NormalTok{ var, }\AttributeTok{y =} \StringTok{"pct\_Very\_bad\_health"}\NormalTok{)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Scatterplot of pct\_Very\_bad\_health vs"}\NormalTok{, var), }
         \AttributeTok{x =}\NormalTok{ var, }\AttributeTok{y =} \StringTok{"pct\_Very\_bad\_health"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{theme\_minimal}\NormalTok{()}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
i Please use tidy evaluation idioms with `aes()`.
i See also `vignette("ggplot2-in-packages")` for more information.
\end{verbatim}

\textbf{Q2. Which of the associations do you think is strongest, which
one is the weakest?}

As noted, before, an observed association between two variables is no
guarantee of causation. It could be that the observed association is:

\begin{itemize}
\tightlist
\item
  simply a chance one due to sampling uncertainty;
\item
  caused by some third underlying variable which explains the spatial
  variation of both of the variables in the scatterplot;
\item
  due to the inherent arbitrariness of the boundaries used to define the
  areas being analysed (the `Modifiable Area Unit Problem').
\end{itemize}

\textbf{Q3. Setting these caveats to one side, are the associations
observed in the scatter-plots suggestive of any causative mechanisms of
bad health?}

Rather than relying upon an impressionistic view of the strength of the
association between two variables, we can measure that association by
calculating the relevant correlation coefficient. The Table below
identifies the statistically appropriate measure of correlation to use
between two continuous variables.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.5405}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3243}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1351}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable Data Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Measure of Correlation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Range
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Both symmetrically distributed & Pearson's & -1 to +1 \\
One or both with a skewed distribution & Spearman's Rank & -1 to +1 \\
\end{longtable}

\textbf{Different Calculation Methods}: Pearson's correlation assumes
linear relationships and is suitable for symmetrically distributed
(normally distributed) variables, measuring the strength of the linear
relationship. Spearman's rank correlation, however, works on ranked
data, so it's more suitable for skewed data or variables with non-linear
relationships, measuring the strength and direction of a monotonic
relationship.

When calculating correlation for a single pair of variables, select the
method that best fits their data distribution:

\begin{verbatim}
-   Use **Pearson’s** if both variables are symmetrically distributed.
-   Use **Spearman’s** if one or both variables are skewed.
\end{verbatim}

\includegraphics[width=7.93in,height=\textheight]{labs/../img/distributions.png}

You can check the distribution of a variable
(e.g.~\texttt{pct\_No\_qualifications} like this):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot histogram with density overlay for a chosen variable (e.g., \textquotesingle{}pct\_No\_qualifications\textquotesingle{})}
\FunctionTok{ggplot}\NormalTok{(census, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ pct\_No\_qualifications)) }\SpecialCharTok{+} 
    \FunctionTok{geom\_histogram}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y =} \FunctionTok{after\_stat}\NormalTok{(density)), }\AttributeTok{bins =} \DecValTok{30}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"skyblue"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{geom\_density}\NormalTok{(}\AttributeTok{color =} \StringTok{"darkblue"}\NormalTok{, }\AttributeTok{linewidth =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of pct\_No\_qualifications"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Value"}\NormalTok{, }\AttributeTok{y =}  \StringTok{"Density"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/02.MultipleLinear_files/figure-pdf/unnamed-chunk-5-1.pdf}

When analyzing multiple pairs of variables, using different measures
(Pearson for some pairs, Spearman for others) creates inconsistencies
since Pearson and Spearman values aren't directly comparable in size due
to their different calculation methods. To maintain consistency across
comparisons, calculate \textbf{both Pearson's and Spearman's
correlations} for each pair, e.g.~do the trends align (both showing
strong, weak, or moderate correlation in the same direction)? This
consistency check can give confidence that the relationships observed
are not dependent on the correlation method chosen. While in a report
you'd typically include only one set of correlations (usually Pearson's
if the relationships appear linear), calculating both can validate that
your observations aren't an artifact of the correlation method.

\textbf{Research Question 1: Which of our selected variables are most
strongly correlated with \% of population with bad health?}

To answer this question, complete the Table below by editing/running
this code:.

Pearson correlations

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pearson\_correlation }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(census}\SpecialCharTok{$}\NormalTok{pct\_Very\_bad\_health,}
\NormalTok{    census}\SpecialCharTok{$}\NormalTok{pct\_No\_qualifications,}\AttributeTok{use =} \StringTok{"complete.obs"}\NormalTok{, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{)}
    
\CommentTok{\# Display the results}
\FunctionTok{cat}\NormalTok{(}\StringTok{"Pearson Correlation:"}\NormalTok{, pearson\_correlation, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Pearson Correlation: 0.7621 
\end{verbatim}

Spearman correlations:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spearman\_correlation }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(census}\SpecialCharTok{$}\NormalTok{pct\_Very\_bad\_health,}
\NormalTok{    census}\SpecialCharTok{$}\NormalTok{pct\_No\_qualifications, }\AttributeTok{use =} \StringTok{"complete.obs"}\NormalTok{, }\AttributeTok{method =} \StringTok{"spearman"}\NormalTok{)}

\FunctionTok{cat}\NormalTok{(}\StringTok{"Spearman Correlation:"}\NormalTok{, spearman\_correlation, }\StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Spearman Correlation: 0.7785 
\end{verbatim}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Covariates & Pearson & Spearman \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
pct\_Very\_bad\_health - pct\_No\_qualifications & & \\
pct\_Very\_bad\_health - pct\_Age\_65\_to\_84 & & \\
pct\_Very\_bad\_health - pct\_Married\_couple & & \\
pct\_Very\_bad\_health - pct\_Higher\_manager\_prof & & \\
\end{longtable}

\textbf{What can you make of this numbers?}

If you think you have found a correlation between two variables in our
dataset, this doesn't mean that an association exists between these two
variables in the population at large. The uncertainty arises because, by
chance, the random sample included in our dataset might not be fully
representative of the wider population.

For this reason, we need to verify whether the correlation is
statistically significant,

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# significance test for pearson, for example}
\NormalTok{pearson\_test }\OtherTok{\textless{}{-}} \FunctionTok{cor.test}\NormalTok{(census}\SpecialCharTok{$}\NormalTok{pct\_Very\_bad\_health,}
\NormalTok{    census}\SpecialCharTok{$}\NormalTok{pct\_No\_qualifications, }\AttributeTok{method =} \StringTok{"pearson"}\NormalTok{, }\AttributeTok{use =} \StringTok{"complete.obs"}\NormalTok{)}
\NormalTok{pearson\_test}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  census$pct_Very_bad_health and census$pct_No_qualifications
t = 21, df = 329, p-value <0.0000000000000002
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.7128 0.8038
sample estimates:
   cor 
0.7621 
\end{verbatim}

Look at
https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/cor.test
for details about the function. But in general, when calculating the
correlation between two variables, a \textbf{p-value} accompanies the
correlation coefficient to indicate the statistical significance of the
observed association. This p-value tests the null hypothesis that there
is no association between the two variables (i.e., that the correlation
is zero).

When interpreting p-values, certain thresholds denote different levels
of confidence. A p-value less than 0.05 is generally considered
statistically significant at the 95\% confidence level, suggesting that
we can be 95\% confident there is an association between the variables
in the broader population. When the p-value is below 0.01, the result is
significant at the 99\% confidence level, meaning we have even greater
confidence (99\%) that an association exists. Sometimes, on research
papers or tables significance levels are denoted with asterisks: one
asterisk (\texttt{*}) typically indicates significance at the 95\% level
(p \textless{} 0.05), two asterisks (\texttt{**}) significance at the
99\% level (p \textless{} 0.01), three asterisks (\texttt{***})
significance at the 99.99\% level (p \textless{} 0.01).

Typically, p-values are reported under labels such as ``Sig
(2-tailed),'' where ``2-tailed'' refers to the fact that the test
considers both directions (positive and negative correlations).
Reporting the exact p-value (e.g., p = 0.002) is more informative than
using thresholds alone, as it gives a clearer picture of how strongly
the data contradicts the null hypothesis of no association.

\textbf{In a nutshell, lower p-values suggest a stronger statistical
basis for believing that an observed correlation is not due to random
chance. A statistically significant p-value reinforces confidence that
an association is likely to exist in the wider population, though it
does not imply causation.}

\subsection{Part. 2: Implementing a Linear Regression
Model}\label{part.-2-implementing-a-linear-regression-model}

A key goal of data analysis is to explore the potential factors of
health at the local district level. So far, we have used
cross-tabulations and various bivariate correlation analysis methods to
explore the relationships between variables. One key limitation of
standard correlation analysis is that it remains hard to look at the
associations of an outcome/dependent variable to multiple
independent/explanatory variables at the same time. Regression analysis
provides a very useful and flexible methodological framework for such a
purpose. Therefore, we will investigate how various local factors impact
residents' health by building a multiple linear regression model in R.

We use \texttt{pct\_Very\_bad\_health} as a proxy for residents' health.

\textbf{Research Question 2: How do local factors affect residents'
health?}

\textbf{Dependent (or Response) Variable}:

\begin{itemize}
\tightlist
\item
  \% of population with bad health (\texttt{pct\_Very\_bad\_health}).
\end{itemize}

\textbf{Independent (or Explanatory) Variables}:

\begin{itemize}
\tightlist
\item
  \% of population with no qualifications
  (\texttt{pct\_No\_qualifications}).
\item
  \% of male population (\texttt{pct\_Males}).
\item
  \% of population in a higher managerial/professional occupation
  (\texttt{pct\_Higher\_manager\_prof}).
\end{itemize}

Load some other Libraries

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: package 'tidyverse' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'tibble' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'tidyr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'readr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'purrr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'stringr' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'forcats' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
Warning: package 'lubridate' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --
v forcats   1.0.0     v stringr   1.5.1
v lubridate 1.9.3     v tibble    3.2.1
v purrr     1.0.2     v tidyr     1.3.1
v readr     2.1.5     
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
i Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: package 'broom' was built under R version 4.3.2
\end{verbatim}

and the data (if not loaded):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load dataset}
\NormalTok{census }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/Census2021/EW\_DistrictPercentages.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Regression models are the standard method for constructing predictive
and explanatory models. They tell us how changes in one variable (the
target variable or independent variable, \(Y\)) are \emph{associated
with} changes in explanatory variables, or dependent variables,
\(X_1, X_2, X_3\) (\(X_n\)), etc. Classic linear regression is referred
to \emph{Ordinary least squares} (OLS) regression because they estimate
the relationship between one or more independent variables and a
dependent variable \(Y\) using a hyperplane (i.e.~a multi-dimensional
line) that minimises the sum of the squared difference between the
observed values of \(Y\) and the values predicted by the model (denoted
as \(\hat{Y}\), \(Y\)-hat).

Having seen \textbf{Single Linear Regression} in class - where the
relationship between one independent variable and a dependent variable
is modeled - we can extend this concept to situations where more than
one explanatory variable might influence the outcome. While single
linear regression helps us understand the effect of \textbf{ONE}
variable in isolation, real-world phenomena are often influenced by
multiple factors simultaneously. Multiple linear regression addresses
this complexity by allowing us to model the relationship between a
dependent variable and multiple independent variables, providing a more
comprehensive view of how various explanatory variables contribute to
changes in the outcome.

Here, regression allows us to examine the relationship between people's
health rates and multiple dependent variables.

Before starting, we define two hypotheses:

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis} (\(H_0\)): For each variable \(X_n\), there
  is no effect of \(X_n\) on \(Y\).
\item
  \textbf{Alternative hypothesis} (\(H_1\)): There is an effect
  of\(X_n\) on \(Y\).
\end{itemize}

We will test if we can reject the null hypothesis.

\subsection{Model fit}\label{model-fit}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Linear regression model}
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(pct\_Very\_bad\_health }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pct\_No\_qualifications }\SpecialCharTok{+}\NormalTok{ pct\_Males }\SpecialCharTok{+}\NormalTok{ pct\_Higher\_manager\_prof, }\AttributeTok{data =}\NormalTok{ census)}
\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pct_Very_bad_health ~ pct_No_qualifications + pct_Males + 
    pct_Higher_manager_prof, data = census)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.4903 -0.1369 -0.0352  0.0983  0.7658 

Coefficients:
                        Estimate Std. Error t value             Pr(>|t|)    
(Intercept)              4.01799    0.88004    4.57            0.0000071 ***
pct_No_qualifications    0.05296    0.00591    8.96 < 0.0000000000000002 ***
pct_Males               -0.07392    0.01785   -4.14            0.0000440 ***
pct_Higher_manager_prof -0.01309    0.00494   -2.65               0.0084 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.213 on 327 degrees of freedom
Multiple R-squared:  0.61,  Adjusted R-squared:  0.607 
F-statistic:  171 on 3 and 327 DF,  p-value: <0.0000000000000002
\end{verbatim}

\textbf{Code explanation}

\textbf{\texttt{lm()} Function}:

\begin{itemize}
\tightlist
\item
  \texttt{lm()} stands for ``linear model'' and is used to fit a linear
  regression model in R.
\item
  The formula syntax
  \texttt{pct\_Very\_bad\_health\ \textasciitilde{}\ pct\_No\_qualifications\ +\ pct\_Males\ +\ pct\_Higher\_manager\_prof}
  specifies a relationship between:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Dependent Variable}: \texttt{pct\_Very\_bad\_health}.
  \item
    \textbf{Independent Variables}: \texttt{pct\_No\_qualifications},
    \texttt{pct\_Males}, and \texttt{pct\_Higher\_manager\_prof}. The
    model is trained on the \texttt{data} dataset.
  \end{itemize}
\end{itemize}

\textbf{Storing the Model}: The \texttt{model\ \textless{}-} syntax
stores the fitted model in an object called \texttt{model}.

\texttt{summary(model)} provides a detailed output of the model's
results, including:

\begin{itemize}
\tightlist
\item
  \textbf{Coefficients}: Estimates of the regression slopes (i.e., how
  each independent variableaffects \texttt{pct\_Very\_bad\_health}).
\item
  \textbf{Standard Errors}: The variability of each coefficient
  estimate.
\item
  \textbf{t-values} and \textbf{p-values}: Indicate the statistical
  significance of the effect of each independent (explanatory)
  variable.\\
\item
  \textbf{R-squared} and \textbf{Adjusted R-squared}: Show how well the
  independent variables explain the variance in the dependent variable.
\item
  \textbf{F-statistic}: Tests the overall significance of the model.
\end{itemize}

\textbf{We can focus only on certain output metrics:}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Regression coefficients}
\NormalTok{coefficients }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(model)}
\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  term                    estimate std.error statistic  p.value
  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>
1 (Intercept)               4.02     0.880        4.57 7.06e- 6
2 pct_No_qualifications     0.0530   0.00591      8.96 2.54e-17
3 pct_Males                -0.0739   0.0179      -4.14 4.40e- 5
4 pct_Higher_manager_prof  -0.0131   0.00494     -2.65 8.43e- 3
\end{verbatim}

These are:

\begin{itemize}
\tightlist
\item
  \textbf{Regression Coefficient Estimates.}
\item
  \textbf{P-values.}
\item
  \textbf{Adjusted R-squared}.
\end{itemize}

\subsection{How to interpret the output
metrics}\label{how-to-interpret-the-output-metrics}

\subsubsection{\texorpdfstring{\textbf{Regression} Coefficient
Estimates}{Regression Coefficient Estimates}}\label{regression-coefficient-estimates}

The \texttt{Estimate} column in the output table tells us the rate of
change between each dependent variable \(X_n\) and \(Y\).

\textbf{Intercept}: In the regression equation, this is \(β_0\) and it
indicates the value of \(Y\) when \(X_n\) are equal to zero.

\textbf{Slopes}: These are the other regression coefficients of an
independent variable, e.g.~\(β_1\), i.e.~estimated average changes in
\(Y\) for a one unit change in an independent variable, e.g.~\(X_1\),
when all other dependent or explanatory variables are held constant.

\emph{There are two key points worth mentioning:}

\begin{itemize}
\tightlist
\item
  \textbf{The unit of} \(X\) and \(Y\): you need to know what the units
  are of the independent and dependent variables. For instance, one unit
  could be one year if you have an age variable, or a one percentage
  point if the variable is measured in percentages (all the variables in
  this week's practical).
\item
  \textbf{All the other explanatory variables are held constant}. It
  means that the coefficient of an explanatory variable \(X_1\)
  (e.g.~\(β_1\)) should be interpreted as: a one unit change in \(X_1\)
  is associated with \(β_1\) units change in \(Y\), keeping other values
  of explanatory variables (e.g.~\(X_2\), \(X_3\)) constant -- for
  instance, \(X_2\)= 0.1 or \(X_3\)= 0.4.
\end{itemize}

For the independent variable \(X\), we can derive how changes of 1 unit
for the independent are associated with the changes in
\texttt{pct\_Very\_bad\_health}, for example:

\begin{itemize}
\tightlist
\item
  The association of \texttt{pct\_No\_qualifications} is positive and
  strong: each increase in 1\% of \texttt{pct\_No\_qualifications} is
  associated with an increase of 0.05\% of very bad health rate.
\item
  The association of \texttt{pct\_Males} is negative and strong: each
  decrease in 1\% of \texttt{pct\_Males} is associated with an increase
  of 0.07\% of \texttt{pct\_Very\_bad\_health} in the population in
  England and Wales.
\item
  The association of \texttt{pct\_Higher\_manager\_prof} is negative but
  weak: each decrease in 1\% of \texttt{pct\_Higher\_manager\_prof} is
  associated with an increase of 0.013\% of
  \texttt{pct\_Very\_bad\_health}.
\end{itemize}

\subsubsection{P-values and
Significance}\label{p-values-and-significance}

The \textbf{\emph{t tests}} of regression coefficients are used to judge
the statistical inferences on regression coefficients, i.e.~associations
between independent variables and the outcome variable. For a
t-statistic of a dependent variable, there is a corresponding
\textbf{\emph{p-value}} that indicates different levels of significance
in the column \texttt{Pr(\textgreater{}\textbar{}t\textbar{})} and the
asterisks \texttt{∗}.

\begin{itemize}
\tightlist
\item
  \texttt{***} indicates ``changes in \(X_n\) are significantly
  associated with changes in \(Y\) at the \textless0.001 level''.
\item
  \texttt{**} suggests that ``changes in \(X_n\) are significantly
  associated with changes in \(Y\) between the 0.001 and (\textless)
  0.01 levels''.
\item
  Now you should know what \texttt{*} means: The significance is between
  the 0.01 and 0.05 levels, which means that we observe a less
  significant (but still significant) relationship between the
  variables.
\end{itemize}

P-value provide a measure of how significant the relationship is; it is
an indication of whether the relationship between \(X_n\) and \(Y\)
found in this data could have been found by chance. Very small p-values
suggest that the level of association found here might \textbf{not} have
come from a random sample of data.

In this case, we can say:

\begin{itemize}
\tightlist
\item
  Given that the p-value is indicated by \texttt{***}, changes in
  \texttt{pct\_No\_qualifications} and \texttt{pct\_Males} are
  significantly associated with changes in
  \texttt{pct\_Very\_bad\_health} at the \textless0.001 level; the
  association is highly statistically significant; we can be confident
  that the observed relationship between these variables and
  \texttt{pct\_Very\_bad\_health} is not due to chance.
\item
  Given that the p-value is indicated by \texttt{**}, changes in
  \texttt{pct\_Higher\_manager\_prof} are significantly associated with
  changes in \texttt{pct\_Very\_bad\_health} at the 0.001 level. This
  means that the association between the independent and dependent
  variable is not one that would be found by chance in a series of
  random sample 99.999\% of the time.
\end{itemize}

In both cases we can then confidently reject the \textbf{Null}
hypothesis (\(H_0\): no association between dependent and independent
variables exist).

\textbf{Remember}, If the \emph{p-value} of a coefficient is smaller
than 0.05, that coefficient is statistically significant. In this case,
you can say that the relationship between this independent variable and
the outcome variable is \emph{statistically} significant. Contrarily, if
the \emph{p-value} of a coefficient is larger than 0.05 you can conclude
that there is no evidence of an association or relationship between the
independent variable and the outcome variable.

\subsubsection{R-squared and Adjusted
R-squared}\label{r-squared-and-adjusted-r-squared}

These provide a measure of model fit. They are calculated as the
difference between the actual value of \(Y\) and the value predicted by
the model. The \textbf{R-squared} and \textbf{Adjusted R-squared} values
are statistical measures that indicate how well the independent
variables in your model explain the variability of the dependent
variable. Both R-squared and Adjusted R-squared help us understand how
closely the model's predictions align with the actual data. An R-squared
of 0.6, for example, indicates that 60\% of the variability in \(Y\) is
explained by the independent variables in the model. The remaining 40\%
is due to other factors not captured by the model.

Adjusted R-squared also measures the goodness of fit, but it adjusts for
the number of independent variables in the model, accounting for the
fact that adding more variables can artificially inflate R-squared
without genuinely improving the model. This is especially useful when
comparing models with different numbers of independent variables. If
Adjusted R-squared is close to or above 0.6, as in your example, it
implies that the model has a \textbf{strong explanatory power} while not
being overfit with unnecessary explanatory variables.

A high R-squared and Adjusted R-squared indicate that the model captures
much of the variation in the data, making it more reliable for
predictions or for understanding the relationship between \(Y\) and the
explanatory variables. However Low R-squared values suggest (e.g.~0.15)
that the model might be missing important explanatory variables or that
the relationship between \(Y\) and the selected explanatory variables is
not well-captured by a linear approach.

An R-squared and Adjusted R-squared over 0.6 are generally seen as signs
of a \textbf{well-fitting model} in many fields, though the ideal values
can depend on the context and the complexity of the data.

\subsection{Interpreting the Results}\label{interpreting-the-results}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  term                    estimate std.error statistic  p.value
  <chr>                      <dbl>     <dbl>     <dbl>    <dbl>
1 (Intercept)               4.02     0.880        4.57 7.06e- 6
2 pct_No_qualifications     0.0530   0.00591      8.96 2.54e-17
3 pct_Males                -0.0739   0.0179      -4.14 4.40e- 5
4 pct_Higher_manager_prof  -0.0131   0.00494     -2.65 8.43e- 3
\end{verbatim}

\textbf{Q4}. Complete the table above by filling in the coefficients,
t-values, p-values, and indicating if each variable is statistically
significant.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3425}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1918}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1370}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1918}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Coefficients
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
t-values
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p-values
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Significant?
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
pct\_No\_qualifications & & & & \\
pct\_Males & & & & \\
pct\_Higher\_manager\_prof & & & & \\
\end{longtable}

From the lecture notes, you know that the Intercept or Constant
represents the estimated average value of the outcome variable when the
values of all independent variables are equal to zero.

\textbf{Q5}. When values of \texttt{pct\_Males},
\texttt{pct\_No\_qualifications} and \texttt{pct\_Higher\_manager\_prof}
are all \(zero\), what is the \% of population with very bad health? Is
the intercept term meaningful? Are there any districts (or zones,
depending on the dataset you chose) with zero percentages of persons
with no qualification in your data set?

\textbf{Q6}. Interpret the regression coefficients of
\texttt{pct\_Males}, \texttt{pct\_No\_qualifications} and
\texttt{pct\_Higher\_manager\_prof}. Do they make sense?

\subsection{Identify factors of \% bad
health}\label{identify-factors-of-bad-health}

Now combine the above two sections and identify factors affecting the
percentage of population with very bad health. Fill in each row for the
direction (positive or negative) and significance level of each
variable.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3425}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3014}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3562}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Variable Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Positive or Negative
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistical Significance
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
pct\_No\_qualifications & & \\
pct\_Higher\_manager\_prof & & \\
pct\_Males & & \\
\end{longtable}

\textbf{Q7}. Think about the potential conclusions that can be drawn
from the above analyses. Try to answer the research question of this
practical: How do local factors affect residents' health? Think about
causation \emph{vs} association and consider potential confounders when
interpreting the results. How could these findings influence local
health policies?

\section{Part C: Practice and
Extension}\label{part-c-practice-and-extension}

\textbf{If you haven't understood something, if you have doubts, even if
they seem silly, ask.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Finish working through the practical.
\item
  Revise the material.
\item
  Extension activities (optional): Think about other potential factors
  of very bad health and test your ideas with new linear regression
  models.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Lab: Correlation and Multiple Linear Regression with
Qualitative
Variables}\label{lab-correlation-and-multiple-linear-regression-with-qualitative-variables}

The lecture's slides can be
found~\href{https://github.com/GDSL-UL/stats/blob/main/lectures/lecture09.pdf}{here}.

In last week, we introduced Multiple Linear Regression (MLR) - a
statistical method that models the relationship between a dependent
variable and two or more independent variables, allowing researchers to
examine how various predictors jointly influence an outcome. By using
the following R, we create and interpret the model:

\texttt{model\ \textless{}-\ lm(pct\_Very\_bad\_health\ \textasciitilde{}\ pct\_No\_qualifications\ +\ pct\_Males\ +\ pct\_Higher\_manager\_prof,\ data\ =\ census)}

\texttt{summary(model)}

In a regression model, independent/predictor variables could be
continuous or categorical (or qualitative). While continuous variables
capture quantitative effects, \textbf{categorical variables provide
insights into differences across groups}. When we say categorical
variables, we normally mean:

\begin{itemize}
\item
  Nominal Data: categorical data without natural order. E.g. Gender,
  Colour, Country\ldots{}
\item
  Ordinal Data: categorical data with a meaningful order. E.g. Education
  level, Customer satisfaction, Grade\ldots{}
\end{itemize}

By blending continuous and categorical predictors, MLR with categorical
variables enhances the model's ability to reflect real-world
complexities and improves interpretability, as it allows analysts to
assess how each category or group within a independent variable
influences the dependent variable.

For most categorical (especially the \emph{nominal}) variables, they
cannot be included in the regression model directly as a continuous
independent variable. Instead, these qualitative independent variables
should be included in regression models by using the \textbf{dummy
variable} approach, transforming categorical information into a
numerical format suitable for regression analysis.

However, R provides a powerful way, by automatively handling with such
process when the categorical variable is designated as a factor and to
be included in the regression model. This makes it much easier for you
to use categorical variables in the regression model to assess the
effects of categorical groupings on the dependent variable alongside
continuous predictors.

Learning Objectives:

In this week's practical we are going to

\begin{itemize}
\item
  Analysis of categorical/qualitative variables
\item
  Estimate and interpret a multiple linear regression model with
  categorical variables
\item
  Make predictions using a regression model
\end{itemize}

\section{Analysis categorical
variables}\label{analysis-categorical-variables}

Recall in Week 7, you get familiar to R by using the Family Resource
Survey data. Today we will keep explore the data by using its
categorical variables. As usual we first load the necessary libraries.

\textbf{Some tips to avoid R returning can't find data errors:}

Check your working directory by

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{getwd}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Check the relative path of your data folder on your PC/laptop, make sure
you know the relative path of your data from your workding directory,
returned by \texttt{getwd()}.

\textbf{Library knowledge used in today:}

\begin{itemize}
\item
  \textbf{\texttt{dplyr}}: a basic library provides a suite of functions
  for data manipulation
\item
  \textbf{\texttt{ggplot2}}: a widely-used data visualisation library to
  help you create nice plots through layered plotting.
\item
  \textbf{\texttt{tidyverse}}: a collection of R packages designed for
  data science, offering a cohesive framework for data manipulation,
  visualization, and analysis. Containing dyplyr, ggplot2 and other
  basic libraries.
\item
  \textbf{\texttt{broom}}: a part of the tidyverse and is designed to
  convert statistical analysis results into tidy data frames.
\item
  \textbf{\texttt{forcats}}: designed to work with factors, which are
  used to represent categorical data. It simplifies the process of
  creating, modifying, and ordering factors.
\item
  \textbf{\texttt{vcd}}: visualise and analyse categorical data.
\end{itemize}

\textbf{A useful shortcut to format your code: select all your code
lines, use Ctrl+Shift+A for automatically format them in a tidy way.}

\subsection{Data overview}\label{data-overview}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{,}\AttributeTok{dependencies =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: dplyr
\end{verbatim}

\begin{verbatim}

Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    filter, lag
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load necessary libraries }
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{,}\AttributeTok{dependencies =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: ggplot2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"broom"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"broom"}\NormalTok{,}\AttributeTok{dependencies =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: broom
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr) }
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

Or we can use library \texttt{tidyverse} which includes
\texttt{ggplot2}, \texttt{dplyr,broom} and other foundamental libraries
together already, remember you need first install the package if you
haven't by using \texttt{install.packages("tidyverse")}.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{,}\AttributeTok{dependencies =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: tidyverse
\end{verbatim}

\begin{verbatim}
-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --
v forcats   1.0.0     v stringr   1.5.1
v lubridate 1.9.3     v tibble    3.2.1
v purrr     1.0.2     v tidyr     1.3.1
v readr     2.1.5     
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
i Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

We will also use forcat library, so

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"forcats"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"forcats"}\NormalTok{)}

\FunctionTok{library}\NormalTok{(forcats)}
\end{Highlighting}
\end{Shaded}

Exactly as you did in previous weeks, we first load in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frs\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/FamilyResourceSurvey/FRS16{-}17\_labels.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Recall in previous weeks, we used the following code to overview the
dataset. Familiar yourself again by using them:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(frs\_data)}
\FunctionTok{glimpse}\NormalTok{(frs\_data)}
\end{Highlighting}
\end{Shaded}

and also \texttt{summary()} to produce summaries of each variable

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(frs\_data)}
\end{Highlighting}
\end{Shaded}

You may notice that for the numeric variables such as
\emph{hh\_income\_gross} (household gross income) and
\emph{work\_hours}(worked hours per week), the \texttt{summary()} offers
useful descriptive statistics. While for the qualitative information,
such as \emph{age\_group} (age group), \emph{highest\_qual (}Highest
educational qualification), \emph{marital\_status (}Marital status) and
\emph{nssec (}Socio-economic status), the \texttt{summary()} function is
not that useful by providing mean or median values.

Performing descriptive analysis for categorical variables or qualitative
variables, we focus on summarising the frequency and distribution of
categories within the variable. This analysis helps understand the
composition and diversity of categories in the data, which is especially
useful for identifying patterns, common categories, or potential data
imbalances.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Frequency count}
\FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{age\_group)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  0-4 05-10 11-15 16-19 20-24 25-29 30-34 35-39 40-44 45-49 50-54 55-59 60-64 
 2914  3575  2599  1858  1929  2353  2800  2840  2790  2883  2975  2767  2775 
65-69 70-74   75+ 
 2990  2354  3743 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{highest\_qual)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

A-level or equivalent       Degree or above       Dependent child 
                 5260                  9156                 10298 
   GCSE or equivalent             Not known                 Other 
                 9729                  6820                  2882 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{marital\_status)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                          Cohabiting Divorced/civil partnership dissolved 
                                4015                                 2199 
           Married/Civil partnership                            Separated 
                               18195                                  747 
                              Single                              Widowed 
                               16663                                 2326 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{nssec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                              Dependent child 
                                        10299 
                            Full-time student 
                                          963 
              Higher professional occupations 
                                         3004 
                     Intermediate occupations 
                                         4372 
          Large employers and higher managers 
                                         1025 
Lower managerial and professional occupations 
                                         8129 
  Lower supervisory and technical occupations 
                                         2400 
         Never worked or long-term unemployed 
                                         1516 
                             Not classifiable 
                                          107 
                          Routine occupations 
                                         4205 
                     Semi-routine occupations 
                                         5226 
      Small employers and own account workers 
                                         2899 
\end{verbatim}

By using ggplot2, it is easy to create some nice descriptive charts for
the categorical variables, such like what you did for the continuous
variables last week.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(frs\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ highest\_qual)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill=}\StringTok{"brown"}\NormalTok{,}\AttributeTok{width=}\FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Histogram of Highest Qualification in FRS"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Highest Qualification"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}\SpecialCharTok{+}\CommentTok{\#set text info}
  \FunctionTok{theme\_classic}\NormalTok{()}\CommentTok{\#choose theme type, try theme\_bw(), theme\_minimal() see differences}
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/03.QualitativeVariable_files/figure-pdf/unnamed-chunk-9-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(frs\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ health)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill=}\StringTok{"skyblue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\AttributeTok{stat =} \StringTok{"count"}\NormalTok{, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ ..count..),}\AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{0.3}\NormalTok{,}\AttributeTok{colour =} \StringTok{"grey"}\NormalTok{)}\SpecialCharTok{+} \CommentTok{\#add text}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Histogram of Health in FRS"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Health"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{)}\SpecialCharTok{+}\CommentTok{\#set text info}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/03.QualitativeVariable_files/figure-pdf/unnamed-chunk-10-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(frs\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ nssec)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{"yellow4"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Histogram of NSSEC in FRS"}\NormalTok{, }\AttributeTok{x =} \StringTok{"NSSEC"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}\SpecialCharTok{+} \CommentTok{\#Flip the Axes, add a \# in front of this line, to make the code in gray and you will see why we would better flip the axes at here}
  \FunctionTok{theme\_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/03.QualitativeVariable_files/figure-pdf/unnamed-chunk-11-1.pdf}

If we want to reorder the Y axis by from highest to lowest, we use the
functions in \texttt{forcats} library. \texttt{fct\_infreq()}: orders by
the value's frequency of the variable \texttt{nssec}.
\texttt{fct\_rev()}: reverses the order to go from highest to lowest.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(frs\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fct\_rev}\NormalTok{(}\FunctionTok{fct\_infreq}\NormalTok{(nssec)))) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{fill =} \StringTok{"yellow4"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Histogram of NSSEC in FRS"}\NormalTok{, }\AttributeTok{x =} \StringTok{"NSSEC"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Count"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}\SpecialCharTok{+} \CommentTok{\#Flip the Axes, add a \# in front of this line, to make the code in gray and you will see why we would better flip the axes at here}
  \FunctionTok{theme\_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/03.QualitativeVariable_files/figure-pdf/unnamed-chunk-12-1.pdf}

You can change the variables in ggplot() to make your own histogram
chart for the variables you are interested in. You will learn more of
visualisation methods in Week11's practical.

\subsection{Correlation}\label{correlation}

\textbf{Q1. Which of the associations do you think is strongest? Which
is the weakest?}

As before, rather than relying upon an impressionistic view of the
strength of the association between two variables, we can measure that
association by calculating the relevant correlation coefficient.

To calculate the correlation between categorical data, we first use
Chi-squared test to assess the independence between pairs of categorical
variables, then we use Cramer's V to measures the strength of
association - the correlation coefficents in R.

\textbf{Pearson's chi-squared test} (χ2) is a statistical test applied
to sets of categorical data to evaluate how likely it is that any
observed difference between the sets arose by chance. If the p-value is
low (typically \textless{} 0.05), it suggests a significant association
between the two variables.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{health,frs\_data}\SpecialCharTok{$}\NormalTok{happy) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in chisq.test(frs_data$health, frs_data$happy): Chi-squared
approximation may be incorrect
\end{verbatim}

\begin{verbatim}

    Pearson's Chi-squared test

data:  frs_data$health and frs_data$happy
X-squared = 45594, df = 60, p-value < 2.2e-16
\end{verbatim}

If you see a warning message of Chi-squared approximation may be
incorrect. This is because some expected frequencies in one or more
cells of the cross-tabular (health * happy) are too low. The df means
degrees of freedom and it related to the size of the table and the
number of categories in each variable. The most important message from
the output is the estimated p-value, which shows as p-value \textless{}
2.2e-16 (2.2 with 16 decimals move to the left, it is a very small
number so written in scientific notation). P-value of the chi-squared
test is far smaller than 0.05, so we can say the correlation is
statistically significant.

\textbf{Cramér's V} is a measure of association for categorical (nominal
or ordinal) data. It ranges from 0 (no association) to 1 (strong
association). The main downside of using Cramer's V is that no
information is provided on whether the correlation is positive or
negative. This is not a problem if the variable pair includes a nominal
variable but represents an information loss if the both variables being
correlated are ordinal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install the \textquotesingle{}vcd\textquotesingle{} package if not installed }
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"vcd"}\NormalTok{))   }
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"vcd"}\NormalTok{, }\AttributeTok{repos =} \StringTok{"https://cran.r{-}project.org"}\NormalTok{, }\AttributeTok{dependencies =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: vcd
\end{verbatim}

\begin{verbatim}
Warning: package 'vcd' was built under R version 4.3.3
\end{verbatim}

\begin{verbatim}
Loading required package: grid
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(vcd)  }

\CommentTok{\# creat the crosstable }
\NormalTok{crosstab }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{health, frs\_data}\SpecialCharTok{$}\NormalTok{happy)}

\CommentTok{\# Calculate Cramér\textquotesingle{}s V }
\FunctionTok{assocstats}\NormalTok{(crosstab)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   X^2 df P(> X^2)
Likelihood Ratio 54036 60        0
Pearson          45594 60        0

Phi-Coefficient   : NA 
Contingency Coeff.: 0.713 
Cramer's V        : 0.454 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#you can also directly calculate the assoication between variables}
\FunctionTok{assocstats}\NormalTok{(}\FunctionTok{table}\NormalTok{(frs\_data}\SpecialCharTok{$}\NormalTok{health, frs\_data}\SpecialCharTok{$}\NormalTok{age\_group))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   X^2 df P(> X^2)
Likelihood Ratio 26557 75        0
Pearson          23854 75        0

Phi-Coefficient   : NA 
Contingency Coeff.: 0.592 
Cramer's V        : 0.329 
\end{verbatim}

\textbf{Research Question 1. Which of our selected person-level
variables is most strongly correlated with an individual's health
status?}

Use the codes of Chi-test and Cramer's V to answer this question by
completing Table 1.

\textbf{Table 1 Person-level correlations with health status}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Covariates} & & \textbf{Correlation Coefficient} &
\textbf{Statistical Significance} \\
& & \emph{Cramer's V} & \emph{p-value} \\
\emph{health} & \emph{age\_group} & & \\
\emph{Health} & \emph{highest\_qual} & & \\
\emph{health} & \emph{marital\_status} & & \\
\emph{Health} & \emph{nssec} & & \\
\end{longtable}

\section{\texorpdfstring{\textbf{Implementing a linear regression model
with a qualitative independent
variable}}{Implementing a linear regression model with a qualitative independent variable}}\label{implementing-a-linear-regression-model-with-a-qualitative-independent-variable}

\textbf{Research Question 2: How does health vary across regions in the
UK?}

The practical is split into two main parts. The first focuses on
implementing a linear regression model with a qualitative independent
variable. \textbf{Note that} you need first to set the reference
category (baseline) as the outcomes of the model reflects the
\textbf{differences} between categories with the baseline. The second
part focuses prediction based the estimated linear regression model.

First we load the UK district-level census dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load data}
\NormalTok{LAcensus }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"../data/Census2011/UK\_DistrictPercentages.csv"}\NormalTok{) }\CommentTok{\# Local authority level}
\end{Highlighting}
\end{Shaded}

Using the district-level census dataset
``\textbf{UK\_DistrictPercentages.csv}''. the variable ``Region''
(labelled as Government Office Region) is used to explore regional
inequality in health.

Familiar yourself with the dataset by using the same codes as last week:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#view the data }
\FunctionTok{View}\NormalTok{(LAcensus)  }
\FunctionTok{glimpse}\NormalTok{(LAcensus)}
\end{Highlighting}
\end{Shaded}

The names() function returns all the column names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

The dim() function can merely returns the number of rows and number of
columns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(LAcensus) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 406 128
\end{verbatim}

There are 406 rows and 130 columns in the dataset. It would be very hard
to scan throught the data if we use so many variables altogether.
Therefore, we can select several columns to tailor for this practical.
You can of course include other variables you are interested in also by
their names:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ LAcensus }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"pct\_Long\_term\_ill"}\NormalTok{,}
                            \StringTok{"pct\_No\_qualifications"}\NormalTok{,}
                            \StringTok{"pct\_Males"}\NormalTok{,}
                            \StringTok{"pct\_Higher\_manager\_prof"}\NormalTok{,}
                            \StringTok{"Region"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Simply descriptive of this new data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(df)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 pct_Long_term_ill pct_No_qualifications   pct_Males    
 Min.   :11.20     Min.   : 6.721        Min.   :47.49  
 1st Qu.:15.57     1st Qu.:19.406        1st Qu.:48.67  
 Median :18.41     Median :23.056        Median :49.09  
 Mean   :18.27     Mean   :23.257        Mean   :49.10  
 3rd Qu.:20.72     3rd Qu.:26.993        3rd Qu.:49.48  
 Max.   :27.97     Max.   :40.522        Max.   :55.47  
 pct_Higher_manager_prof     Region      
 Min.   : 4.006          Min.   : 1.000  
 1st Qu.: 7.664          1st Qu.: 3.000  
 Median : 9.969          Median : 6.000  
 Mean   :10.747          Mean   : 6.034  
 3rd Qu.:12.986          3rd Qu.: 8.000  
 Max.   :37.022          Max.   :12.000  
\end{verbatim}

Now we can retrieve the ``Region'' column from the data frame by simply
use \textbf{df\$Region}. But what if we want to understand the data
better, like the following questions?

\textbf{Q2. How many categories do the variable ``Region'' entail? How
many local authority districts does each region include?}

Simply use the function table() would return you the answer.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Region) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

 1  2  3  4  5  6  7  8  9 10 11 12 
40 47 33 12 39 67 37 30 21 22 32 26 
\end{verbatim}

The numbers in Region column indicate different regions in the UK - 1:
East Midlands; 2: East of England; 3: London; 4: North East; 5: North
West; 6: South East; 7: South West; 8: West Midlands; 9: Yorkshire and
the Humber; 10: Wales; 11: Scotland; and 12: Northern Ireland.

The \texttt{table()} function tells us that this data frame contains 12
regions, and the number of LAs belongs to each region.

Now, for better interpration of our regions with their real name rather
than the code, we can create a new column named ``\emph{Region\_label}''
by using the following code. **R can only include the categorical
variables in the \textbf{factor} type, so we set the new column
\emph{Region\_label} in \texttt{factor()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Region\_label }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Region,}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{12}\NormalTok{),}\AttributeTok{labels=}\FunctionTok{c}\NormalTok{(}\StringTok{"East Midlands"}\NormalTok{,}
                                                     \StringTok{"East of England"}\NormalTok{,}
                                                     \StringTok{"London"}\NormalTok{,}
                                                     \StringTok{"North East"}\NormalTok{,}
                                                     \StringTok{"North West"}\NormalTok{,}
                                                     \StringTok{"South East"}\NormalTok{,}
                                                     \StringTok{"South West"}\NormalTok{,}
                                                     \StringTok{"West Midlands"}\NormalTok{,}
                                                     \StringTok{"Yorkshire and the Humber"}\NormalTok{,}
                                                     \StringTok{"Wales"}\NormalTok{,}
                                                     \StringTok{"Scotland"}\NormalTok{,}
                                                     \StringTok{"Northern Ireland"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

If you re-run the table() function, the output is now more readable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Region\_label)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

           East Midlands          East of England                   London 
                      40                       47                       33 
              North East               North West               South East 
                      12                       39                       67 
              South West            West Midlands Yorkshire and the Humber 
                      37                       30                       21 
                   Wales                 Scotland         Northern Ireland 
                      22                       32                       26 
\end{verbatim}

\subsection{\texorpdfstring{\textbf{Include the categorical variables
into a regression
model}}{Include the categorical variables into a regression model}}\label{include-the-categorical-variables-into-a-regression-model}

We will continue with a very similar regression model fitted in last
week that relates Percentages long-term illness
(\emph{pct\_Long\_term\_ill}) to Percentages no-qualification
(\emph{pct\_No\_qualifications}), Percentage Males (\emph{pct\_Males})
and Percentages Higher Managerial or Professional occupation
(\emph{pct\_Higher\_manager\_prof}).

Decide which region to be set as the baseline category. The principle is
that if you want to compare the (average) long term illness outcome of
Region A to those of other regions, Region A should be chosen as the
baseline category. For example, if you want to compare the (average)
long term illness outcome of London to rest of regions in the UK, London
should be selected as the baseline category.

Implement the regression model with the newly created categorical
variables - \emph{Region\_label} in our case. R will automatically
handle the qualitative variable as dummy variables so you don't need to
concern any of that. But you need to let R knows which category of your
qualitative variable is your reference category or the baseline. Here we
will use London as our first go. \textbf{Note:} We choose London as the
baseline category so the London region will be \ul{\textbf{excluded}} in
the independent variable list.

Therefore, first, we set London as the reference:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Region\_label }\OtherTok{\textless{}{-}} \FunctionTok{fct\_relevel}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Region\_label,  }\StringTok{"London"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Similar to last week, we build our linear regression model, but also
include the \emph{Region\_label} variable into the model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(pct\_Long\_term\_ill }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pct\_Males }\SpecialCharTok{+}\NormalTok{ pct\_No\_qualifications }\SpecialCharTok{+}\NormalTok{ pct\_Higher\_manager\_prof }\SpecialCharTok{+}\NormalTok{ Region\_label, }\AttributeTok{data =}\NormalTok{ df)}

\FunctionTok{summary}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + 
    pct_Higher_manager_prof + Region_label, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2963 -0.9090 -0.1266  0.8168  5.2821 

Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                          41.54134    5.22181   7.955 1.95e-14 ***
pct_Males                            -0.75756    0.10094  -7.505 4.18e-13 ***
pct_No_qualifications                 0.50573    0.03062  16.515  < 2e-16 ***
pct_Higher_manager_prof               0.08910    0.03674   2.426  0.01574 *  
Region_labelEast Midlands             1.14167    0.35015   3.260  0.00121 ** 
Region_labelEast of England          -0.01113    0.33140  -0.034  0.97322    
Region_labelNorth East                2.70447    0.49879   5.422 1.03e-07 ***
Region_labelNorth West                2.64240    0.35468   7.450 6.03e-13 ***
Region_labelSouth East                0.48327    0.30181   1.601  0.11013    
Region_labelSouth West                2.62729    0.34572   7.600 2.22e-13 ***
Region_labelWest Midlands             0.91064    0.37958   2.399  0.01690 *  
Region_labelYorkshire and the Humber  1.03930    0.41050   2.532  0.01174 *  
Region_labelWales                     4.63424    0.41368  11.202  < 2e-16 ***
Region_labelScotland                  0.46291    0.38916   1.189  0.23497    
Region_labelNorthern Ireland          0.55722    0.42215   1.320  0.18762    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.394 on 391 degrees of freedom
Multiple R-squared:  0.8298,    Adjusted R-squared:  0.8237 
F-statistic: 136.2 on 14 and 391 DF,  p-value: < 2.2e-16
\end{verbatim}

You have already learnt how to interpret the output of regression model
last week: \textbf{Significance} (p-value), \textbf{Coefficient
Estimates, and Model fit} (R squared and Adjusted R-squared).

\textbf{Q3. Relating back to this week's lecture notes, indicate what
regions have statistically significant differences in the percentage of
long-term illness, compared to London?}

First, the \textbf{Significance} and the \textbf{Coefficient Estimates}.
By examining the P-value, which is the last column in the output table,
we can see that most of the independent variables are significant
predictor of \texttt{pct\_Long\_term\_ill}.

\begin{itemize}
\item
  Similarly to last week, we learn that the changes in
  \texttt{pct\_No\_qualifications} and \texttt{pct\_Males}are
  significantly associated with changes in \texttt{pct\_Long\_term\_ill}
  at the \textless0.001 level (with the three asterisks *** ), which is
  actually an indicator of highly statistically significant, while we
  are less confident that the observed relationship between
  \texttt{pct\_Higher\_manager\_prof} and \texttt{pct\_Long\_term\_ill}
  are statistically significant (with the two asterisks **). Through
  their coefficient estimates, we learn that:

  \begin{itemize}
  \item
    The association of \texttt{pct\_Males} is negative and strong: each
    decrease in 1\% of \texttt{pct\_Males} is associated with an
    increase of 0.75\% of long term illness rate in the population of
    UK.
  \item
    The association of \texttt{pct\_No\_qualifications} is positive and
    strong: each increase in 1\% of \texttt{pct\_No\_qualifications} is
    associated with an increase of 0.5\% of long term illness rate.
  \item
    The association of \texttt{pct\_Higher\_manager\_prof} is positive
    but weak: each increase in 1\% of
    \texttt{pct\_Higher\_manager\_prof} is associated with an increase
    of 0.08\% of \texttt{pct\_Long\_term\_ill}.
  \end{itemize}
\item
  Now comes to the dummy variables (all the items starts with
  Region\_label) created by R for our qualitative variable
  \emph{Region\_label}: \texttt{Region\_labelNorth\ East},
  \texttt{Region\_labelNorth\ West}, \texttt{Region\_labelSouth\ West}
  and \texttt{Region\_labelWales} are also statistically significant at
  the \textless0.001 level. The changes in
  \texttt{Region\_labelEast\ Midlands} are significantly associated with
  changes in \texttt{pct\_Long\_term\_ill} at the 0.001 level, while the
  changes in \texttt{Region\_labelWest\ Midlands} and
  \texttt{Region\_labelYorkshire\ and\ the\ Humber} are significantly
  associated with changes in \texttt{pct\_Long\_term\_ill} at the 0.01
  level. The 0.01 level suggests that it is a mild likelihood that the
  relationship between these independent variables and the dependent
  variable is not due to random change. They are just mildly
  statistically significant.
\item
  The coefficient estimates of them need to be interpreted by comparing
  to the reference category London. The Estimate column tells us: North
  East region is associated with a 2.7\% higher rate of long term
  illness than London when the other predictors remain the same.
  Similarly, Wales is 4.6\% higher rate of long term illness than London
  when the other predictors remain the same. You can draw the conclusion
  for the other regions in this way by using their coefficient estimate
  values.

  \textbf{Reminder}: You \textbf{cannot} draw conclusion between North
  East and Wales, nor comparison between any regions beyond London. It
  is because the regression model is built for the comparison between
  regions to your reference category London. If we want to compare
  between North East and Wales, we need to set either of them as the
  reference category by using
  \texttt{df\$Region\_label\ \textless{}-\ fct\_relevel(df\$Region\_label,\ "North\ East")}
  or
  \texttt{df\$Region\_label\ \textless{}-\ fct\_relevel(df\$Region\_label,\ "Wales")}.
\item
  \texttt{Region\_labelEast\ of\ England},
  \texttt{Region\_labelSouth\ Eest}, \texttt{Region\_labelScotland} and
  \texttt{RegionlabelNorthern\ Ireland} were not found to be
  significantly associated with \texttt{pct\_Long\_term\_ill}.
\end{itemize}

Last but not least, the \textbf{Measure of Model Fit}. The model output
suggests the R-squared and Adjusted R-squared are of greater than 0.8
indicate a reasonably well fitting model. he model explains 83.0 \% of
the variance in the dependent variable. After adjusting for the number
of independent variable, the model explains 82.4\% of the variance. They
two suggest a strong fit of the model.

Now, complete the following table.

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Region names
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Higher or lower than London
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Whether the difference is statistically significant (Yes or No)
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
East Midlands & & \\
East of England & & \\
North East & & \\
North West & & \\
South East & & \\
South West & & \\
West Midlands & & \\
Yorkshire and The Humber & & \\
Wales & & \\
Scotland & & \\
Northern Ireland & & \\
\end{longtable}

\subsection{\texorpdfstring{\textbf{Change the baseline
category}}{Change the baseline category}}\label{change-the-baseline-category}

If you would like to learn about differences in long-term illness
between North East and other regions in the UK, you need to change the
baseline category (from London) to the North East region (with variable
name ``Region\_2'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{$}\NormalTok{Region\_label }\OtherTok{\textless{}{-}} \FunctionTok{fct\_relevel}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{Region\_label, }\StringTok{"North East"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The regression model is specified again as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
\NormalTok{  pct\_Long\_term\_ill }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pct\_Males }\SpecialCharTok{+}\NormalTok{ pct\_No\_qualifications }\SpecialCharTok{+}\NormalTok{ pct\_Higher\_manager\_prof }\SpecialCharTok{+}\NormalTok{ Region\_label,}
  \AttributeTok{data =}\NormalTok{ df}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + 
    pct_Higher_manager_prof + Region_label, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.2963 -0.9090 -0.1266  0.8168  5.2821 

Coefficients:
                                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)                          44.24582    5.20125   8.507 3.85e-16 ***
pct_Males                            -0.75756    0.10094  -7.505 4.18e-13 ***
pct_No_qualifications                 0.50573    0.03062  16.515  < 2e-16 ***
pct_Higher_manager_prof               0.08910    0.03674   2.426 0.015738 *  
Region_labelLondon                   -2.70447    0.49879  -5.422 1.03e-07 ***
Region_labelEast Midlands            -1.56281    0.46292  -3.376 0.000809 ***
Region_labelEast of England          -2.71561    0.45836  -5.925 6.87e-09 ***
Region_labelNorth West               -0.06208    0.46209  -0.134 0.893206    
Region_labelSouth East               -2.22120    0.45667  -4.864 1.67e-06 ***
Region_labelSouth West               -0.07718    0.47482  -0.163 0.870957    
Region_labelWest Midlands            -1.79384    0.48230  -3.719 0.000229 ***
Region_labelYorkshire and the Humber -1.66517    0.50695  -3.285 0.001113 ** 
Region_labelWales                     1.92976    0.50111   3.851 0.000137 ***
Region_labelScotland                 -2.24157    0.47299  -4.739 3.01e-06 ***
Region_labelNorthern Ireland         -2.14725    0.49296  -4.356 1.70e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.394 on 391 degrees of freedom
Multiple R-squared:  0.8298,    Adjusted R-squared:  0.8237 
F-statistic: 136.2 on 14 and 391 DF,  p-value: < 2.2e-16
\end{verbatim}

Now it is very easy to use your model to estimate the results Y
(dependent variable) by setting all the input independent variable X.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obj\_London }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pct\_Males =} \FloatTok{49.7}\NormalTok{,}
  \AttributeTok{pct\_No\_qualifications =} \FloatTok{24.3}\NormalTok{,}
  \AttributeTok{pct\_Higher\_manager\_prof =} \FloatTok{14.7}\NormalTok{,}
  \AttributeTok{Region\_label =} \StringTok{"London"}
\NormalTok{) }
\NormalTok{obj\_NW }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pct\_Males =} \FloatTok{49.8}\NormalTok{,}
  \AttributeTok{pct\_No\_qualifications =} \FloatTok{23.3}\NormalTok{,}
  \AttributeTok{pct\_Higher\_manager\_prof =} \FloatTok{11.2}\NormalTok{,}
  \AttributeTok{Region\_label =} \StringTok{"North West"}
\NormalTok{) }
\NormalTok{obj\_NE }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pct\_Males =} \FloatTok{49.8}\NormalTok{,}
  \AttributeTok{pct\_No\_qualifications =} \FloatTok{23.3}\NormalTok{,}
  \AttributeTok{pct\_Higher\_manager\_prof =} \FloatTok{11.2}\NormalTok{,}
  \AttributeTok{Region\_label =} \StringTok{"North East"}
  
\NormalTok{) }

\FunctionTok{predict}\NormalTok{(model1, obj\_London) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
17.48952 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model1, obj\_NW) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
19.23858 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model1, obj\_NE)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
19.30065 
\end{verbatim}

\subsection{Recode the Region variable and explore regional inequality
in
health}\label{recode-the-region-variable-and-explore-regional-inequality-in-health}

In many real-word studies, we might not be interested in health
inequality across all regions. For example, in this case study, we are
interested in health inequality between \emph{London}, \emph{Other
regions in England}, \emph{Wales}, \emph{Scotland} and \emph{Northern
Ireland}. We can achieve this by re-grouping regions in the UK based on
the variable ``Region''. That said, we need to have a new grouping of
regions as follows:

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Original region labels} & \textbf{New region labels} \\
East Midlands & Other regions in England \\
East of England & Other regions in England \\
London & London \\
North East & Other regions in England \\
North West & Other regions in England \\
South East & Other regions in England \\
South West & Other regions in England \\
West Midlands & Other regions in England \\
Yorkshire and The Humber & Other regions in England \\
Wales & Wales \\
Scotland & Scotland \\
Northern Ireland & Northern Ireland \\
\end{longtable}

Here we use mutate() function in R to make it happen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df }\OtherTok{\textless{}{-}}\NormalTok{ df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{New\_region\_label =} \FunctionTok{fct\_other}\NormalTok{(}
\NormalTok{  Region\_label,}
  \AttributeTok{keep =} \FunctionTok{c}\NormalTok{(}\StringTok{"London"}\NormalTok{, }\StringTok{"Wales"}\NormalTok{, }\StringTok{"Scotland"}\NormalTok{, }\StringTok{"Northern Ireland"}\NormalTok{),}
  \AttributeTok{other\_level =} \StringTok{"Other regions in England"}
\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This code may looks a bit complex. You can simply type ?mutate in your
console. Now in your right hand Help window, the R studio offers your
the explanation of the mutate function. This is a common way you can use
R studio to help you learn what the function ca\texttt{ate()} creates
new columns that are functions of existing variables. Therefore, the
\texttt{df\ \%\textgreater{}\%\ mutate()} means add a new column into
the current dataframe \texttt{df}; the \texttt{New\_region\_label} in
the \texttt{mutate()} function indicates the name of this new column is
\texttt{New\_region\_label}. The right side of the
\texttt{New\_region\_label\ =} indicates the value we want to assign to
the \texttt{New\_region\_label} in each row.

The right side of \texttt{New\_region\_label} is

\texttt{fct\_other(Region\_label,\ keep=c("London","Wales","Scotland","Northern\ Ireland"),\ other\_level="Other\ regions\ in\ England")}

By using the code, the \texttt{fct\_other()} function checks whether
each value in the \texttt{Region\_label} column is one of the
\textbf{keep} regions: ``London'', ``Wales'', ``Scotland'', or
``Northern Ireland''. If the region is not in this list, the value is
replaced with the label ``Other regions in England''. If the region is
one of these four, the original value in \texttt{Region\_label} is kept.
This process categorizes regions that are outside of the four specified
ones into a new group labeled ``Other regions in England'', while
preserving the original labels for the specified regions.

Now we use the same way to examine our new column
\texttt{New\_region\_label}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{New\_region\_label)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                  London                    Wales                 Scotland 
                      33                       22                       32 
        Northern Ireland Other regions in England 
                      26                      293 
\end{verbatim}

Comparing with the \texttt{Region\_label}, we now can see the mutate
worked:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[,}\FunctionTok{c}\NormalTok{(}\StringTok{"Region\_label"}\NormalTok{,}\StringTok{"New\_region\_label"}\NormalTok{)]}
\end{Highlighting}
\end{Shaded}

Now you will have a new qualitative variable named
\texttt{New\_region\_label} in which the UK is divided into five
regions: London, Other regions in England, Wales, Scotland and Northern
Ireland.

\emph{Based on the newly generated qualitative variable}
\texttt{New\_region\_label}, we can now build our new linear regression
model. Don't forget:

(1) R need to deal with the categorical variables in regression model in
the factor type;

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{class}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{New\_region\_label)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "factor"
\end{verbatim}

The \texttt{class()} returns the type of the variable. The
\texttt{New\_region\_label} is already a factor variable. If not, we
need to convert it by the \texttt{as.factor()}, as we used above.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{$}\NormalTok{New\_region\_label }\OtherTok{=} \FunctionTok{as.factor}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{New\_region\_label)}
\end{Highlighting}
\end{Shaded}

2) Let R know which region you want to use as the baseline category.
Here I will use London again, but of course you can choose other
regions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df}\SpecialCharTok{$}\NormalTok{New\_region\_label }\OtherTok{\textless{}{-}} \FunctionTok{fct\_relevel}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{New\_region\_label, }\StringTok{"London"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The linear regression window is set up below. This time we include
\texttt{New\_region\_label} rather than \texttt{Region\_label} as the
region variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(}
\NormalTok{  pct\_Long\_term\_ill }\SpecialCharTok{\textasciitilde{}}\NormalTok{ pct\_Males }\SpecialCharTok{+}\NormalTok{ pct\_No\_qualifications }\SpecialCharTok{+}\NormalTok{ pct\_Higher\_manager\_prof }\SpecialCharTok{+}\NormalTok{ New\_region\_label,}
  \AttributeTok{data =}\NormalTok{ df}
\NormalTok{)}

\FunctionTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = pct_Long_term_ill ~ pct_Males + pct_No_qualifications + 
    pct_Higher_manager_prof + New_region_label, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.6719 -1.1252 -0.0556  0.9564  7.3768 

Coefficients:
                                          Estimate Std. Error t value Pr(>|t|)
(Intercept)                              47.217525   5.795439   8.147 4.86e-15
pct_Males                                -0.834471   0.113398  -7.359 1.07e-12
pct_No_qualifications                     0.472354   0.032764  14.417  < 2e-16
pct_Higher_manager_prof                   0.000497   0.040851   0.012 0.990300
New_region_labelWales                     4.345262   0.471088   9.224  < 2e-16
New_region_labelScotland                  0.143672   0.442989   0.324 0.745863
New_region_labelNorthern Ireland          0.264425   0.474074   0.558 0.577314
New_region_labelOther regions in England  1.071719   0.312746   3.427 0.000674
                                            
(Intercept)                              ***
pct_Males                                ***
pct_No_qualifications                    ***
pct_Higher_manager_prof                     
New_region_labelWales                    ***
New_region_labelScotland                    
New_region_labelNorthern Ireland            
New_region_labelOther regions in England ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.609 on 398 degrees of freedom
Multiple R-squared:  0.7693,    Adjusted R-squared:  0.7653 
F-statistic: 189.6 on 7 and 398 DF,  p-value: < 2.2e-16
\end{verbatim}

\textbf{Q4. Are there statistically significant differences in the
percentage of people with long-term illness between London and Scotland,
and between London and Wales, controlling for other variables? What
conclusions could be drawn in terms of regional differences in health
outcome?}

\section{Predictions using fitted regression
model}\label{predictions-using-fitted-regression-model}

\subsection{\texorpdfstring{\textbf{Write down the \% illness regression
model with the new region label categorical
variables}}{Write down the \% illness regression model with the new region label categorical variables}}\label{write-down-the-illness-regression-model-with-the-new-region-label-categorical-variables}

Relating to this week's lecture, the \% \emph{pct\_Long\_term\_ill} is
equal to:

\textbf{{[}write down the model{]}}

\textbf{Q5. Now imagine that the values of variables \emph{pct\_Males,
pct\_No\_qualifications, and pct\_Higher\_manager\_prof} are 49, 23 and
11, respectively, what would the percentage of persons with long-term
illness in Wales and London be?}

Check the answer at the end of this practical page.

\section{Income inequality with respect to gender and health
status}\label{income-inequality-with-respect-to-gender-and-health-status}

In this section, we will work with individual-level data (``FRS
2016-17\_label.csv'') again to explore income inequality with respect to
gender and health status.

To explore income inequality, we need to work with a data set excluding
dependent children. In addition, we look at individuals who are the
representative persons of households. Therefore, we will select cases
(or samples) that meet both conditions.

We want R to select persons only if they are the representative persons
of households and they are not dependent children. The involved
variables are \texttt{hrp} and \texttt{Dependent} for the categories
``Household Reference Person'' and ``independent'', you can select the
appropriate cases. We also want to exclude the health variable reported
as ``Not known''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frs\_df }\OtherTok{\textless{}{-}}\NormalTok{ frs\_data }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(hrp }\SpecialCharTok{==} \StringTok{"HRP"} \SpecialCharTok{\&}
\NormalTok{                                dependent }\SpecialCharTok{==} \StringTok{"Independent"} \SpecialCharTok{\&}\NormalTok{ health }\SpecialCharTok{!=} \StringTok{"Not known"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Then, we create a new numeric variable \texttt{Net\_inc\_perc} indicate
net income per capita as our dependent variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frs\_df}\SpecialCharTok{$}\NormalTok{Net\_inc\_pp }\OtherTok{=}\NormalTok{ frs\_df}\SpecialCharTok{$}\NormalTok{hh\_income\_net }\SpecialCharTok{/}\NormalTok{ frs\_df}\SpecialCharTok{$}\NormalTok{hh\_size}

\FunctionTok{summary}\NormalTok{(frs\_df}\SpecialCharTok{$}\NormalTok{Net\_inc\_pp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-238160    9074   13347   15834   19136  864812 
\end{verbatim}

The distribution of the net household income per capita can be
visualised by using \texttt{ggplot()}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(frs\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Net\_inc\_pp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}
    \AttributeTok{bins =} \DecValTok{1000}\NormalTok{,}
    \AttributeTok{color =} \StringTok{"black"}\NormalTok{,}
    \AttributeTok{fill =} \StringTok{"skyblue"}\NormalTok{,}
    \AttributeTok{alpha =} \FloatTok{0.7}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Net\_inc\_pp"}\NormalTok{, }\AttributeTok{x =} \StringTok{"Value"}\NormalTok{, }\AttributeTok{y =}  \StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\FunctionTok{label\_comma}\NormalTok{()) }\SpecialCharTok{+}  \CommentTok{\# Prevent scientific notation on the x axis}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{labs/03.QualitativeVariable_files/figure-pdf/unnamed-chunk-38-1.pdf}

Our two qualitative independent variables ``\texttt{sex}'' and
``\texttt{health}''. Let's first know what they look like:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(frs\_df}\SpecialCharTok{$}\NormalTok{sex)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Female   Male 
  7647   9180 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(frs\_df}\SpecialCharTok{$}\NormalTok{health)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

      Bad      Fair      Good  Very Bad Very Good 
     1472      4253      6277       426      4399 
\end{verbatim}

Remember what we did in the Region long-term illness practical
previously before we put the qualitative variable into the regression
model? Yes. First, make sure they are in factor type and Second, decide
the reference category. Here, I will use Female and Very Bad health
status as my base categories. You can decide what you wish to use. This
time, I use the following codes to combine these two steps in one line.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{frs\_df}\SpecialCharTok{$}\NormalTok{sex }\OtherTok{\textless{}{-}} \FunctionTok{fct\_relevel}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(frs\_df}\SpecialCharTok{$}\NormalTok{sex), }\StringTok{"Female"}\NormalTok{)}
\NormalTok{frs\_df}\SpecialCharTok{$}\NormalTok{health }\OtherTok{\textless{}{-}} \FunctionTok{fct\_relevel}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(frs\_df}\SpecialCharTok{$}\NormalTok{health), }\StringTok{"Very Bad"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Implement the regression model with the two qualitative independent
variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model\_frs }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Net\_inc\_pp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sex }\SpecialCharTok{+}\NormalTok{ health, }\AttributeTok{data =}\NormalTok{ frs\_df)}
\FunctionTok{summary}\NormalTok{(model\_frs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Net_inc_pp ~ sex + health, data = frs_df)

Residuals:
    Min      1Q  Median      3Q     Max 
-255133   -6547   -2213    3515  845673 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)      12115.5      762.9  15.881  < 2e-16 ***
sexMale           2091.2      240.6   8.691  < 2e-16 ***
healthBad         -102.8      854.3  -0.120 0.904205    
healthFair        1051.3      789.0   1.332 0.182751    
healthGood        2766.0      777.4   3.558 0.000375 ***
healthVery Good   4931.8      787.8   6.260 3.95e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 15530 on 16821 degrees of freedom
Multiple R-squared:  0.01646,   Adjusted R-squared:  0.01616 
F-statistic: 56.29 on 5 and 16821 DF,  p-value: < 2.2e-16
\end{verbatim}

The result can be formatted by:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{tidy}\NormalTok{(model\_frs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
  term            estimate std.error statistic  p.value
  <chr>              <dbl>     <dbl>     <dbl>    <dbl>
1 (Intercept)       12116.      763.    15.9   2.21e-56
2 sexMale            2091.      241.     8.69  3.90e-18
3 healthBad          -103.      854.    -0.120 9.04e- 1
4 healthFair         1051.      789.     1.33  1.83e- 1
5 healthGood         2766.      777.     3.56  3.75e- 4
6 healthVery Good    4932.      788.     6.26  3.95e-10
\end{verbatim}

\textbf{Q6.} \textbf{What conclusions could be drawn in terms of income
inequalities with respect to gender and health status? Also think about
the statistical significance of these differences.}

\section{\texorpdfstring{\textbf{Extension
activities}}{Extension activities}}\label{extension-activities}

The extension activities are designed to get yourself prepared for the
Assignment 2 in progress. For this week, try whether you can:

\begin{itemize}
\item
  Present descriptive statistics for independent variable and the
  dependent variable: counts, percentages, a centrality measure, a
  spread measure, histograms or any relevant statistic
\item
  Report the observed association between the dependent and independent
  variables: correlation plus a graphic or tabular visualisation
\item
  Briefly describe and critically discuss the results
\item
  Think about other potential factors of long-term illness and income,
  and then test your ideas with linear regression models
\item
  Summaries your model outputs and interpret the results.
\end{itemize}

\textbf{Answer of the written down model and Q5}

The model of the new region label is: \emph{pct\_Long\_term\_ill} (\%)
\emph{= 47.218+ (-0.834)* pct\_Males} (\%) \emph{+ 0.472 *
pct\_No\_qualifications} (\%) \emph{+ 1.072*Other Regions in England +
4.345* Wales}

So if the values of variables \emph{pct\_Males, pct\_No\_qualifications,
and pct\_Higher\_manager\_prof} are 49, 23 and 11,

the model of Wales will be: \emph{pct\_Long\_term\_ill} (\%) \emph{=
47.218+ (-0.834)* 49 + 0.472 * 23 + 1.072*0+ 4.345* 1 = 21.553} (you can
direct paste the number sentence into your R studio Console and the
result will be returned)

the model of London will be: \emph{pct\_Long\_term\_ill} (\%) \emph{=
47.218+ (-0.834)* 49 + 0.472 * 23 + 1.072*0+ 4.345* 0 = 17.208}

You can also make a new object like

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{obj\_London }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pct\_Males =} \DecValTok{49}\NormalTok{,}
  \AttributeTok{pct\_No\_qualifications =} \DecValTok{23}\NormalTok{,}
  \AttributeTok{pct\_Higher\_manager\_prof =} \DecValTok{11}\NormalTok{,}
  \AttributeTok{New\_region\_label =} \StringTok{"London"}
\NormalTok{)}
\NormalTok{obj\_Wales }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{pct\_Males =} \DecValTok{49}\NormalTok{,}
  \AttributeTok{pct\_No\_qualifications =} \DecValTok{23}\NormalTok{,}
  \AttributeTok{pct\_Higher\_manager\_prof =} \DecValTok{11}\NormalTok{,}
  \AttributeTok{New\_region\_label =} \StringTok{"Wales"}
\NormalTok{)}
\FunctionTok{predict}\NormalTok{(model2, obj\_London)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
17.19808 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model2, obj\_Wales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
21.54334 
\end{verbatim}

\textbf{Therefore, the percentage of persons with long-term illness in
Wales and London be 21.5\% and 17.2\% separately. If you got the right
answers, then congratulations you can now use regression model to make
prediction.}

\bookmarksetup{startatroot}

\chapter{Lab: LogisticRegression}\label{lab-logisticregression}

\textbf{Learning objectives}

An understanding of, and ability to:

\begin{itemize}
\item
  Estimate and interpret a logistic regression model
\item
  Assess the model fit
\end{itemize}

The application context of a logistic regression model is when the
dependent variable under investigation is a binary variable. Usually, a
value of 1 for this dependent variable means the occurrence of an event;
and, 0 otherwise. For example, the dependent variable for this practical
is whether a person is a long-distance commuter i.e.~1, and 0 otherwise.

In this week's practical, we are going to apply logistic regression
analysis in an attempt to answer the following research question:

\textbf{\emph{RESEARCH QUESTION: Who is willing to commute long
distances?}}

The practical is split into two main parts. The first focuses on
implementing a binary logistic regression model with SPSS. The second
part focuses the interpretation of the resulting estimates.

\section{Preparing the input
variables}\label{preparing-the-input-variables}

Prepare the data for implementing a logistic regression model. The data
set used in this practical is the ``sar\_sample\_label.csv'' and
``sar\_sample\_code.csv''. They are actually the same dataframe, only
one uses the label as the value but the other uses the code. We will
first read in both for the data overview the labels are more friendly,
and then we focus on using ``sar\_sample\_code.csv'' in the regression
model as it is easier for coding.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --
v dplyr     1.1.4     v readr     2.1.5
v forcats   1.0.0     v stringr   1.5.1
v ggplot2   3.5.0     v tibble    3.2.1
v lubridate 1.9.3     v tidyr     1.3.1
v purrr     1.0.2     
-- Conflicts ------------------------------------------ tidyverse_conflicts() --
x dplyr::filter() masks stats::filter()
x dplyr::lag()    masks stats::lag()
i Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_label }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/SAR/sar\_sample\_label.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50000 Columns: 39
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (39): country, region, county, la_group, age_group, sex, marital_status,...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_code }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"../data/SAR/sar\_sample\_code.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50000 Columns: 39
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (39): country, region, county, la_group, age_group, sex, marital_status,...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(sar\_label)}
\FunctionTok{glimpse}\NormalTok{(sar\_code)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(sar\_label)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   country             region             county            la_group        
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
  age_group             sex            marital_status      ethnicity        
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
  accom_type        arrival_year       birth_country       care_hours       
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
     cars           census_return      central_heating      crowding        
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
 dependent_children english_ability       health          hh_composition    
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
   hh_size          highest_qual         industry         last_worked       
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
 long_term_ill      main_language      move_distance      move_origin       
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
 national_identity     nssec           ons_supergroup     relation_to_hrp   
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
   religion         rural_urban           tenure          work_distance     
 Length:50000       Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character   Mode  :character  
  work_hours        work_status        work_transport    
 Length:50000       Length:50000       Length:50000      
 Class :character   Class :character   Class :character  
 Mode  :character   Mode  :character   Mode  :character  
\end{verbatim}

The outcome variable is a person's commuting distance captured by the
variable ``work\_distance''.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(sar\_label}\SpecialCharTok{$}\NormalTok{work\_distance)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                                10 to <20 km 
                                        3650 
                                  2 to <5 km 
                                        4414 
                                20 to <40 km 
                                        2014 
                                 40 to <60km 
                                         572 
                                 5 to <10 km 
                                        4190 
                                60km or more 
                                         703 
                       Age<16 or not working 
                                       25975 
                                     At home 
                                        2427 
                              Less than 2 km 
                                        4028 
                              No fixed place 
                                        1943 
Work outside England and Wales but within UK 
                                          29 
                             Work outside UK 
                                          21 
  Works at offshore installation (within UK) 
                                          34 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(sar\_code}\SpecialCharTok{$}\NormalTok{work\_distance)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   -9     1     2     3     4     5     6     7     8     9    10    11    12 
25975  4028  4414  4190  3650  2014   572   703  2427  1943    29    21    34 
\end{verbatim}

There are a variety of categories in the variable, however, we are only
interested in commuting distance and therefore in people reporting their
commuting distance. Thus, we will explore the numeric codes of the
variable ranging from 1 to 8.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3429}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6571}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Code for Work\_distance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Categories
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Less than 2 km \\
2 & 2 to \textless5 km \\
3 & 5 to \textless10 km \\
4 & 10 to \textless20 km \\
5 & 20 to \textless40 km \\
6 & 40 to \textless60 km \\
7 & 60km or more \\
8 & At home \\
9 & No fixed place \\
10 & Work outside England and Wales but within UK \\
11 & Work outside UK \\
12 & Works at offshore installation (within UK) \\
\end{longtable}

As we are also interested in exploring whether people with different
socio-economic statuses (or occupations) tend to be associated with
varying probabilities of commuting over long distances, we further
filter or select cases.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(sar\_label}\SpecialCharTok{$}\NormalTok{nssec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

                              Child aged 0-15 
                                         9698 
                            Full-time student 
                                         3041 
              Higher professional occupations 
                                         3162 
                     Intermediate occupations 
                                         5288 
          Large employers and higher managers 
                                          909 
Lower managerial and professional occupations 
                                         8345 
  Lower supervisory and technical occupations 
                                         2924 
         Never worked or long-term unemployed 
                                         2261 
                          Routine occupations 
                                         4660 
                     Semi-routine occupations 
                                         5893 
      Small employers and own account workers 
                                         3819 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(sar\_code}\SpecialCharTok{$}\NormalTok{nssec)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

   1    2    3    4    5    6    7    8    9   10   12 
 909 3162 8345 5288 3819 2924 5893 4660 2261 3041 9698 
\end{verbatim}

Using \texttt{nssec}, we select people who reported an occupation, and
delete cases with numeric codes from 9 to 12, who are \emph{unemployed},
\emph{full-time students}, \emph{children} and \emph{not classifiable}.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Code for nssec & Category labels \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Large employers and higher managers \\
2 & Higher professional occupations \\
3 & Lower managerial and professional occupations \\
4 & Intermediate occupations \\
5 & Small employers and own account workers \\
6 & Lower supervisory and technical occupations \\
7 & Semi-routine occupations \\
8 & Routine occupations \\
9 & Never worked or long-term employed \\
10 & Full-time student \\
11 & Not classifiable \\
12 & Child aged 0-15 \\
\end{longtable}

Now, similar to next week, we use the \texttt{filter()} to prepare our
dataframe today. You may already realise that using \texttt{sar\_code}
is easier to do the filtering.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_df }\OtherTok{\textless{}{-}}\NormalTok{ sar\_code }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(work\_distance}\SpecialCharTok{\textless{}=}\DecValTok{8} \SpecialCharTok{\&}\NormalTok{ nssec }\SpecialCharTok{\textless{}=}\DecValTok{8}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\textbf{Q1.} Summarise the frequencies of the two variables
``work\_distance'' and ``nssec'' with the new data.

\textbf{Recode the ``work\_distance'' variable into a binary dependent
variable}

A simple way to create a binary dependent variable representing
long-distance commuting is to use the mutate() function as discussed in
last week's practical session. Before creating the binary variables from
the ``work\_distance'' variable, we need to define \emph{what counts as
a long-distance commuting move}. Such definition can vary. Here we
define a long-distance commuting move as any commuting move over a
distance above 60km (the category of ``60km or more'').

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_df }\OtherTok{\textless{}{-}}\NormalTok{ sar\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}
  \AttributeTok{New\_work\_distance =} \FunctionTok{if\_else}\NormalTok{(work\_distance }\SpecialCharTok{\textgreater{}}\DecValTok{6}\NormalTok{, }\DecValTok{1}\NormalTok{,}\DecValTok{0}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\textbf{Q2.} Check the new \texttt{sar\_df} dataframe with new column
named \texttt{New\_work\_distance} by using the codes you have learnt.

\textbf{Prepare your ``nssec'' variable before the regression model}

The \texttt{nssec} is a categorical variable. Therefore, as we've learnt
last week, before adding the categorical variables into the regression
model, we need first make it a factor and then identify the reference
category.

We are interested in whether people with occupations being
``Small-employers and Own account workers'' are associated with a lower
probability of commuting over long distances when comparing to people in
other occupations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_df}\SpecialCharTok{$}\NormalTok{nssec }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(sar\_df}\SpecialCharTok{$}\NormalTok{nssec), }\AttributeTok{ref =} \StringTok{"5"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection{\texorpdfstring{\textbf{Implementing a logistic regression
model}}{Implementing a logistic regression model}}\label{implementing-a-logistic-regression-model}

The binary dependent variable is long-distance commuting, variable name
\texttt{New\_work\_distance}.

The independent variables are gender and socio-economic status.

For gender, we use male as the basline.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_df}\SpecialCharTok{$}\NormalTok{sex }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(}\FunctionTok{as.factor}\NormalTok{(sar\_df}\SpecialCharTok{$}\NormalTok{sex),}\AttributeTok{ref=}\StringTok{"1"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

For socio-economic status, we use code 5 (Small employers and Own
account workers) as the baseline category to explore whether people work
as independent employers show lower probability of commuting longer than
60km compared with other occupations.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#create the model}
\NormalTok{m.glm }\OtherTok{=} \FunctionTok{glm}\NormalTok{(New\_work\_distance}\SpecialCharTok{\textasciitilde{}}\NormalTok{sex }\SpecialCharTok{+}\NormalTok{ nssec, }
            \AttributeTok{data =}\NormalTok{ sar\_df, }
            \AttributeTok{family=} \StringTok{"binomial"}\NormalTok{)}
\CommentTok{\# inspect the results}
\FunctionTok{summary}\NormalTok{(m.glm) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = New_work_distance ~ sex + nssec, family = "binomial", 
    data = sar_df)

Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept) -0.44698    0.04138 -10.801   <2e-16 ***
sex2        -0.36678    0.04196  -8.742   <2e-16 ***
nssec1      -1.35519    0.10782 -12.569   <2e-16 ***
nssec2      -1.22639    0.06489 -18.898   <2e-16 ***
nssec3      -1.61400    0.05482 -29.442   <2e-16 ***
nssec4      -2.25717    0.07696 -29.329   <2e-16 ***
nssec6      -2.61631    0.10377 -25.212   <2e-16 ***
nssec7      -2.66548    0.08317 -32.047   <2e-16 ***
nssec8      -2.71172    0.09021 -30.059   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 20441  on 33025  degrees of freedom
Residual deviance: 17968  on 33017  degrees of freedom
AIC: 17986

Number of Fisher Scoring iterations: 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# odds ratios}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{coef}\NormalTok{(m.glm)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)        sex2      nssec1      nssec2      nssec3      nssec4 
 0.63955383  0.69296493  0.25789714  0.29335107  0.19909052  0.10464615 
     nssec6      nssec7      nssec8 
 0.07307217  0.06956621  0.06642224 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# confidence intervals}
\FunctionTok{exp}\NormalTok{(}\FunctionTok{confint}\NormalTok{(m.glm, }\AttributeTok{level =} \FloatTok{0.95}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Waiting for profiling to be done...
\end{verbatim}

\begin{verbatim}
                 2.5 %     97.5 %
(Intercept) 0.58959085 0.69343772
sex2        0.63818099 0.75227729
nssec1      0.20787318 0.31733101
nssec2      0.25813190 0.33291942
nssec3      0.17876868 0.22162844
nssec4      0.08984430 0.12149367
nssec6      0.05933796 0.08915692
nssec7      0.05895858 0.08169756
nssec8      0.05547661 0.07902917
\end{verbatim}

Q3. If we want to explore whether people with occupation being ``Large
employers and higher managers'', ``Higher professional occupations'' and
``Routine occupations'' are associated with higher probability of
commuting over long distance when comparing to people in other
occupation, how will we prepare the input independent variables and what
will be the specified regression model?

Hint: use mutate() to create a new column, set the value of ``Large
employers and higher managers'', ``Higher professional occupations'' and
``Routine occupations'' as original, while the rest as ``Other
occupations'' ().

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sar\_df }\OtherTok{\textless{}{-}}\NormalTok{ sar\_df }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{New\_nssec =} \FunctionTok{if\_else}\NormalTok{(}\SpecialCharTok{!}\NormalTok{nssec }\SpecialCharTok{\%in\%} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{8}\NormalTok{), }\StringTok{"0"}\NormalTok{ ,nssec))}
\end{Highlighting}
\end{Shaded}

Use ``Other occupations'' (code: 0) as the reference category by
\texttt{relevel(as.factor())} and then create the regression model:
\texttt{glm(New\_work\_distance\textasciitilde{}sex\ +\ New\_nssec,\ data\ =\ sar\_df,\ family=\ "binomial")}

\subsection{\texorpdfstring{\textbf{Model
fit}}{Model fit}}\label{model-fit-1}

Relating back to this week's lecture notes, what is the Pseudo
R\textsuperscript{2} of the fitted logistic model (from the Model
Summary table below)?

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"pscl"}\NormalTok{))}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"pscl"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: pscl
\end{verbatim}

\begin{verbatim}
Warning: package 'pscl' was built under R version 4.3.3
\end{verbatim}

\begin{verbatim}
Classes and Methods for R originally developed in the
Political Science Computational Laboratory
Department of Political Science
Stanford University (2002-2015),
by and under the direction of Simon Jackman.
hurdle and zeroinfl functions by Achim Zeileis.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(pscl)}

\CommentTok{\# Pseudo R{-}squared}
\FunctionTok{tidy}\NormalTok{(}\FunctionTok{pR2}\NormalTok{(m.glm))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
fitting null model for pseudo-r2
\end{verbatim}

\begin{verbatim}
Warning: 'tidy.numeric' is deprecated.
See help("Deprecated")
\end{verbatim}

\begin{verbatim}
# A tibble: 6 x 2
  names              x
  <chr>          <dbl>
1 llh       -8984.    
2 llhNull  -10220.    
3 G2         2473.    
4 McFadden      0.121 
5 r2ML          0.0721
6 r2CU          0.156 
\end{verbatim}

\subsection{\texorpdfstring{\textbf{Interpreting estimated regression
coefficients}}{Interpreting estimated regression coefficients}}\label{interpreting-estimated-regression-coefficients}

\begin{itemize}
\item
  The interpretation of coefficients (B) and odds ratios (Exp(B)) for
  the independent variables differs from that in a linear regression
  setting.
\item
  Interpreting the regression coefficients.
\end{itemize}

o~~ For the variable \textbf{Sex}, a negative sign and the odds ratio
estimate indicate that being female decreases the odds of commuting over
long distances by a factor of 0.462, holding all other variables
constant. Put it differently, the odds of commuting over long distances
for females are 53.8\% (i.e., 1 -- 0.462, presented as percentages)
smaller than that for males, holding all other variables constant.

o~~ For variable ``\textbf{nssec=Higher professional occupations}'', a
positive sign and the odds ratio estimate indicate that being employed
in a higher professional occupation increases the odds of commuting over
long distances by a factor of 1.873 comparing to being employed in other
occupations (the baseline categories), holding all other variables
constant (the Sex variable).

\textbf{Q4.} Interpret the regression coefficients (i.e.~Exp(B)) of
variables ``nssec=Large employers and higher managers'' and
``nssec=Routine occupations''.

\subsection{\texorpdfstring{\textbf{Statistical significance of
regression coefficients or covariate
effects}}{Statistical significance of regression coefficients or covariate effects}}\label{statistical-significance-of-regression-coefficients-or-covariate-effects}

Similar to the statistical inference in a linear regression model
context, p-values of regression coefficients are used to assess
significances of coefficients; for instance, by comparing p-values to
the conventional level of significance of 0.05:

·~~~~~~ If the p-value of a coefficient is smaller than 0.05, the
coefficient is statistically significant. In this case, you can say that
the relationship between an independent variable and the outcome
variable is \emph{statistically} significant.

·~~~~~~ If the p-value of a coefficient is larger than 0.05, the
coefficient is statistically insignificant. In this case, you can say or
conclude that there is no statistically significant association or
relationship between an independent variable and the outcome variable.

\textbf{Q5.} Could you identify significant factors of commuting over
long distances?

\section{\texorpdfstring{\textbf{Extension
activities}}{Extension activities}}\label{extension-activities-1}

The extension activities are designed to get yourself prepared for the
Assignment 2 in progress. For this week, try whether you can:

\begin{itemize}
\item
  Select a regression strategy and explain why a linear or logit model
  is appropriate
\item
  Perform one or a series of regression models, including different
  combinations of your chosen independent variables to explain and/or
  predict your dependent variable
\end{itemize}




\end{document}
